{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Autoseq: Python command line tool to analyze clinical genomics data Autoseq can be used to analyze clinical genomics data which is generated by Karolinska Hospital. It has two major piplines called Alascca, LiqBio. Alascca pipeline is used for ALASCCA study which is A randomized, placebo-controlled, multicenter study of the value of low dose aspirin (ASA) added to patients with colorectal cancer. LiqBio pipeline is used to analyze cell free t-DNA from blood samples in PROBIO study, Prostate cancer. Liqbio Pipeline has two versions, one with umi processing steps and another normal pipeline. Source code GitHub License Apache License 2.0 Packages Python 2.7 Q A Questions Answers Quick Start Getting Started Installation Command Line usage General Discriptions CLI Options Pipelines alascca liqbio Tool autoseq generate_ref","title":"Autoseq"},{"location":"#autoseq-python-command-line-tool-to-analyze-clinical-genomics-data","text":"Autoseq can be used to analyze clinical genomics data which is generated by Karolinska Hospital. It has two major piplines called Alascca, LiqBio. Alascca pipeline is used for ALASCCA study which is A randomized, placebo-controlled, multicenter study of the value of low dose aspirin (ASA) added to patients with colorectal cancer. LiqBio pipeline is used to analyze cell free t-DNA from blood samples in PROBIO study, Prostate cancer. Liqbio Pipeline has two versions, one with umi processing steps and another normal pipeline. Source code GitHub License Apache License 2.0 Packages Python 2.7 Q A Questions Answers Quick Start Getting Started Installation Command Line usage General Discriptions","title":"Autoseq: Python command line tool to analyze clinical genomics data"},{"location":"#cli-options","text":"Pipelines alascca liqbio Tool autoseq generate_ref","title":"CLI Options"},{"location":"autoseq/","text":"Getting Started Autoseq is a command line tool and it has multiple pipelines used to analyse cancer genomics data from clinical samples. Piplines Alascca Liqbio (UMI tag processing) RNAseq (under development) Latest version: [v0.6.0][dist] Note: This version of autoseq is specially updated for PROBIO trail on prostate cancer. Installation Step 1: Clone the alascca-dotfiles repo git clone https://github.com/clinseq/probio-dotfiles.git /nfs/PROBIO/probio-dotfiles Step 2: Go to dotfiles directory cd /nfs/PROBIO/probio-dotfiles Step 3: Run the .bash_profile file to create new environment . /nfs/PROBIO/probio-dotfiles/.bash_profile Step 4: Run this script to install all prerequisites bash install-prereqs.sh Command Line Usage Usage: autoseq [OPTIONS] COMMAND [ARGS]... Options: --ref TEXT json with reference files to use --job-params TEXT JSON file specifying various pipeline job parameters. --outdir PATH output directory --libdir TEXT directory to search for libraries --runner_name TEXT Runner to use. --loglevel TEXT level of logging --jobdb TEXT sqlite3 database to write job info and stats --dot_file TEXT write graph to dot file with this name --cores INTEGER max number of cores to allow jobs to use --umi To process the data with UMI- Unique Molecular Identifier --scratch TEXT scratch dir to use --help Show this message and exit. Commands: alascca liqbio liqbio-prepare Generate the reference files generate-ref command can be used to generate all required reference files to run the pipeline. generate-ref --genome-resources /nfs/PROBIO/genome-resources --outdir /nfs/PROBIO/autoseq-genome LiqBio Pipeline autoseq --ref ref.json --outdir /path/to/outdir --jobdb jobdb.json --cores 5 --runner_name slurmrunner --libdir /path/to/libdir liqbio sample.json with UMI --umi flag is used to call UMI pipeline autoseq --ref ref.json --outdir /path/to/outdir --jobdb jobdb.json --cores 5 --runner_name slurmrunner --libdir /path/to/libdir --umi liqbio sample.json General Discriptions Clinseq barcodes Each sample+preparation+capture item should have a corresponding barcode with the format PROJECT-SDID-TYPE-SAMPLEID-PREPID-CAPTUREID where: PROJECT is a two-letter short project designator. One of AL (alascca), LB (liquid biopspy) and OT (other) SDID is an identifier for a single individual. It must match the pattern P-[a-zA-Z0-9]+ ( NOTE: This necessitates an additional \"-\" within this field). TYPE is the sample type, one of T (tumor), N (normal) and CFDNA (ctDNA) SAMPLEID identifies a single biological sample, for example piece of a tumor or a single tube of plasma. It must match the pattern [a-zA-Z0-9]+ . PREPID specifies the library preparation kit used. It must be a two-letter shortname followed by a string matching [0-9]+ , which can be used to indicate the date on which the prep was performed. The date string should preferably be in the format YYYYMMDDHHMM . For example, 201701241540 would indicate year 2017, January 24th, at 15:40. CAPTUREID specifies the capture that was performed on the library (if any). It must match either WGS (indicating that no capture was performed), or else a two-letter shortname indicating the capture kit used, followed by a string matching [0-9]+ , which can be used to indicate the date on which the capture was performed. The date should preferably be in the format YYYYMMDDHHMM . NOTE: The combination SDID-TYPE-SAMPLEID must uniquely identify a single sample. NOTE: A clinseq barcode is not garuanteed to uniquely specify a single sample+library+capture item, but in practice it should be unique if precise preparation and capture times are included within the PREPID and CAPTUREID fields. Allowed Prep IDs Autoseq know about the following preparation methods: BN = BIOO_NEXTFLEX KH = KAPA_HYPERPREP KP = KAPA_HYPERPLUS TD = THRUPLEX_DNASEQ TP = THRUPLEX_PLASMASEQ TF = THRUPLEX_FD TS = TRUSEQ_RNA NN = NEBNEXT_RNA VI = VILO_RNA Allowed Capture IDs Autoseq knows about the following capture kits: CS = clinseq_v3_targets CZ = clinseq_v4 EX = EXOMEV3 EO = EXOMEV1 RF = fusion_v1 CC = core_design CD = discovery_coho CB = big_design TT = test-regions CM = monitor CP = progression PC = probio_comprehensive PB = probio_biomarker_signature PA = pancancer Newer version of autoseq has three more panels such as PC, PB and PA. Liqbio The sample.json file has the format (right side). In this file, a single tumor and normal sample is allowed, but multiple plasma samples. If no tumor or normal sample is avaialble, they can be set to null , but if no plasma samples are available, it should be set to [] (empty list), for example \"CFDNA\": [] . { sdid : NA12877 , panel : { T : NA12877-T-03098849-TD1-TT1 , N : NA12877-N-03098121-TD1-TT1 , CFDNA : [ NA12877-CFDNA-03098850-TD1-TT1 , NA12877-CFDNA-03098850-TD2-TT1 ] }, wgs : { T : NA12877-T-03098849-TD1-WGS , N : NA12877-N-03098121-TD1-WGS , CFDNA : [ NA12877-CFDNA-03098850-TD1-WGS ] } } For the plasma samples, merging of libraries will take place before calling. On alignment, the @RG tag will be set as follows: ID = SDID-TYPE-SAMPLEID-PREPID-CAPTUREID LB = SDID-TYPE-SAMPLEID-PREPID SM = SDID-TYPE-SAMPLEID Of note is that the library tag ( LB ) does not include the CAPTUREID part, to ensure that PCR duplicates are removed correctly. If a single prepared samples is exposed to capture twice, to create the libraries NA12877-T-49-TD1-TT1 and NA12877-T-49-TD1-TT2 (note different digits in the capture id), read pairs being identical between the two libraries should be considered duplicates since the sample was split after the final PCR step. Therefore, the LB for these libraries is set to NA12877-T-49-TD1 . After merging the bam files, removal of PCR duplicates is done using Picard MarkDuplicates, which will do the right thing.","title":"Quick Start"},{"location":"autoseq/#getting-started","text":"Autoseq is a command line tool and it has multiple pipelines used to analyse cancer genomics data from clinical samples. Piplines Alascca Liqbio (UMI tag processing) RNAseq (under development) Latest version: [v0.6.0][dist] Note: This version of autoseq is specially updated for PROBIO trail on prostate cancer.","title":"Getting Started"},{"location":"autoseq/#installation","text":"Step 1: Clone the alascca-dotfiles repo git clone https://github.com/clinseq/probio-dotfiles.git /nfs/PROBIO/probio-dotfiles Step 2: Go to dotfiles directory cd /nfs/PROBIO/probio-dotfiles Step 3: Run the .bash_profile file to create new environment . /nfs/PROBIO/probio-dotfiles/.bash_profile Step 4: Run this script to install all prerequisites bash install-prereqs.sh","title":"Installation"},{"location":"autoseq/#command-line-usage","text":"Usage: autoseq [OPTIONS] COMMAND [ARGS]... Options: --ref TEXT json with reference files to use --job-params TEXT JSON file specifying various pipeline job parameters. --outdir PATH output directory --libdir TEXT directory to search for libraries --runner_name TEXT Runner to use. --loglevel TEXT level of logging --jobdb TEXT sqlite3 database to write job info and stats --dot_file TEXT write graph to dot file with this name --cores INTEGER max number of cores to allow jobs to use --umi To process the data with UMI- Unique Molecular Identifier --scratch TEXT scratch dir to use --help Show this message and exit. Commands: alascca liqbio liqbio-prepare","title":"Command Line Usage"},{"location":"autoseq/#generate-the-reference-files","text":"generate-ref command can be used to generate all required reference files to run the pipeline. generate-ref --genome-resources /nfs/PROBIO/genome-resources --outdir /nfs/PROBIO/autoseq-genome","title":"Generate the reference files"},{"location":"autoseq/#liqbio-pipeline","text":"autoseq --ref ref.json --outdir /path/to/outdir --jobdb jobdb.json --cores 5 --runner_name slurmrunner --libdir /path/to/libdir liqbio sample.json with UMI --umi flag is used to call UMI pipeline autoseq --ref ref.json --outdir /path/to/outdir --jobdb jobdb.json --cores 5 --runner_name slurmrunner --libdir /path/to/libdir --umi liqbio sample.json","title":"LiqBio Pipeline"},{"location":"autoseq/#general-discriptions","text":"Clinseq barcodes Each sample+preparation+capture item should have a corresponding barcode with the format PROJECT-SDID-TYPE-SAMPLEID-PREPID-CAPTUREID where: PROJECT is a two-letter short project designator. One of AL (alascca), LB (liquid biopspy) and OT (other) SDID is an identifier for a single individual. It must match the pattern P-[a-zA-Z0-9]+ ( NOTE: This necessitates an additional \"-\" within this field). TYPE is the sample type, one of T (tumor), N (normal) and CFDNA (ctDNA) SAMPLEID identifies a single biological sample, for example piece of a tumor or a single tube of plasma. It must match the pattern [a-zA-Z0-9]+ . PREPID specifies the library preparation kit used. It must be a two-letter shortname followed by a string matching [0-9]+ , which can be used to indicate the date on which the prep was performed. The date string should preferably be in the format YYYYMMDDHHMM . For example, 201701241540 would indicate year 2017, January 24th, at 15:40. CAPTUREID specifies the capture that was performed on the library (if any). It must match either WGS (indicating that no capture was performed), or else a two-letter shortname indicating the capture kit used, followed by a string matching [0-9]+ , which can be used to indicate the date on which the capture was performed. The date should preferably be in the format YYYYMMDDHHMM . NOTE: The combination SDID-TYPE-SAMPLEID must uniquely identify a single sample. NOTE: A clinseq barcode is not garuanteed to uniquely specify a single sample+library+capture item, but in practice it should be unique if precise preparation and capture times are included within the PREPID and CAPTUREID fields. Allowed Prep IDs Autoseq know about the following preparation methods: BN = BIOO_NEXTFLEX KH = KAPA_HYPERPREP KP = KAPA_HYPERPLUS TD = THRUPLEX_DNASEQ TP = THRUPLEX_PLASMASEQ TF = THRUPLEX_FD TS = TRUSEQ_RNA NN = NEBNEXT_RNA VI = VILO_RNA Allowed Capture IDs Autoseq knows about the following capture kits: CS = clinseq_v3_targets CZ = clinseq_v4 EX = EXOMEV3 EO = EXOMEV1 RF = fusion_v1 CC = core_design CD = discovery_coho CB = big_design TT = test-regions CM = monitor CP = progression PC = probio_comprehensive PB = probio_biomarker_signature PA = pancancer Newer version of autoseq has three more panels such as PC, PB and PA. Liqbio The sample.json file has the format (right side). In this file, a single tumor and normal sample is allowed, but multiple plasma samples. If no tumor or normal sample is avaialble, they can be set to null , but if no plasma samples are available, it should be set to [] (empty list), for example \"CFDNA\": [] . { sdid : NA12877 , panel : { T : NA12877-T-03098849-TD1-TT1 , N : NA12877-N-03098121-TD1-TT1 , CFDNA : [ NA12877-CFDNA-03098850-TD1-TT1 , NA12877-CFDNA-03098850-TD2-TT1 ] }, wgs : { T : NA12877-T-03098849-TD1-WGS , N : NA12877-N-03098121-TD1-WGS , CFDNA : [ NA12877-CFDNA-03098850-TD1-WGS ] } } For the plasma samples, merging of libraries will take place before calling. On alignment, the @RG tag will be set as follows: ID = SDID-TYPE-SAMPLEID-PREPID-CAPTUREID LB = SDID-TYPE-SAMPLEID-PREPID SM = SDID-TYPE-SAMPLEID Of note is that the library tag ( LB ) does not include the CAPTUREID part, to ensure that PCR duplicates are removed correctly. If a single prepared samples is exposed to capture twice, to create the libraries NA12877-T-49-TD1-TT1 and NA12877-T-49-TD1-TT2 (note different digits in the capture id), read pairs being identical between the two libraries should be considered duplicates since the sample was split after the final PCR step. Therefore, the LB for these libraries is set to NA12877-T-49-TD1 . After merging the bam files, removal of PCR duplicates is done using Picard MarkDuplicates, which will do the right thing.","title":"General Discriptions"},{"location":"pipeline/","text":"LiqBio Non UMI pipeline Preprocessing Trimming and Filtering skewer Mapping to Reference BWA Indel Realignment GATK4 Merge Bams and Markdups Picard Germline Variant Calling SNV and small INDEL GATK4-HaplotypeCaller Strelka Germline Combining Variants GATK3-CombineVariants Somatic Variant Calling SNV and small INDEL VarDict Strelka Somatic GATK4-Mutect2 Varscan Combine and Filter Variants(Ensembl Approach) SomaticSeq Annatotaion VeP-95 Structural Variant Calling SV - INS, INV, DEL, TRA, DUP svcaller - In-house developed svict Lumpy svaba Copy Number Variation CNVkit PureCN in-house developed script liqbio_CNA - for visualization QC Metrics Fastq - Quality Check FastQC Coverage alascca coverage hist coverage qc call hsmetrics Picard OXOG metrics Picard Insert size Picard Sequencing Depth sambamba-depth MSI msisensor mSINGS - Tumor only Contamination Test GATK conEst MultiQC UMI pipeline UMI processing Trimming and Filtering skewer Fastq to Bam fgbio-FastqToBam Alignment of Unmapped Bam - 1 BWA Realignment - 1 GATK4 Group reads by umi fgbio-GroupReadsByUmi Call Duplex Consensus Reads CallDuplexConsensusReads Alignment - 2 BWA Realignment - 2 GATK4 Filter Consensus Reads fgbio-FilterConsensusReads Clip Overlapping Reads fgbio-ClipBam Mark Duplicates (bam file from Realignment-1) Picard Germline Variant Calling (bam file from clip overlapping bam) SNV and small INDEL GATK4-HaplotypeCaller Strelka Germline Combining Variants GATK3-CombineVariants Somatic Variant Calling (bam file from clip overlapping bam) SNV and small INDEL VarDict Strelka Somatic GATK4-Mutect2 Varscan Combine and Filter Variants(Ensembl Approach) SomaticSeq Annatotaion VeP-95 Structural Variant Calling SV - INS, INV, DEL, TRA, DUP svcaller - In-house variant caller based on delly Algorythm svict Lumpy svaba Copy Number Variation CNVkit PureCN in-house developed script liqbio_CNA - for visualization QC Metrics Fastq - Quality Check FastQC Coverage alascca coverage hist coverage qc call hsmetrics Picard OXOG metrics Picard Insert size Picard Sequencing Depth sambamba-depth MSI msisensor mSINGS - Tumor only Contamination Test GATK conEst MultiQC Alascca Alascca pipeline needs to be fixed as per new changes done in clinseq pipeline","title":"Work Flow"},{"location":"pipeline/#liqbio","text":"Non UMI pipeline Preprocessing Trimming and Filtering skewer Mapping to Reference BWA Indel Realignment GATK4 Merge Bams and Markdups Picard Germline Variant Calling SNV and small INDEL GATK4-HaplotypeCaller Strelka Germline Combining Variants GATK3-CombineVariants Somatic Variant Calling SNV and small INDEL VarDict Strelka Somatic GATK4-Mutect2 Varscan Combine and Filter Variants(Ensembl Approach) SomaticSeq Annatotaion VeP-95 Structural Variant Calling SV - INS, INV, DEL, TRA, DUP svcaller - In-house developed svict Lumpy svaba Copy Number Variation CNVkit PureCN in-house developed script liqbio_CNA - for visualization QC Metrics Fastq - Quality Check FastQC Coverage alascca coverage hist coverage qc call hsmetrics Picard OXOG metrics Picard Insert size Picard Sequencing Depth sambamba-depth MSI msisensor mSINGS - Tumor only Contamination Test GATK conEst MultiQC UMI pipeline UMI processing Trimming and Filtering skewer Fastq to Bam fgbio-FastqToBam Alignment of Unmapped Bam - 1 BWA Realignment - 1 GATK4 Group reads by umi fgbio-GroupReadsByUmi Call Duplex Consensus Reads CallDuplexConsensusReads Alignment - 2 BWA Realignment - 2 GATK4 Filter Consensus Reads fgbio-FilterConsensusReads Clip Overlapping Reads fgbio-ClipBam Mark Duplicates (bam file from Realignment-1) Picard Germline Variant Calling (bam file from clip overlapping bam) SNV and small INDEL GATK4-HaplotypeCaller Strelka Germline Combining Variants GATK3-CombineVariants Somatic Variant Calling (bam file from clip overlapping bam) SNV and small INDEL VarDict Strelka Somatic GATK4-Mutect2 Varscan Combine and Filter Variants(Ensembl Approach) SomaticSeq Annatotaion VeP-95 Structural Variant Calling SV - INS, INV, DEL, TRA, DUP svcaller - In-house variant caller based on delly Algorythm svict Lumpy svaba Copy Number Variation CNVkit PureCN in-house developed script liqbio_CNA - for visualization QC Metrics Fastq - Quality Check FastQC Coverage alascca coverage hist coverage qc call hsmetrics Picard OXOG metrics Picard Insert size Picard Sequencing Depth sambamba-depth MSI msisensor mSINGS - Tumor only Contamination Test GATK conEst MultiQC","title":"LiqBio"},{"location":"pipeline/#alascca","text":"Alascca pipeline needs to be fixed as per new changes done in clinseq pipeline","title":"Alascca"},{"location":"about/license/","text":"","title":"License"},{"location":"lld/alascca/","text":"AlasccaPipeline The main class to run Alascca pipeline. class AlasccaPipeline(ClinseqPipeline): def __init__(self, sampledata, refdata, job_params, outdir, libdir, maxcores=1, scratch= /scratch/tmp/tmp/ , referral_db_conf= tests/referrals/referral-db-config.json , addresses= tests/referrals/addresses.csv , **kwargs): ClinseqPipeline.__init__(self, sampledata, refdata, job_params, outdir, libdir, maxcores, scratch, **kwargs) self.referral_db_conf = referral_db_conf self.addresses = addresses self.default_job_params[ vardict-min-num-reads ] = 6 self.default_job_params[ create_alascca_report ] = True # Check to ensure that the sample data is valid for an ALASCCA analysis: self.validate_sample_data_for_alascca() # Remove sample capture items for which data is not available: self.check_sampledata() # Configure alignment and merging of fastq data for all clinseq barcodes: self.configure_align_and_merge() # Configure all panel analyses: self.configure_panel_analyses() # Configure QC of all panel data: self.configure_all_panel_qcs() # Configure ALASCCA report generation: self.configure_alascca_specific_analysis() # Configure fastq QCs: self.configure_fastq_qcs() # Configure MultiQC: self.configure_multi_qc() get_normal_and_tumor_captures Retrieves the unique normal and tumor capture identifiers for this ALASCCA analysis. :return: (normal_capture, tumor_capture) tuple, denoting those unique library captures. def get_normal_and_tumor_captures(self): # There must be exactly one tumor and exactly one normal for this to be valid: if len(self.get_mapped_captures_cancer()) != 1 or \\ len(self.get_mapped_captures_normal()) != 1: raise ValueError( Invalid pipeline state for configuration of ALASCCA CNA. ) normal_capture = self.get_mapped_captures_normal()[0] tumor_capture = self.get_mapped_captures_cancer()[0] return normal_capture, tumor_capture configure_alascca_specific_analysis Configure the Jobs for specific to the ALASCCA pipeline. The panel analyses and panel QC analyses must be configured before this can be configured. def configure_alascca_specific_analysis(self): normal_capture, tumor_capture = self.get_normal_and_tumor_captures() alascca_cna_ouput, alascca_cna_purity = \\ self.configure_alascca_cna(normal_capture, tumor_capture) self.configure_alascca_report_generation( normal_capture, tumor_capture, alascca_cna_ouput, alascca_cna_purity) configure_alascca_cna def configure_alascca_cna(self, normal_capture, tumor_capture): tumor_vs_normal_results = self.normal_cancer_pair_to_results[(normal_capture, tumor_capture)] tumor_results = self.capture_to_results[tumor_capture] alascca_cna = AlasccaCNAPlot() alascca_cna.input_somatic_vcf = tumor_vs_normal_results.vepped_vcf alascca_cna.input_germline_vcf = tumor_vs_normal_results.vcf_addsample_output alascca_cna.input_cnr = tumor_results.cnr alascca_cna.input_cns = tumor_results.cns alascca_cna.chrsizes = self.refdata['chrsizes'] tumor_str = compose_lib_capture_str(tumor_capture) alascca_cna.output_cna = {}/variants/{}-alascca-cna.json .format( self.outdir, tumor_str) alascca_cna.output_purity = {}/variants/{}-alascca-purity.json .format( self.outdir, tumor_str) alascca_cna.output_png = {}/qc/{}-alascca-cna.png .format( self.outdir, tumor_str) alascca_cna.jobname = alascca-cna/{} .format(tumor_str) self.add(alascca_cna) return alascca_cna.output_cna, alascca_cna.output_purity configure_compile_metadata def configure_compile_metadata(self, normal_capture, tumor_capture): blood_barcode = normal_capture.sample_id tumor_barcode = tumor_capture.sample_id metadata_json = {}/report/{}-{}.metadata.json .format(self.outdir, blood_barcode, tumor_barcode) compile_metadata_json = CompileMetadata( self.referral_db_conf, blood_barcode, tumor_barcode, metadata_json, self.addresses) compile_metadata_json.jobname = compile-metadata/{}-{} .format(tumor_barcode, blood_barcode) self.add(compile_metadata_json) return compile_metadata_json.output_json configure_compile_genomic_json def configure_compile_genomic_json(self, normal_capture, tumor_capture, alascca_cna_output, alascca_cna_purity_call): tumor_vs_normal_results = self.normal_cancer_pair_to_results[(normal_capture, tumor_capture)] tumor_results = self.capture_to_results[tumor_capture] normal_results = self.capture_to_results[normal_capture] blood_barcode = normal_capture.sample_id tumor_barcode = tumor_capture.sample_id genomic_json = {}/report/{}-{}.genomic.json .format( self.outdir, blood_barcode, tumor_barcode) compile_genomic_json = CompileAlasccaGenomicJson( input_somatic_vcf=tumor_vs_normal_results.vepped_vcf, input_cn_calls=alascca_cna_output, input_msisensor=tumor_vs_normal_results.msi_output, input_purity_qc=alascca_cna_purity_call, input_contam_qc=tumor_vs_normal_results.cancer_contam_call, input_tcov_qc=tumor_results.cov_qc_call, input_ncov_qc=normal_results.cov_qc_call, output_json=genomic_json) compile_genomic_json.jobname = compile-genomic/{}-{} .format(tumor_barcode, blood_barcode) self.add(compile_genomic_json) return compile_genomic_json.output_json configure_write_alascca_report def configure_write_alascca_report( self, normal_capture, tumor_capture, metadata_json, genomic_json): blood_barcode = normal_capture.sample_id tumor_barcode = tumor_capture.sample_id only_alascca = only_alascca_class_report(blood_barcode, tumor_barcode, self.referral_db_conf) pdf = {}/report/AlasccaReport-{}-{}.pdf .format(self.outdir, blood_barcode, tumor_barcode) write_alascca_pdf = WriteAlasccaReport( genomic_json, metadata_json, pdf, only_alascca) write_alascca_pdf.jobname = writeAlasccaPdf/{}-{} .format(tumor_barcode, blood_barcode) self.add(write_alascca_pdf) configure_alascca_report_generation Configure the generation of the ALASCCA report for this pipeline instance. def configure_alascca_report_generation(self, normal_capture, tumor_capture, alascca_cna_output, alascca_cna_purity_call): # Always create genomic json genomic_json = self.configure_compile_genomic_json( normal_capture, tumor_capture, alascca_cna_output, alascca_cna_purity_call) # Create metadata json and final pdf only if supposed to create_alascca_report = self.get_job_param( create_alascca_report ) if create_alascca_report: metadata_json = self.configure_compile_metadata(normal_capture, tumor_capture) self.configure_write_alascca_report(normal_capture, tumor_capture, metadata_json, genomic_json) get_coverage_bed Retrieve the targets bed file to use for calculating coverage, given the specified targets name. Uses the alascca bed file if available. param targets: Target capture name return: bed file name def get_coverage_bed(self, targets): if 'alascca_targets' in self.refdata['targets']: return self.refdata['targets']['alascca_targets']['targets-bed-slopped20'] else: return self.refdata['targets'][targets]['targets-bed-slopped20']","title":"Alascca"},{"location":"lld/alascca/#alasccapipeline","text":"The main class to run Alascca pipeline. class AlasccaPipeline(ClinseqPipeline): def __init__(self, sampledata, refdata, job_params, outdir, libdir, maxcores=1, scratch= /scratch/tmp/tmp/ , referral_db_conf= tests/referrals/referral-db-config.json , addresses= tests/referrals/addresses.csv , **kwargs): ClinseqPipeline.__init__(self, sampledata, refdata, job_params, outdir, libdir, maxcores, scratch, **kwargs) self.referral_db_conf = referral_db_conf self.addresses = addresses self.default_job_params[ vardict-min-num-reads ] = 6 self.default_job_params[ create_alascca_report ] = True # Check to ensure that the sample data is valid for an ALASCCA analysis: self.validate_sample_data_for_alascca() # Remove sample capture items for which data is not available: self.check_sampledata() # Configure alignment and merging of fastq data for all clinseq barcodes: self.configure_align_and_merge() # Configure all panel analyses: self.configure_panel_analyses() # Configure QC of all panel data: self.configure_all_panel_qcs() # Configure ALASCCA report generation: self.configure_alascca_specific_analysis() # Configure fastq QCs: self.configure_fastq_qcs() # Configure MultiQC: self.configure_multi_qc()","title":"AlasccaPipeline"},{"location":"lld/alascca/#get_normal_and_tumor_captures","text":"Retrieves the unique normal and tumor capture identifiers for this ALASCCA analysis. :return: (normal_capture, tumor_capture) tuple, denoting those unique library captures. def get_normal_and_tumor_captures(self): # There must be exactly one tumor and exactly one normal for this to be valid: if len(self.get_mapped_captures_cancer()) != 1 or \\ len(self.get_mapped_captures_normal()) != 1: raise ValueError( Invalid pipeline state for configuration of ALASCCA CNA. ) normal_capture = self.get_mapped_captures_normal()[0] tumor_capture = self.get_mapped_captures_cancer()[0] return normal_capture, tumor_capture","title":"get_normal_and_tumor_captures"},{"location":"lld/alascca/#configure_alascca_specific_analysis","text":"Configure the Jobs for specific to the ALASCCA pipeline. The panel analyses and panel QC analyses must be configured before this can be configured. def configure_alascca_specific_analysis(self): normal_capture, tumor_capture = self.get_normal_and_tumor_captures() alascca_cna_ouput, alascca_cna_purity = \\ self.configure_alascca_cna(normal_capture, tumor_capture) self.configure_alascca_report_generation( normal_capture, tumor_capture, alascca_cna_ouput, alascca_cna_purity)","title":"configure_alascca_specific_analysis"},{"location":"lld/alascca/#configure_alascca_cna","text":"def configure_alascca_cna(self, normal_capture, tumor_capture): tumor_vs_normal_results = self.normal_cancer_pair_to_results[(normal_capture, tumor_capture)] tumor_results = self.capture_to_results[tumor_capture] alascca_cna = AlasccaCNAPlot() alascca_cna.input_somatic_vcf = tumor_vs_normal_results.vepped_vcf alascca_cna.input_germline_vcf = tumor_vs_normal_results.vcf_addsample_output alascca_cna.input_cnr = tumor_results.cnr alascca_cna.input_cns = tumor_results.cns alascca_cna.chrsizes = self.refdata['chrsizes'] tumor_str = compose_lib_capture_str(tumor_capture) alascca_cna.output_cna = {}/variants/{}-alascca-cna.json .format( self.outdir, tumor_str) alascca_cna.output_purity = {}/variants/{}-alascca-purity.json .format( self.outdir, tumor_str) alascca_cna.output_png = {}/qc/{}-alascca-cna.png .format( self.outdir, tumor_str) alascca_cna.jobname = alascca-cna/{} .format(tumor_str) self.add(alascca_cna) return alascca_cna.output_cna, alascca_cna.output_purity","title":"configure_alascca_cna"},{"location":"lld/alascca/#configure_compile_metadata","text":"def configure_compile_metadata(self, normal_capture, tumor_capture): blood_barcode = normal_capture.sample_id tumor_barcode = tumor_capture.sample_id metadata_json = {}/report/{}-{}.metadata.json .format(self.outdir, blood_barcode, tumor_barcode) compile_metadata_json = CompileMetadata( self.referral_db_conf, blood_barcode, tumor_barcode, metadata_json, self.addresses) compile_metadata_json.jobname = compile-metadata/{}-{} .format(tumor_barcode, blood_barcode) self.add(compile_metadata_json) return compile_metadata_json.output_json","title":"configure_compile_metadata"},{"location":"lld/alascca/#configure_compile_genomic_json","text":"def configure_compile_genomic_json(self, normal_capture, tumor_capture, alascca_cna_output, alascca_cna_purity_call): tumor_vs_normal_results = self.normal_cancer_pair_to_results[(normal_capture, tumor_capture)] tumor_results = self.capture_to_results[tumor_capture] normal_results = self.capture_to_results[normal_capture] blood_barcode = normal_capture.sample_id tumor_barcode = tumor_capture.sample_id genomic_json = {}/report/{}-{}.genomic.json .format( self.outdir, blood_barcode, tumor_barcode) compile_genomic_json = CompileAlasccaGenomicJson( input_somatic_vcf=tumor_vs_normal_results.vepped_vcf, input_cn_calls=alascca_cna_output, input_msisensor=tumor_vs_normal_results.msi_output, input_purity_qc=alascca_cna_purity_call, input_contam_qc=tumor_vs_normal_results.cancer_contam_call, input_tcov_qc=tumor_results.cov_qc_call, input_ncov_qc=normal_results.cov_qc_call, output_json=genomic_json) compile_genomic_json.jobname = compile-genomic/{}-{} .format(tumor_barcode, blood_barcode) self.add(compile_genomic_json) return compile_genomic_json.output_json","title":"configure_compile_genomic_json"},{"location":"lld/alascca/#configure_write_alascca_report","text":"def configure_write_alascca_report( self, normal_capture, tumor_capture, metadata_json, genomic_json): blood_barcode = normal_capture.sample_id tumor_barcode = tumor_capture.sample_id only_alascca = only_alascca_class_report(blood_barcode, tumor_barcode, self.referral_db_conf) pdf = {}/report/AlasccaReport-{}-{}.pdf .format(self.outdir, blood_barcode, tumor_barcode) write_alascca_pdf = WriteAlasccaReport( genomic_json, metadata_json, pdf, only_alascca) write_alascca_pdf.jobname = writeAlasccaPdf/{}-{} .format(tumor_barcode, blood_barcode) self.add(write_alascca_pdf)","title":"configure_write_alascca_report"},{"location":"lld/alascca/#configure_alascca_report_generation","text":"Configure the generation of the ALASCCA report for this pipeline instance. def configure_alascca_report_generation(self, normal_capture, tumor_capture, alascca_cna_output, alascca_cna_purity_call): # Always create genomic json genomic_json = self.configure_compile_genomic_json( normal_capture, tumor_capture, alascca_cna_output, alascca_cna_purity_call) # Create metadata json and final pdf only if supposed to create_alascca_report = self.get_job_param( create_alascca_report ) if create_alascca_report: metadata_json = self.configure_compile_metadata(normal_capture, tumor_capture) self.configure_write_alascca_report(normal_capture, tumor_capture, metadata_json, genomic_json)","title":"configure_alascca_report_generation"},{"location":"lld/alascca/#get_coverage_bed","text":"Retrieve the targets bed file to use for calculating coverage, given the specified targets name. Uses the alascca bed file if available. param targets: Target capture name return: bed file name def get_coverage_bed(self, targets): if 'alascca_targets' in self.refdata['targets']: return self.refdata['targets']['alascca_targets']['targets-bed-slopped20'] else: return self.refdata['targets'][targets]['targets-bed-slopped20']","title":"get_coverage_bed"},{"location":"lld/alignment/","text":"align_library Align fastq files for a PE library param remove_duplicates: param pipeline: param fq1_files: param fq2_files: param lib: param ref: param outdir: param maxcores: return: def align_library(pipeline, fq1_files, fq2_files, clinseq_barcode, ref, outdir, maxcores=1, remove_duplicates=True): if not fq2_files: logging.debug( lib {} is SE .format(clinseq_barcode)) return align_se(pipeline, fq1_files, clinseq_barcode, ref, outdir, maxcores, remove_duplicates) else: logging.debug( lib {} is PE .format(clinseq_barcode)) return align_pe(pipeline, fq1_files, fq2_files, clinseq_barcode, ref, outdir, maxcores, remove_duplicates) align_se Align single end data param pipeline: param fq1_files: param lib: param ref: param outdir: param maxcores: param remove_duplicates: return: bam def align_se(pipeline, fq1_files, clinseq_barcode, ref, outdir, maxcores, remove_duplicates=True): logging.debug( Aligning files: {} .format(fq1_files)) fq1_abs = [normpath(x) for x in fq1_files] fq1_trimmed = [] for fq1 in fq1_abs: skewer = Skewer() skewer.input1 = fq1 skewer.input2 = None skewer.output1 = outdir + /skewer/{} .format(os.path.basename(fq1)) skewer.output2 = outdir + /skewer/unused-dummyfq2-{} .format(os.path.basename(fq1)) skewer.stats = outdir + /skewer/skewer-stats-{}.log .format(os.path.basename(fq1)) skewer.threads = maxcores skewer.jobname = skewer/{} .format(os.path.basename(fq1)) skewer.scratch = pipeline.scratch skewer.is_intermediate = True fq1_trimmed.append(skewer.output1) pipeline.add(skewer) cat1 = Cat() cat1.input = fq1_trimmed cat1.output = outdir + /skewer/{}_1.fastq.gz .format(clinseq_barcode) cat1.jobname = cat/{} .format(clinseq_barcode) cat1.is_intermediate = False pipeline.add(cat1) bwa = Bwa() bwa.input_fastq1 = cat1.output bwa.input_reference_sequence = ref bwa.remove_duplicates = remove_duplicates library_id = parse_prep_id(clinseq_barcode) sample_string = compose_sample_str(extract_unique_capture(clinseq_barcode)) bwa.readgroup = \\ @RG\\\\tID:{rg_id}\\\\tSM:{rg_sm}\\\\tLB:{rg_lb}\\\\tPL:ILLUMINA\\ .format(\\ rg_id=clinseq_barcode, rg_sm=sample_string, rg_lb=library_id) bwa.threads = maxcores bwa.output = {}/{}.bam .format(outdir, clinseq_barcode) bwa.scratch = pipeline.scratch bwa.jobname = bwa/{} .format(clinseq_barcode) bwa.is_intermediate = False pipeline.add(bwa) return bwa.output align_pe align paired end data param pipeline: param fq1_files: param fq2_files: param lib: param ref: param outdir: param maxcores: param remove_duplicates: return: def align_pe(pipeline, fq1_files, fq2_files, clinseq_barcode, ref, outdir, maxcores=1, remove_duplicates=True): fq1_abs = [normpath(x) for x in fq1_files] fq2_abs = [normpath(x) for x in fq2_files] logging.debug( Trimming {} and {} .format(fq1_abs, fq2_abs)) pairs = [(fq1_abs[k], fq2_abs[k]) for k in range(len(fq1_abs))] fq1_trimmed = [] fq2_trimmed = [] for fq1, fq2 in pairs: skewer = Skewer() skewer.input1 = fq1 skewer.input2 = fq2 skewer.output1 = outdir + /skewer/libs/{} .format(os.path.basename(fq1)) skewer.output2 = outdir + /skewer/libs/{} .format(os.path.basename(fq2)) skewer.stats = outdir + /skewer/libs/skewer-stats-{}.log .format(os.path.basename(fq1)) skewer.threads = maxcores skewer.jobname = skewer/{} .format(os.path.basename(fq1)) skewer.scratch = pipeline.scratch skewer.is_intermediate = True fq1_trimmed.append(skewer.output1) fq2_trimmed.append(skewer.output2) pipeline.add(skewer) cat1 = Cat() cat1.input = fq1_trimmed cat1.output = outdir + /skewer/{}-concatenated_1.fastq.gz .format(clinseq_barcode) cat1.jobname = cat1/{} .format(clinseq_barcode) cat1.is_intermediate = True pipeline.add(cat1) cat2 = Cat() cat2.input = fq2_trimmed cat2.jobname = cat2/{} .format(clinseq_barcode) cat2.output = outdir + /skewer/{}-concatenated_2.fastq.gz .format(clinseq_barcode) cat2.is_intermediate = True pipeline.add(cat2) bwa = Bwa() bwa.input_fastq1 = cat1.output bwa.input_fastq2 = cat2.output bwa.input_reference_sequence = ref bwa.remove_duplicates = remove_duplicates library_id = parse_prep_id(clinseq_barcode) sample_string = compose_sample_str(extract_unique_capture(clinseq_barcode)) bwa.readgroup = \\ @RG\\\\tID:{rg_id}\\\\tSM:{rg_sm}\\\\tLB:{rg_lb}\\\\tPL:ILLUMINA\\ .format(\\ rg_id=clinseq_barcode, rg_sm=sample_string, rg_lb=library_id) bwa.threads = maxcores bwa.output = {}/{}.bam .format(outdir, clinseq_barcode) bwa.jobname = bwa/{} .format(clinseq_barcode) bwa.scratch = pipeline.scratch bwa.is_intermediate = False pipeline.add(bwa) return bwa.output fq_trimming return concatinated fastq files def fq_trimming(pipeline, fq1_files, fq2_files, clinseq_barcode, ref, outdir, maxcores=1): fq1_abs = [normpath(x) for x in fq1_files] fq2_abs = [normpath(x) for x in fq2_files] logging.debug( Trimming {} and {} .format(fq1_abs, fq2_abs)) pairs = [(fq1_abs[k], fq2_abs[k]) for k in range(len(fq1_abs))] fq1_trimmed = [] fq2_trimmed = [] for fq1, fq2 in pairs: skewer = Skewer() skewer.input1 = fq1 skewer.input2 = fq2 skewer.output1 = outdir + /skewer/libs/{} .format(os.path.basename(fq1)) skewer.output2 = outdir + /skewer/libs/{} .format(os.path.basename(fq2)) skewer.stats = outdir + /skewer/libs/skewer-stats-{}.log .format(os.path.basename(fq1)) skewer.threads = maxcores skewer.jobname = skewer/{} .format(os.path.basename(fq1)) skewer.scratch = pipeline.scratch skewer.is_intermediate = True fq1_trimmed.append(skewer.output1) fq2_trimmed.append(skewer.output2) pipeline.add(skewer) cat1 = Cat() cat1.input = fq1_trimmed cat1.output = outdir + /skewer/{}-concatenated_1.fastq.gz .format(clinseq_barcode) cat1.jobname = cat1/{} .format(clinseq_barcode) cat1.is_intermediate = True pipeline.add(cat1) cat2 = Cat() cat2.input = fq2_trimmed cat2.jobname = cat2/{} .format(clinseq_barcode) cat2.output = outdir + /skewer/{}-concatenated_2.fastq.gz .format(clinseq_barcode) cat2.is_intermediate = True pipeline.add(cat2) return cat1.output, cat2.output Bwa class Bwa(Job): def __init__(self): Job.__init__(self) self.input_fastq1 = None # input ports must start with input self.input_fastq2 = None self.input_reference_sequence = None self.mark_secondary = None self.remove_duplicates = True self.readgroup = None self.output = None # output ports must start with output , can be output_metrics , output , etc self.duplication_metrics = None self.jobname = bwa def command(self): bwalog = self.output + .bwa.log samblasterlog = self.output + .samblaster.log tmpprefix = {}/{} .format(self.scratch, uuid.uuid4()) return bwa mem -M -v 1 + \\ required( -R , self.readgroup) + \\ optional( -t , self.threads) + \\ required( , self.input_reference_sequence) + \\ required( , self.input_fastq1) + \\ optional( , self.input_fastq2) + \\ required( 2 , bwalog) + \\ | samblaster -M --addMateTags + \\ conditional(self.remove_duplicates, --removeDups ) + \\ optional( --metricsFile , self.duplication_metrics) + \\ required( 2 , samblasterlog) + \\ | samtools view -Sb -u - + \\ | samtools sort + \\ required( -T , tmpprefix) + \\ optional( -@ , self.threads) + \\ required( -o , self.output) + \\ - + \\ samtools index + self.output + \\ cat {} {} .format(bwalog, samblasterlog) + \\ rm {} {} .format(bwalog, samblasterlog) Skewer class Skewer(Job): def __init__(self): Job.__init__(self) self.input1 = None self.input2 = None self.output1 = None self.output2 = None self.stats = None self.jobname = skewer def command(self): if not self.output1.endswith( .gz ) or not self.output2.endswith( .gz ): raise ValueError( Output files need to end with .gz ) tmpdir = os.path.join(self.scratch, skewer- + str(uuid.uuid4())) prefix = {}/skewer .format(tmpdir) if self.input2: out_fq1 = prefix + -trimmed-pair1.fastq.gz out_fq2 = prefix + -trimmed-pair2.fastq.gz else: out_fq1 = prefix + -trimmed.fastq.gz out_fq2 = out_stats = prefix + -trimmed.log mkdir_cmd = mkdir -p {} .format(tmpdir) skewer_cmd = skewer -z + \\ optional( -t , self.threads) + --quiet + \\ required( -o , prefix) + \\ required( , self.input1) + \\ optional( , self.input2) copy_output_cmd = cp + out_fq1 + + self.output1 + \\ conditional(self.input2, cp + out_fq2 + + self.output2) copy_stats_cmd = cp + out_stats + + self.stats rm_cmd = rm -r {} .format(tmpdir) return .join([mkdir_cmd, skewer_cmd, copy_output_cmd, copy_stats_cmd, rm_cmd])","title":"Alignment"},{"location":"lld/alignment/#align_library","text":"Align fastq files for a PE library param remove_duplicates: param pipeline: param fq1_files: param fq2_files: param lib: param ref: param outdir: param maxcores: return: def align_library(pipeline, fq1_files, fq2_files, clinseq_barcode, ref, outdir, maxcores=1, remove_duplicates=True): if not fq2_files: logging.debug( lib {} is SE .format(clinseq_barcode)) return align_se(pipeline, fq1_files, clinseq_barcode, ref, outdir, maxcores, remove_duplicates) else: logging.debug( lib {} is PE .format(clinseq_barcode)) return align_pe(pipeline, fq1_files, fq2_files, clinseq_barcode, ref, outdir, maxcores, remove_duplicates)","title":"align_library"},{"location":"lld/alignment/#align_se","text":"Align single end data param pipeline: param fq1_files: param lib: param ref: param outdir: param maxcores: param remove_duplicates: return: bam def align_se(pipeline, fq1_files, clinseq_barcode, ref, outdir, maxcores, remove_duplicates=True): logging.debug( Aligning files: {} .format(fq1_files)) fq1_abs = [normpath(x) for x in fq1_files] fq1_trimmed = [] for fq1 in fq1_abs: skewer = Skewer() skewer.input1 = fq1 skewer.input2 = None skewer.output1 = outdir + /skewer/{} .format(os.path.basename(fq1)) skewer.output2 = outdir + /skewer/unused-dummyfq2-{} .format(os.path.basename(fq1)) skewer.stats = outdir + /skewer/skewer-stats-{}.log .format(os.path.basename(fq1)) skewer.threads = maxcores skewer.jobname = skewer/{} .format(os.path.basename(fq1)) skewer.scratch = pipeline.scratch skewer.is_intermediate = True fq1_trimmed.append(skewer.output1) pipeline.add(skewer) cat1 = Cat() cat1.input = fq1_trimmed cat1.output = outdir + /skewer/{}_1.fastq.gz .format(clinseq_barcode) cat1.jobname = cat/{} .format(clinseq_barcode) cat1.is_intermediate = False pipeline.add(cat1) bwa = Bwa() bwa.input_fastq1 = cat1.output bwa.input_reference_sequence = ref bwa.remove_duplicates = remove_duplicates library_id = parse_prep_id(clinseq_barcode) sample_string = compose_sample_str(extract_unique_capture(clinseq_barcode)) bwa.readgroup = \\ @RG\\\\tID:{rg_id}\\\\tSM:{rg_sm}\\\\tLB:{rg_lb}\\\\tPL:ILLUMINA\\ .format(\\ rg_id=clinseq_barcode, rg_sm=sample_string, rg_lb=library_id) bwa.threads = maxcores bwa.output = {}/{}.bam .format(outdir, clinseq_barcode) bwa.scratch = pipeline.scratch bwa.jobname = bwa/{} .format(clinseq_barcode) bwa.is_intermediate = False pipeline.add(bwa) return bwa.output","title":"align_se"},{"location":"lld/alignment/#align_pe","text":"align paired end data param pipeline: param fq1_files: param fq2_files: param lib: param ref: param outdir: param maxcores: param remove_duplicates: return: def align_pe(pipeline, fq1_files, fq2_files, clinseq_barcode, ref, outdir, maxcores=1, remove_duplicates=True): fq1_abs = [normpath(x) for x in fq1_files] fq2_abs = [normpath(x) for x in fq2_files] logging.debug( Trimming {} and {} .format(fq1_abs, fq2_abs)) pairs = [(fq1_abs[k], fq2_abs[k]) for k in range(len(fq1_abs))] fq1_trimmed = [] fq2_trimmed = [] for fq1, fq2 in pairs: skewer = Skewer() skewer.input1 = fq1 skewer.input2 = fq2 skewer.output1 = outdir + /skewer/libs/{} .format(os.path.basename(fq1)) skewer.output2 = outdir + /skewer/libs/{} .format(os.path.basename(fq2)) skewer.stats = outdir + /skewer/libs/skewer-stats-{}.log .format(os.path.basename(fq1)) skewer.threads = maxcores skewer.jobname = skewer/{} .format(os.path.basename(fq1)) skewer.scratch = pipeline.scratch skewer.is_intermediate = True fq1_trimmed.append(skewer.output1) fq2_trimmed.append(skewer.output2) pipeline.add(skewer) cat1 = Cat() cat1.input = fq1_trimmed cat1.output = outdir + /skewer/{}-concatenated_1.fastq.gz .format(clinseq_barcode) cat1.jobname = cat1/{} .format(clinseq_barcode) cat1.is_intermediate = True pipeline.add(cat1) cat2 = Cat() cat2.input = fq2_trimmed cat2.jobname = cat2/{} .format(clinseq_barcode) cat2.output = outdir + /skewer/{}-concatenated_2.fastq.gz .format(clinseq_barcode) cat2.is_intermediate = True pipeline.add(cat2) bwa = Bwa() bwa.input_fastq1 = cat1.output bwa.input_fastq2 = cat2.output bwa.input_reference_sequence = ref bwa.remove_duplicates = remove_duplicates library_id = parse_prep_id(clinseq_barcode) sample_string = compose_sample_str(extract_unique_capture(clinseq_barcode)) bwa.readgroup = \\ @RG\\\\tID:{rg_id}\\\\tSM:{rg_sm}\\\\tLB:{rg_lb}\\\\tPL:ILLUMINA\\ .format(\\ rg_id=clinseq_barcode, rg_sm=sample_string, rg_lb=library_id) bwa.threads = maxcores bwa.output = {}/{}.bam .format(outdir, clinseq_barcode) bwa.jobname = bwa/{} .format(clinseq_barcode) bwa.scratch = pipeline.scratch bwa.is_intermediate = False pipeline.add(bwa) return bwa.output","title":"align_pe"},{"location":"lld/alignment/#fq_trimming","text":"return concatinated fastq files def fq_trimming(pipeline, fq1_files, fq2_files, clinseq_barcode, ref, outdir, maxcores=1): fq1_abs = [normpath(x) for x in fq1_files] fq2_abs = [normpath(x) for x in fq2_files] logging.debug( Trimming {} and {} .format(fq1_abs, fq2_abs)) pairs = [(fq1_abs[k], fq2_abs[k]) for k in range(len(fq1_abs))] fq1_trimmed = [] fq2_trimmed = [] for fq1, fq2 in pairs: skewer = Skewer() skewer.input1 = fq1 skewer.input2 = fq2 skewer.output1 = outdir + /skewer/libs/{} .format(os.path.basename(fq1)) skewer.output2 = outdir + /skewer/libs/{} .format(os.path.basename(fq2)) skewer.stats = outdir + /skewer/libs/skewer-stats-{}.log .format(os.path.basename(fq1)) skewer.threads = maxcores skewer.jobname = skewer/{} .format(os.path.basename(fq1)) skewer.scratch = pipeline.scratch skewer.is_intermediate = True fq1_trimmed.append(skewer.output1) fq2_trimmed.append(skewer.output2) pipeline.add(skewer) cat1 = Cat() cat1.input = fq1_trimmed cat1.output = outdir + /skewer/{}-concatenated_1.fastq.gz .format(clinseq_barcode) cat1.jobname = cat1/{} .format(clinseq_barcode) cat1.is_intermediate = True pipeline.add(cat1) cat2 = Cat() cat2.input = fq2_trimmed cat2.jobname = cat2/{} .format(clinseq_barcode) cat2.output = outdir + /skewer/{}-concatenated_2.fastq.gz .format(clinseq_barcode) cat2.is_intermediate = True pipeline.add(cat2) return cat1.output, cat2.output","title":"fq_trimming"},{"location":"lld/alignment/#bwa","text":"class Bwa(Job): def __init__(self): Job.__init__(self) self.input_fastq1 = None # input ports must start with input self.input_fastq2 = None self.input_reference_sequence = None self.mark_secondary = None self.remove_duplicates = True self.readgroup = None self.output = None # output ports must start with output , can be output_metrics , output , etc self.duplication_metrics = None self.jobname = bwa def command(self): bwalog = self.output + .bwa.log samblasterlog = self.output + .samblaster.log tmpprefix = {}/{} .format(self.scratch, uuid.uuid4()) return bwa mem -M -v 1 + \\ required( -R , self.readgroup) + \\ optional( -t , self.threads) + \\ required( , self.input_reference_sequence) + \\ required( , self.input_fastq1) + \\ optional( , self.input_fastq2) + \\ required( 2 , bwalog) + \\ | samblaster -M --addMateTags + \\ conditional(self.remove_duplicates, --removeDups ) + \\ optional( --metricsFile , self.duplication_metrics) + \\ required( 2 , samblasterlog) + \\ | samtools view -Sb -u - + \\ | samtools sort + \\ required( -T , tmpprefix) + \\ optional( -@ , self.threads) + \\ required( -o , self.output) + \\ - + \\ samtools index + self.output + \\ cat {} {} .format(bwalog, samblasterlog) + \\ rm {} {} .format(bwalog, samblasterlog)","title":"Bwa"},{"location":"lld/alignment/#skewer","text":"class Skewer(Job): def __init__(self): Job.__init__(self) self.input1 = None self.input2 = None self.output1 = None self.output2 = None self.stats = None self.jobname = skewer def command(self): if not self.output1.endswith( .gz ) or not self.output2.endswith( .gz ): raise ValueError( Output files need to end with .gz ) tmpdir = os.path.join(self.scratch, skewer- + str(uuid.uuid4())) prefix = {}/skewer .format(tmpdir) if self.input2: out_fq1 = prefix + -trimmed-pair1.fastq.gz out_fq2 = prefix + -trimmed-pair2.fastq.gz else: out_fq1 = prefix + -trimmed.fastq.gz out_fq2 = out_stats = prefix + -trimmed.log mkdir_cmd = mkdir -p {} .format(tmpdir) skewer_cmd = skewer -z + \\ optional( -t , self.threads) + --quiet + \\ required( -o , prefix) + \\ required( , self.input1) + \\ optional( , self.input2) copy_output_cmd = cp + out_fq1 + + self.output1 + \\ conditional(self.input2, cp + out_fq2 + + self.output2) copy_stats_cmd = cp + out_stats + + self.stats rm_cmd = rm -r {} .format(tmpdir) return .join([mkdir_cmd, skewer_cmd, copy_output_cmd, copy_stats_cmd, rm_cmd])","title":"Skewer"},{"location":"lld/cli/","text":"CLI - Options and Commands Params can be used to run the pipeline --ref TEXT json with reference files to us e --job-params TEXT JSON file specifying various pipeline job parameters. --outdir PATH output directory --libdir TEXT directory to search for libraries --runner_name TEXT Runner to use. --loglevel TEXT level of logging --jobdb TEXT sqlite3 database to write job info and stats --dot_file TEXT write graph to dot file with this name --cores INTEGER max number of cores to allow jobs to use --scratch TEXT scratch dir to use --help Show this message and exit. @click.group() @click.option('--ref', default='/nfs/ALASCCA/autoseq-genome/autoseq-genome.json', help='json with reference files to use', type=str) @click.option('--job-params', default=None, help='JSON file specifying various pipeline job ' + \\ 'parameters.', type=str) @click.option('--outdir', default='/tmp/autoseq-test', help='output directory', type=click.Path()) @click.option('--libdir', default= /tmp , help= directory to search for libraries ) @click.option('--runner_name', default='shellrunner', help='Runner to use.') @click.option('--loglevel', default='INFO', help='level of logging') @click.option('--jobdb', default=None, help= sqlite3 database to write job info and stats ) @click.option('--dot_file', default=None, help= write graph to dot file with this name ) @click.option('--cores', default=1, help= max number of cores to allow jobs to use ) @click.option('--umi', is_flag=True, help= To process the data with UMI- Unique Molecular Identifier ) @click.option('--scratch', default= /tmp , help= scratch dir to use ) @click.pass_context def cli(ctx, ref, job_params, outdir, libdir, runner_name, loglevel, jobdb, dot_file, cores, umi, scratch): setup_logging(loglevel) logging.debug( Reading reference data from {} .format(ref)) ctx.obj = {} ctx.obj['refdata'] = load_ref(ref) ctx.obj['job_params'] = load_job_params(job_params) ctx.obj['outdir'] = outdir ctx.obj['libdir'] = libdir ctx.obj['pipeline'] = None ctx.obj['runner'] = get_runner(runner_name, cores) ctx.obj['jobdb'] = jobdb ctx.obj['dot_file'] = dot_file ctx.obj['cores'] = cores ctx.obj['umi'] = umi ctx.obj['scratch'] = scratch def capture_sigint(sig, frame): Capture ctrl-c (or SIGINT sent in other ways). 1. update remote log :param sig: :param frame: :return: try: ctx.obj['pipeline'].stop() logging.info( Stopping jobs... ) except AttributeError: logging.debug( No pipeline to stop. ) signal.signal(signal.SIGINT, capture_sigint) signal.signal(signal.SIGTERM, capture_sigint) load job params Return an empty dictionary if no job parameters file is specified, which will result in default job parameters being applied: def load_job_params(job_params_filename): if job_params_filename: return json.load(open(job_params_filename)) else: return {} convert to absolute path Convert the input potential relative file path to an absolute path by prepending the specified base_path, but only if the resulting absolute path points to a pre-existing file or directory. If the base_path cannot be prepended, then simply return the original input value. possible_relative_path : A string potentially indicating a relative file/directory path. base_path : The base path to prepend. return : Modified path string. def convert_to_absolute_path(possible_relative_path, base_path): converted_value = possible_relative_path try: if not os.path.isabs(possible_relative_path): joined_path = os.path.join(base_path, possible_relative_path) if os.path.isfile(joined_path) or os.path.isdir(joined_path): converted_value = joined_path except Exception, e: pass return converted_value Make paths absolute Processes the input dictionary, converting relative file paths to absolute file paths throughout the dictionary structure. Specifically, for each value in the dictionary: If it is also a dictionary, then recursively apply this function, replacing the initial dictionary. Otherwise: If the value is a non-null string that is not already an absolute path, then try prepending the specified base_path and see if the resulting file name exists, and in that case then replace the string with the resulting absolute path. def make_paths_absolute(input_dict, base_path): for curr_key, curr_value in input_dict.items(): if isinstance(curr_value, dict): input_dict[curr_key] = make_paths_absolute(curr_value, base_path) else: converted_value = convert_to_absolute_path(curr_value, base_path) input_dict[curr_key] = converted_value return input_dict load reference Processes the input genomic reference data JSON file, converting relative file paths to absolute paths where required. ref Input reference file configuration JSON file. return Modified reference file dictionary with relative- absolute file path conversions performed. autoseq-genome.json file which is generated by generate-ref command def load_ref(ref): basepath = os.path.dirname(ref) with open(ref, 'r') as fh: refjson = json.load(fh) refjson_abs = make_paths_absolute(refjson, basepath) return refjson_abs get runner The runner module imported from pypedream. Here default runner is shellrunner. Based on the environment setup, It will return the runner. def get_runner(runner_name, maxcores): try: module = __import__( pypedream.runners. + runner_name, fromlist= runners ) runner_class = getattr(module, runner_name.title()) if runner_name == 'localqrunner': runner = runner_class(maxcores) else: runner = runner_class() return runner except ImportError: print Couldn't find runner + runner_name + . Available Runners: import inspect for name, obj in inspect.getmembers(runners): if name != runner and runner in name: print - + name raise ImportError setup logging Set up logging * loglevel loglevel to use, one of ERROR, WARNING, DEBUG, INFO (default INFO) def setup_logging(loglevel= INFO ): numeric_level = getattr(logging, loglevel.upper(), None) if not isinstance(numeric_level, int): raise ValueError('Invalid log level: %s' % loglevel) logging.basicConfig(level=numeric_level, format='%(levelname)s %(asctime)s %(funcName)s - %(message)s') logging.info( Started log with loglevel %(loglevel)s % { loglevel : loglevel}) Commands alascca - call the alascca pipeline liqbio - call the liqbio pipeline liqbio_prepare - generate the sample json file based on clinseq barcodes cli.add_command(alascca_cmd) cli.add_command(liqbio_cmd) cli.add_command(liqbio_prepare_cmd) alascca Alascca Pipline @click.command() @click.argument('sample', type=click.File('r')) @click.pass_context def alascca(ctx, sample): logging.info( Running Alascca pipeline ) logging.info( sample is {} .format(sample)) logging.debug( Reading sample config from {} .format(sample)) sampledata = json.load(sample) if ctx.obj['jobdb']: mkdir(os.path.dirname(ctx.obj['jobdb'])) ctx.obj['pipeline'] = AlasccaPipeline(sampledata=sampledata, refdata=ctx.obj['refdata'], job_params=ctx.obj['job_params'], outdir=ctx.obj['outdir'], libdir=ctx.obj['libdir'], maxcores=ctx.obj['cores'], runner=ctx.obj['runner'], jobdb=ctx.obj['jobdb'], dot_file=ctx.obj['dot_file'], scratch=ctx.obj['scratch'] ) # start main analysis ctx.obj['pipeline'].start() logging.info( Waiting for AlasccaPipeline to finish. ) while ctx.obj['pipeline'].is_alive(): logging.debug( Waiting for AutoseqPipeline ) time.sleep(5) # return_code from run_pipeline() will be != 0 if the pipeline fails sys.exit(ctx.obj['pipeline'].exitcode) liqbio @click.command() @click.argument('sample', type=click.File('r')) @click.pass_context def liqbio(ctx, sample): logging.info( Running Liquid Biopsy pipeline ) logging.info( Sample is {} .format(sample)) logging.debug( Reading sample config from {} .format(sample)) sampledata = json.load(sample) if ctx.obj['jobdb']: mkdir(os.path.dirname(ctx.obj['jobdb'])) ctx.obj['pipeline'] = LiqBioPipeline(sampledata=sampledata, refdata=ctx.obj['refdata'], job_params=ctx.obj['job_params'], outdir=ctx.obj['outdir'], libdir=ctx.obj['libdir'], maxcores=ctx.obj['cores'], runner=ctx.obj['runner'], jobdb=ctx.obj['jobdb'], dot_file=ctx.obj['dot_file'], umi=ctx.obj['umi'], scratch=ctx.obj['scratch']) # start main analysis ctx.obj['pipeline'].start() # logging.info( Waiting for pipeline to finish. ) while ctx.obj['pipeline'].is_alive(): logging.debug( Waiting for LiqBioPipeline ) time.sleep(5) # # return_code from run_pipeline() will be != 0 if the pipeline fails sys.exit(ctx.obj['pipeline'].exitcode) liqbio_prepare It will read the input file with clinseq barcodes (txt or xlxs) and return the json file for analysis output: sample-id.json file @click.command() @click.option('--outdir', required=True, help= directory to write config files ) @click.argument('barcodes-filename', type=str) @click.pass_context def liqbio_prepare(ctx, outdir, barcodes_filename): logging.info( Extracting clinseq barcodes from input file: + barcodes_filename) clinseq_barcodes = extract_clinseq_barcodes(barcodes_filename) logging.info( Validating all clinseq barcodes. ) validate_clinseq_barcodes(clinseq_barcodes) logging.info( Generating sample dictionary from the input clinseq barcode strings. ) sample_dict = convert_barcodes_to_sampledict(clinseq_barcodes) for sdid in sample_dict: fn = {}/{}.json .format(outdir, sdid) with open(fn, 'w') as f: json.dump(sample_dict[sdid], f, sort_keys=True, indent=4)","title":"Cli"},{"location":"lld/cli/#cli-options-and-commands","text":"Params can be used to run the pipeline --ref TEXT json with reference files to us e --job-params TEXT JSON file specifying various pipeline job parameters. --outdir PATH output directory --libdir TEXT directory to search for libraries --runner_name TEXT Runner to use. --loglevel TEXT level of logging --jobdb TEXT sqlite3 database to write job info and stats --dot_file TEXT write graph to dot file with this name --cores INTEGER max number of cores to allow jobs to use --scratch TEXT scratch dir to use --help Show this message and exit. @click.group() @click.option('--ref', default='/nfs/ALASCCA/autoseq-genome/autoseq-genome.json', help='json with reference files to use', type=str) @click.option('--job-params', default=None, help='JSON file specifying various pipeline job ' + \\ 'parameters.', type=str) @click.option('--outdir', default='/tmp/autoseq-test', help='output directory', type=click.Path()) @click.option('--libdir', default= /tmp , help= directory to search for libraries ) @click.option('--runner_name', default='shellrunner', help='Runner to use.') @click.option('--loglevel', default='INFO', help='level of logging') @click.option('--jobdb', default=None, help= sqlite3 database to write job info and stats ) @click.option('--dot_file', default=None, help= write graph to dot file with this name ) @click.option('--cores', default=1, help= max number of cores to allow jobs to use ) @click.option('--umi', is_flag=True, help= To process the data with UMI- Unique Molecular Identifier ) @click.option('--scratch', default= /tmp , help= scratch dir to use ) @click.pass_context def cli(ctx, ref, job_params, outdir, libdir, runner_name, loglevel, jobdb, dot_file, cores, umi, scratch): setup_logging(loglevel) logging.debug( Reading reference data from {} .format(ref)) ctx.obj = {} ctx.obj['refdata'] = load_ref(ref) ctx.obj['job_params'] = load_job_params(job_params) ctx.obj['outdir'] = outdir ctx.obj['libdir'] = libdir ctx.obj['pipeline'] = None ctx.obj['runner'] = get_runner(runner_name, cores) ctx.obj['jobdb'] = jobdb ctx.obj['dot_file'] = dot_file ctx.obj['cores'] = cores ctx.obj['umi'] = umi ctx.obj['scratch'] = scratch def capture_sigint(sig, frame): Capture ctrl-c (or SIGINT sent in other ways). 1. update remote log :param sig: :param frame: :return: try: ctx.obj['pipeline'].stop() logging.info( Stopping jobs... ) except AttributeError: logging.debug( No pipeline to stop. ) signal.signal(signal.SIGINT, capture_sigint) signal.signal(signal.SIGTERM, capture_sigint)","title":"CLI - Options and Commands"},{"location":"lld/cli/#load-job-params","text":"Return an empty dictionary if no job parameters file is specified, which will result in default job parameters being applied: def load_job_params(job_params_filename): if job_params_filename: return json.load(open(job_params_filename)) else: return {}","title":"load job params"},{"location":"lld/cli/#convert-to-absolute-path","text":"Convert the input potential relative file path to an absolute path by prepending the specified base_path, but only if the resulting absolute path points to a pre-existing file or directory. If the base_path cannot be prepended, then simply return the original input value. possible_relative_path : A string potentially indicating a relative file/directory path. base_path : The base path to prepend. return : Modified path string. def convert_to_absolute_path(possible_relative_path, base_path): converted_value = possible_relative_path try: if not os.path.isabs(possible_relative_path): joined_path = os.path.join(base_path, possible_relative_path) if os.path.isfile(joined_path) or os.path.isdir(joined_path): converted_value = joined_path except Exception, e: pass return converted_value","title":"convert to absolute path"},{"location":"lld/cli/#make-paths-absolute","text":"Processes the input dictionary, converting relative file paths to absolute file paths throughout the dictionary structure. Specifically, for each value in the dictionary: If it is also a dictionary, then recursively apply this function, replacing the initial dictionary. Otherwise: If the value is a non-null string that is not already an absolute path, then try prepending the specified base_path and see if the resulting file name exists, and in that case then replace the string with the resulting absolute path. def make_paths_absolute(input_dict, base_path): for curr_key, curr_value in input_dict.items(): if isinstance(curr_value, dict): input_dict[curr_key] = make_paths_absolute(curr_value, base_path) else: converted_value = convert_to_absolute_path(curr_value, base_path) input_dict[curr_key] = converted_value return input_dict","title":"Make paths absolute"},{"location":"lld/cli/#load-reference","text":"Processes the input genomic reference data JSON file, converting relative file paths to absolute paths where required. ref Input reference file configuration JSON file. return Modified reference file dictionary with relative- absolute file path conversions performed. autoseq-genome.json file which is generated by generate-ref command def load_ref(ref): basepath = os.path.dirname(ref) with open(ref, 'r') as fh: refjson = json.load(fh) refjson_abs = make_paths_absolute(refjson, basepath) return refjson_abs","title":"load reference"},{"location":"lld/cli/#get-runner","text":"The runner module imported from pypedream. Here default runner is shellrunner. Based on the environment setup, It will return the runner. def get_runner(runner_name, maxcores): try: module = __import__( pypedream.runners. + runner_name, fromlist= runners ) runner_class = getattr(module, runner_name.title()) if runner_name == 'localqrunner': runner = runner_class(maxcores) else: runner = runner_class() return runner except ImportError: print Couldn't find runner + runner_name + . Available Runners: import inspect for name, obj in inspect.getmembers(runners): if name != runner and runner in name: print - + name raise ImportError","title":"get runner"},{"location":"lld/cli/#setup-logging","text":"Set up logging * loglevel loglevel to use, one of ERROR, WARNING, DEBUG, INFO (default INFO) def setup_logging(loglevel= INFO ): numeric_level = getattr(logging, loglevel.upper(), None) if not isinstance(numeric_level, int): raise ValueError('Invalid log level: %s' % loglevel) logging.basicConfig(level=numeric_level, format='%(levelname)s %(asctime)s %(funcName)s - %(message)s') logging.info( Started log with loglevel %(loglevel)s % { loglevel : loglevel})","title":"setup logging"},{"location":"lld/cli/#commands","text":"alascca - call the alascca pipeline liqbio - call the liqbio pipeline liqbio_prepare - generate the sample json file based on clinseq barcodes cli.add_command(alascca_cmd) cli.add_command(liqbio_cmd) cli.add_command(liqbio_prepare_cmd)","title":"Commands"},{"location":"lld/cli/#alascca","text":"Alascca Pipline @click.command() @click.argument('sample', type=click.File('r')) @click.pass_context def alascca(ctx, sample): logging.info( Running Alascca pipeline ) logging.info( sample is {} .format(sample)) logging.debug( Reading sample config from {} .format(sample)) sampledata = json.load(sample) if ctx.obj['jobdb']: mkdir(os.path.dirname(ctx.obj['jobdb'])) ctx.obj['pipeline'] = AlasccaPipeline(sampledata=sampledata, refdata=ctx.obj['refdata'], job_params=ctx.obj['job_params'], outdir=ctx.obj['outdir'], libdir=ctx.obj['libdir'], maxcores=ctx.obj['cores'], runner=ctx.obj['runner'], jobdb=ctx.obj['jobdb'], dot_file=ctx.obj['dot_file'], scratch=ctx.obj['scratch'] ) # start main analysis ctx.obj['pipeline'].start() logging.info( Waiting for AlasccaPipeline to finish. ) while ctx.obj['pipeline'].is_alive(): logging.debug( Waiting for AutoseqPipeline ) time.sleep(5) # return_code from run_pipeline() will be != 0 if the pipeline fails sys.exit(ctx.obj['pipeline'].exitcode)","title":"alascca"},{"location":"lld/cli/#liqbio","text":"@click.command() @click.argument('sample', type=click.File('r')) @click.pass_context def liqbio(ctx, sample): logging.info( Running Liquid Biopsy pipeline ) logging.info( Sample is {} .format(sample)) logging.debug( Reading sample config from {} .format(sample)) sampledata = json.load(sample) if ctx.obj['jobdb']: mkdir(os.path.dirname(ctx.obj['jobdb'])) ctx.obj['pipeline'] = LiqBioPipeline(sampledata=sampledata, refdata=ctx.obj['refdata'], job_params=ctx.obj['job_params'], outdir=ctx.obj['outdir'], libdir=ctx.obj['libdir'], maxcores=ctx.obj['cores'], runner=ctx.obj['runner'], jobdb=ctx.obj['jobdb'], dot_file=ctx.obj['dot_file'], umi=ctx.obj['umi'], scratch=ctx.obj['scratch']) # start main analysis ctx.obj['pipeline'].start() # logging.info( Waiting for pipeline to finish. ) while ctx.obj['pipeline'].is_alive(): logging.debug( Waiting for LiqBioPipeline ) time.sleep(5) # # return_code from run_pipeline() will be != 0 if the pipeline fails sys.exit(ctx.obj['pipeline'].exitcode)","title":"liqbio"},{"location":"lld/cli/#liqbio_prepare","text":"It will read the input file with clinseq barcodes (txt or xlxs) and return the json file for analysis output: sample-id.json file @click.command() @click.option('--outdir', required=True, help= directory to write config files ) @click.argument('barcodes-filename', type=str) @click.pass_context def liqbio_prepare(ctx, outdir, barcodes_filename): logging.info( Extracting clinseq barcodes from input file: + barcodes_filename) clinseq_barcodes = extract_clinseq_barcodes(barcodes_filename) logging.info( Validating all clinseq barcodes. ) validate_clinseq_barcodes(clinseq_barcodes) logging.info( Generating sample dictionary from the input clinseq barcode strings. ) sample_dict = convert_barcodes_to_sampledict(clinseq_barcodes) for sdid in sample_dict: fn = {}/{}.json .format(outdir, sdid) with open(fn, 'w') as f: json.dump(sample_dict[sdid], f, sort_keys=True, indent=4)","title":"liqbio_prepare"},{"location":"lld/clinseq/","text":"ClinseqPipeline A pipeline for processing clinseq cancer genomics. sampledata: A dictionary specifying the clinseq barcodes of samples of different types. refdata: A dictionary specifying the reference data used for configuring the pipeline jobs. outdir: Output folder location string. libdir: String specifying location of the library fastq files. maxcores: Maximum number of cores to use concurrently in this analysis. scratch: String indicating folder in which jobs should output all temporary files. kwargs: Additional key-word arguments. ClinseqPipeline is backbone of autoseq, most of the funtions and file manipulations are written with in this class. class ClinseqPipeline(PypedreamPipeline): def __init__(self, sampledata, refdata, job_params, outdir, libdir, maxcores=1, scratch= /scratch/tmp/tmp , analysis_id=None, **kwargs): PypedreamPipeline.__init__(self, normpath(outdir), **kwargs) self.sampledata = sampledata self.refdata = refdata # FIXME: Introduced a simple dictionary for configuring various pipeline job parameters. # However, this solution is still not elegant: self.job_params = job_params self.maxcores = maxcores self.libdir = libdir self.qc_files = [] self.scratch = scratch self.analysis_id = analysis_id # Set up default job parameters: self.default_job_params = { cov-high-thresh-fraction : 0.95, cov-high-thresh-fold-cov : 100, cov-low-thresh-fraction : 0.95, cov-low-thresh-fold-cov : 50, vardict-min-alt-frac : 0.02, vardict-min-num-reads : None, vep-additional-options : } # Dictionary linking unique captures to corresponding generic single panel # analysis results (SinglePanelResults objects as values): self.capture_to_results = collections.defaultdict(SinglePanelResults) # Dictionary linking unique normal library capture items to their corresponding # germline VCF filenames: self.normal_capture_to_vcf = {} # Dictionary linking (normal capture, cancer capture) pairings to corresponding # cancer library capture analysis results (CancerPanelResults objects as values): self.normal_cancer_pair_to_results = collections.defaultdict(CancerVsNormalPanelResults) get_job_param Retrieve the parameter of the specified name from the job parameters, or return a default value if it is not found. param param_name: String defining the parameter to retrieve return: The parameter value to use. def get_job_param(self, param_name): if param_name in self.job_params.keys(): return self.job_params[param_name] else: return self.default_job_params[param_name] set_germline_vcf Registers the specified vcf filename for the specified normal capture item, for this analysis. normal_capture: Normal panel capture identifier. vcfs: Tuple specifying (germline_variants_vcf, vepped_germline_variants_vcf) def set_germline_vcf(self, normal_capture, vcfs): self.normal_capture_to_vcf[normal_capture] = vcfs get_germline_vcf Obtain the germline VCF (original, un-vepped) for the given normal sample capture item. param normal_capture: Named tuple indicating a unique library capture. return: Original (un-vepped) germline VCF if available, otherwise None. def get_germline_vcf(self, normal_capture): if normal_capture in self.normal_capture_to_vcf: return self.normal_capture_to_vcf[normal_capture][0] else: return None get_vepped_germline_vcf Obtain the VEPped germline VCFs (original) for the given normal sample capture item. normal_capture: Named tuple indicating a unique library capture. return: VEPped germline VCF if available, otherwise None. def get_vepped_germline_vcf(self, normal_capture): if normal_capture in self.normal_capture_to_vcf: return self.normal_capture_to_vcf[normal_capture][1] else: return None set_capture_bam Set the bam file corresponding to the specified unique_capture in this analysis. unique_capture: A UniqueCapture item. bam: The bam filename. def set_capture_bam(self, unique_capture, bam): self.capture_to_results[unique_capture].merged_bamfile = bam set_capture_sveffect Record the structural variants effect prediction. unique_capture: Named tuple indicating unique library capture. effects_json: String indicating JSON file of predicted effects def set_capture_sveffect(self, unique_capture, effects_json): self.capture_to_results[unique_capture].sv_effects = effects_json set_capture_svs Record the structural variants results for the given library capture and event type. unique_capture: Named tuple indicating unique library capture. event_type: String indicating structural variant event type svs_tup: Tuple containing (bam_filename, gtf_filename) def set_capture_svs(self, unique_capture, event_type, svs_tup): self.capture_to_results[unique_capture].svs[event_type] = svs_tup set_capture_cnr Record the CNR copy number information (CNV kit output) for the given library capture. unique_capture: Named tuple indicating unique library capture. cnr: CNR output filename. def set_capture_cnr(self, unique_capture, cnr): self.capture_to_results[unique_capture].cnr = cnr set_capture_cns Record the CNS copy number information (CNV kit output) for the given library capture. unique_capture: Named tuple indicating unique library capture. cnr: CNS output filename. def set_capture_cns(self, unique_capture, cns): self.capture_to_results[unique_capture].cns = cns set_capture_seg Record the seg file (converted CNV kit output) for the given library capture. unique_capture: Named tuple indicating unique library capture. cnr: CNS output filename. def set_capture_seg(self, unique_capture, seg): self.capture_to_results[unique_capture].seg = seg get_capture_bam Retrieve the bam file corresponding to the specified unique_capture in this analysis. unique_capture: Named tuple indicating unique library capture. return: The corresponding bam filename, or None if it has not been configured. def get_capture_bam(self, unique_capture): if unique_capture in self.get_mapped_captures_all(): return self.capture_to_results[unique_capture].merged_bamfile else: return None check_sampledata Check this pipeline for validity of the sample data. In particular, check that each clinseq barcode has a corresponding fastq file, and if not, then modify the pipeline's sampledata by removing that clinseq barcode from the analysis. def check_sampledata(self): for sample_type in ['N', 'T', 'CFDNA']: clinseq_barcodes_with_data = [] for clinseq_barcode in self.sampledata[sample_type]: if data_available_for_clinseq_barcode(self.libdir, clinseq_barcode): clinseq_barcodes_with_data.append(clinseq_barcode) self.sampledata[sample_type] = clinseq_barcodes_with_data vep_data_is_available Indicates whether the VEP folder has been set for this analysis. return: Boolean. def vep_data_is_available(self): return self.refdata['vep_dir'] != None get_mapped_captures_all Obtain all unique sample library captures in this pipeline instance (including library - WGS items). return: List of unique capture named tuples. def get_mapped_captures_all(self): return self.capture_to_results.keys() get_mapped_captures_no_wgs Obtain all unique sample library captures in this pipeline instance (excluding library - WGS items). return: List of unique capture named tuples. def get_mapped_captures_no_wgs(self): return [capture for capture in self.capture_to_results.keys() if capture.capture_kit_id != WG ] get_mapped_captures_only_wgs Obtain all unique sample library captures in this pipeline instance (only including library - WGS items). return: List of unique capture named tuples. def get_mapped_captures_only_wgs(self): return [capture for capture in self.capture_to_results.keys() if capture.capture_kit_id == WG ] get_mapped_captures_normal Obtain tuples for all unique normal sample library captures in this pipeline instance - not including \"WGS\" (no) capture items. return: List of named tuples. def get_mapped_captures_normal(self): non_wgs_unique_captures = self.get_mapped_captures_no_wgs() return filter(lambda unique_capture: unique_capture.sample_type == N , non_wgs_unique_captures) get_mapped_captures_cancer Obtain all unique cancer sample library captures items in this pipeline instance - not including \"WGS\" (no) capture items. return: List of named tuples. def get_mapped_captures_cancer(self): non_wgs_unique_captures = self.get_mapped_captures_no_wgs() return filter(lambda unique_capture: unique_capture.sample_type != N , non_wgs_unique_captures) get_prep_kit_name Convert a two-letter library kit code to the corresponding library kit name. param prep_kit_code: Two-letter library prep code. return: The library prep kit name. def get_prep_kit_name(self, prep_kit_code): # FIXME: Move this information to a config JSON file. prep_kit_lookup = { BN : BIOO_NEXTFLEX , KH : KAPA_HYPERPREP , TD : THRUPLEX_DNASEQ , TP : THRUPLEX_PLASMASEQ , TF : THRUPLEX_FD , TS : TRUSEQ_RNA , NN : NEBNEXT_RNA , VI : VILO_RNA } return prep_kit_lookup[prep_kit_code] get_capture_name Convert a two-letter capture kit code to the corresponding capture kit name. param capture_kit_code: The two-letter capture kit code. return: The capture-kit name. def get_capture_name(self, capture_kit_code): # FIXME: Move this information to a config JSON file. capture_kit_loopkup = { CS : clinseq_v3_targets , CZ : clinseq_v4 , EX : EXOMEV3 , EO : EXOMEV1 , RF : fusion_v1 , CC : core_design , CD : discovery_coho , CB : big_design , AL : alascca_targets , TT : test-regions , CP : progression , CM : monitor } if capture_kit_code == 'WG': return 'lowpass_wgs' else: return capture_kit_loopkup[capture_kit_code] get_all_clinseq_barcodes return: All clinseq barcodes included in this clinseq analysis pipeline's panel data. def get_all_clinseq_barcodes(self): all_clinseq_barcodes = \\ self.sampledata['T'] + \\ self.sampledata['N'] + \\ self.sampledata['CFDNA'] return filter(lambda bc: bc != None, all_clinseq_barcodes) get_unique_capture_to_clinseq_barcodes Retrieves all clinseq barcodes for this clinseq analysis, and organises them according to unique library captures. return: A dictionary with tuples indicating unique library captures as keys, and barcode lists as values. def get_unique_capture_to_clinseq_barcodes(self): capture_to_barcodes = collections.defaultdict(list) for clinseq_barcode in self.get_all_clinseq_barcodes(): unique_capture = extract_unique_capture(clinseq_barcode) capture_to_barcodes[unique_capture].append(clinseq_barcode) return capture_to_barcodes merge_and_rm_dup Configures Picard merging and duplicate marking, for the specified group input bams, which should all correspond to the specified sample library capture. Registers the final output bam file for this library capture in this analysis. param unique_capture: A unique library capture specification param input_bams: The bam filenames for which to do merging and duplicate marking def merge_and_rm_dup(self, unique_capture, input_bams): # Strings indicating the sample and capture, for use in output file names below: capture_str = compose_lib_capture_str(unique_capture) #sample_str = {}-{} .format(unique_capture.sample_type, unique_capture.sample_id) #capture_str = {}-{}-{} .format(sample_str, unique_capture.library_kit_id, unique_capture.capture_kit_id) # Configure merging: merged_bam_filename = \\ {}/bams/{}/{}.bam .format(self.outdir, unique_capture.capture_kit_id, capture_str) merge_bams = PicardMergeSamFiles(input_bams, merged_bam_filename) merge_bams.is_intermediate = True merge_bams.jobname = picard-mergebams-{} .format(capture_str) self.add(merge_bams) # Configure duplicate marking: mark_dups_bam_filename = \\ {}/bams/{}/{}-nodups.bam .format(self.outdir, unique_capture.capture_kit_id, capture_str) mark_dups_metrics_filename = \\ {}/qc/picard/{}/{}-markdups-metrics.txt .format( self.outdir, unique_capture.capture_kit_id, capture_str) markdups = PicardMarkDuplicates( merge_bams.output_bam, mark_dups_bam_filename, mark_dups_metrics_filename) markdups.is_intermediate = False self.add(markdups) self.set_capture_bam(unique_capture, markdups.output_bam) self.qc_files.append(markdups.output_metrics) configure_fastq_qcs Configure QC on all fastq files that exist for this pipeline instance. return: List of qc output filenames. def configure_fastq_qcs(self): for clinseq_barcode in self.get_all_clinseq_barcodes(): curr_fqs = reduce(lambda l1, l2: l1 + l2, find_fastqs(clinseq_barcode, self.libdir)) for fq in curr_fqs: fastqc = FastQC() fastqc.input = fq fastqc.outdir = {}/qc/fastqc/ .format(self.outdir) fastqc.output = {}/qc/fastqc/{}_fastqc.zip .format( self.outdir, clinseq_barcode) fastqc.jobname = fastqc-{} .format(clinseq_barcode) self.qc_files.append(fastqc.output) self.add(fastqc) configure_align_and_merge Configure the aligning of the fastq files for all clinseq barcodes in this pipeline, and configure merging of the resulting bam files organised according to unique sample library captures (including \"WGS\" captures - i.e. no capture). def configure_align_and_merge(self): capture_to_barcodes = self.get_unique_capture_to_clinseq_barcodes() for unique_capture in capture_to_barcodes.keys(): curr_bamfiles = [] capture_kit = unique_capture.capture_kit_id for clinseq_barcode in capture_to_barcodes[unique_capture]: curr_bamfiles.append( align_library(self, fq1_files=find_fastqs(clinseq_barcode, self.libdir)[0], fq2_files=find_fastqs(clinseq_barcode, self.libdir)[1], clinseq_barcode=clinseq_barcode, ref=self.refdata['bwaIndex'], outdir= {}/bams/{} .format(self.outdir, capture_kit), maxcores=self.maxcores, remove_duplicates=True)) self.merge_and_rm_dup(unique_capture, curr_bamfiles) call_germline_variants Configure calling of germline variants for a normal sample library capture, and configure VEP if specified in the analysis. param normal_capture: The normal sample library capture identifier. param bam: Bam filename input to variant calling. def call_germline_variants(self, normal_capture, bam): targets = self.get_capture_name(normal_capture.capture_kit_id) capture_str = compose_lib_capture_str(normal_capture) freebayes = Freebayes() freebayes.input_bams = [bam] freebayes.somatic_only = False freebayes.params = None freebayes.reference_sequence = self.refdata['reference_genome'] freebayes.target_bed = self.refdata['targets'][targets]['targets-bed-slopped20'] freebayes.threads = self.maxcores freebayes.scratch = self.scratch freebayes.output = {}/variants/{}.freebayes-germline.vcf.gz .format(self.outdir, capture_str) freebayes.jobname = freebayes-germline-{} .format(capture_str) self.add(freebayes) vepped_vcf = None if self.vep_data_is_available(): vep_freebayes = VEP() vep_freebayes.input_vcf = freebayes.output vep_freebayes.threads = self.maxcores vep_freebayes.reference_sequence = self.refdata['reference_genome'] vep_freebayes.vep_dir = self.refdata['vep_dir'] vep_freebayes.output_vcf = {}/variants/{}.freebayes-germline.vep.vcf.gz .format(self.outdir, capture_str) vep_freebayes.jobname = vep-freebayes-germline-{} .format(capture_str) self.add(vep_freebayes) vepped_vcf = vep_freebayes.output_vcf self.set_germline_vcf(normal_capture, (freebayes.output, vepped_vcf)) configure_panel_analysis_with_normal Configure panel analyses focused on a specific unique normal library capture. Configure germline variant calling: For each unique cancer library capture, configure a comparative analysis against this normal capture: def configure_panel_analysis_with_normal(self, normal_capture): if normal_capture.sample_type != N : raise ValueError( Invalid input capture: + compose_sample_str(normal_capture)) normal_bam = self.get_capture_bam(normal_capture) self.call_germline_variants(normal_capture, normal_bam) for cancer_capture in self.get_mapped_captures_cancer(): self.configure_panel_analysis_cancer_vs_normal( normal_capture, cancer_capture) configure_make_cnvkit_tracks def configure_make_cnvkit_tracks(self, unique_capture): input_cnr = self.capture_to_results[unique_capture].cnr input_cns = self.capture_to_results[unique_capture].cns sample_str = compose_lib_capture_str(unique_capture) make_cnvkit_tracks = MakeCNVkitTracks() make_cnvkit_tracks.input_cnr = input_cnr make_cnvkit_tracks.input_cns = input_cns make_cnvkit_tracks.output_profile_bedgraph = {}/cnv/{}_profile.bedGraph .format( self.outdir, sample_str) make_cnvkit_tracks.output_segments_bedgraph = {}/cnv/{}_segments.bedGraph .format( self.outdir, sample_str) self.add(make_cnvkit_tracks) configure_fix_cnvkit Configure a job to fix the cnvkit output for the specified unique capture. param unique_capture: Named tuple identifying a sample library capture. param cnr: String indicating unfixed cnr file location param cns: String indicating unfixed cns file location param cnvkit_fix_filename: File containing table of data used to fix the outputs. def configure_fix_cnvkit(self, unique_capture, cnr, cns, cnvkit_fix_filename): sample_str = compose_lib_capture_str(unique_capture) cnvkit_fix = CNVkitFix(input_cnr=cnr, input_cns=cns, input_ref=cnvkit_fix_filename, output_cnr= {}/cnv/{}-fixed.cnr .format(self.outdir, sample_str), output_cns= {}/cnv/{}-fixed.cns .format(self.outdir, sample_str)) self.set_capture_cnr(unique_capture, cnvkit_fix.output_cnr) self.set_capture_cns(unique_capture, cnvkit_fix.output_cns) self.add(cnvkit_fix) configure_single_capture_analysis Configure all general analyses to perform given a single sample library capture. def configure_single_capture_analysis(self, unique_capture): input_bam = self.get_capture_bam(unique_capture) sample_str = compose_lib_capture_str(unique_capture) capture_kit_name = self.get_capture_name(unique_capture.capture_kit_id) library_kit_name = self.get_prep_kit_name(unique_capture.library_kit_id) sample_type = unique_capture.sample_type # Configure CNV kit analysis: cnvkit = CNVkit(input_bam=input_bam, output_cnr= {}/cnv/{}.cnr .format(self.outdir, sample_str), output_cns= {}/cnv/{}.cns .format(self.outdir, sample_str), scratch=self.scratch) # FIXME: Improve this messy code for extracting the relevant cnvkit reference from self.refdata: cnvkit.reference = None if 'cnvkit-ref' in self.refdata['targets'][capture_kit_name]: # Retrieve the first (in arbitrary order) reference available for this capture kit, # as a fall-back: cnvkit.reference = self.refdata['targets'][capture_kit_name]['cnvkit-ref'].values()[0].values()[0] try: # Try to get a more specific reference, if it available: cnvkit.reference = self.refdata['targets'][capture_kit_name]['cnvkit-ref'][library_kit_name][sample_type] except KeyError: pass if not cnvkit.reference: cnvkit.targets_bed = self.refdata['targets'][capture_kit_name]['targets-bed-slopped20'] cnvkit.fasta = self.refdata[ reference_genome ] cnvkit.jobname = cnvkit/{} .format(sample_str) # Register the result of this analysis: self.set_capture_cnr(unique_capture, cnvkit.output_cnr) self.set_capture_cns(unique_capture, cnvkit.output_cns) # FIXME: This extra step (fixing the cnv kit output) should perhaps go elsewhere. try: # Only fix the CNV-kit output if the required file is available: cnvkit_fix_filename = \\ self.refdata['targets'][capture_kit_name][ cnvkit-fix ][library_kit_name][sample_type] self.configure_fix_cnvkit(unique_capture, cnvkit.output_cnr, cnvkit.output_cns, cnvkit_fix_filename) except KeyError: pass self.add(cnvkit) # Configure conversion of CNV kit output to seg format: seg_filename = {}/cnv/{}.seg .format( self.outdir, sample_str) cns2seg = Cns2Seg(self.capture_to_results[unique_capture].cns, seg_filename) self.add(cns2seg) self.set_capture_seg(unique_capture, cns2seg.output_seg) configure_lowpass_analyses Configure generic analyses of all low-pass whole-genome sequencing data for this clinseq pipeline, under the assumption that alignment and bam file merging has already been performed. def configure_lowpass_analyses(self): for unique_wgs in self.get_mapped_captures_only_wgs(): self.configure_single_wgs_analyses(unique_wgs) configure_make_qdnaseq_tracks def configure_make_qdnaseq_tracks(self, qdnaseq_output, sample_str): make_qdnaseq_tracks = MakeQDNAseqTracks() make_qdnaseq_tracks.input_qdnaseq_file = qdnaseq_output make_qdnaseq_tracks.output_segments_bedgraph = {}/cnv/{}_qdnaseq_segments.bedGraph .format( self.outdir, sample_str) make_qdnaseq_tracks.output_copynumber_tdf = {}/cnv/{}_qdnaseq_copynumber.tdf .format( self.outdir, sample_str) make_qdnaseq_tracks.output_readcount_tdf = {}/cnv/{}_qdnaseq_readcount.tdf .format( self.outdir, sample_str) self.add(make_qdnaseq_tracks) configure_single_wgs_analyses Configure generic analyses of a single WGS item in the pipeline. param unique_wgs: An identifier for a single unique library WGS. def configure_single_wgs_analyses(self, unique_wgs): input_bam = self.get_capture_bam(unique_wgs) sample_str = compose_lib_capture_str(unique_wgs) qdnaseq = QDNASeq(input_bam, output_segments= {}/cnv/{}-qdnaseq.segments.txt .format( self.outdir, sample_str), background=None) self.configure_make_qdnaseq_tracks(qdnaseq.output, sample_str) self.add(qdnaseq) run_wgs_bam_qc Run QC on wgs bams param bams: list of bams return: list of generated files def run_wgs_bam_qc(self, bams): qc_files = [] logging.debug( bams are {} .format(bams)) for bam in bams: basefn = stripsuffix(os.path.basename(bam), .bam ) isize = PicardCollectInsertSizeMetrics() isize.input = bam isize.jobname = picard-isize-{} .format(basefn) isize.output_metrics = {}/qc/picard/wgs/{}.picard-insertsize.txt .format(self.outdir, basefn) self.add(isize) wgsmetrics = PicardCollectWgsMetrics() wgsmetrics.input = bam wgsmetrics.reference_sequence = self.refdata['reference_genome'] wgsmetrics.output_metrics = {}/qc/picard/wgs/{}.picard-wgsmetrics.txt .format(self.outdir, basefn) wgsmetrics.jobname = picard-wgsmetrics-{} .format(basefn) self.add(wgsmetrics) qc_files += [isize.output_metrics, wgsmetrics.output_metrics] return qc_files configure_panel_analyses Configure generic analyses of all panel data for this clinseq pipeline, assuming that alignment and bam file merging has been performed. def configure_panel_analyses(self): # Configure analyses to be run on all unique panel captures individually: for unique_capture in self.get_mapped_captures_no_wgs(): self.configure_single_capture_analysis(unique_capture) self.configure_make_cnvkit_tracks(unique_capture) # Configure a separate group of analyses for each unique normal library capture: for normal_capture in self.get_mapped_captures_normal(): self.configure_panel_analysis_with_normal(normal_capture) configure_panel_msings_analyses Configure msings analyses for all unique captures for which this is possible. def configure_panel_msings_analyses(self): for unique_capture in self.get_mapped_captures_cancer(): try: self.configure_msings(unique_capture) except InvalidRefDataException: # This indicates the reference data does not support configuring # msings for this cancer capture = Ignore this and proceed to the next: pass configure_somatic_calling Configure somatic variant calling in this pipeline, for a specified pairing of normal and cancer library capture events. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_somatic_calling(self, normal_capture, cancer_capture): cancer_bam = self.get_capture_bam(cancer_capture) normal_bam = self.get_capture_bam(normal_capture) target_name = self.get_capture_name(cancer_capture.capture_kit_id) # FIXME: Need to fix the configuration of the min_alt_frac threshold, rather than hard-coding it here: somatic_variants = call_somatic_variants( self, cancer_bam=cancer_bam, normal_bam=normal_bam, cancer_capture=cancer_capture, normal_capture=normal_capture, target_name=target_name, outdir=self.outdir, callers=['vardict'], min_alt_frac=self.get_job_param('vardict-min-alt-frac'), min_num_reads=self.get_job_param('vardict-min-num-reads')) self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].somatic_vcf = \\ somatic_variants.values()[0] configure_vep def configure_vep(self, normal_capture, cancer_capture): if not self.vep_data_is_available(): raise ValueError( Invalid call to configure_vep: No vep data available. ) somatic_vcf = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].somatic_vcf cancer_capture_str = compose_lib_capture_str(cancer_capture) normal_capture_str = compose_lib_capture_str(normal_capture) vep = VEP() vep.input_vcf = somatic_vcf vep.threads = self.maxcores vep.reference_sequence = self.refdata['reference_genome'] vep.vep_dir = self.refdata['vep_dir'] vep.output_vcf = {}/variants/{}-{}.somatic.vep.vcf.gz .format( self.outdir, cancer_capture_str, normal_capture_str) vep.jobname = vep-freebayes-somatic/{} .format(cancer_capture_str) vep.additional_options = self.get_job_param( vep-additional-options ) self.add(vep) self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].vepped_vcf = \\ vep.output_vcf configure_make_allelic_fraction_track Configure a small job for converting the germline variant somatic allelic fraction information into tracks for displaying in IGV. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_make_allelic_fraction_track(self, normal_capture, cancer_capture): vcf = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].vcf_addsample_output make_allelic_fraction_track = MakeAllelicFractionTrack() make_allelic_fraction_track.input_vcf = vcf normal_capture_str = compose_lib_capture_str(normal_capture) cancer_capture_str = compose_lib_capture_str(cancer_capture) make_allelic_fraction_track.output_bedgraph = \\ {}/variants/{}-and-{}.germline-variants-somatic-afs.bedGraph .format( self.outdir, normal_capture_str, cancer_capture_str) self.add(make_allelic_fraction_track) configure_vcf_add_sample Configure VCF updating in this pipeline, for a specified pairing of normal and cancer library capture events. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_vcf_add_sample(self, normal_capture, cancer_capture): # Configure VCF add sample: vcfaddsample = VcfAddSample() vcfaddsample.input_bam = self.get_capture_bam(cancer_capture) vcfaddsample.input_vcf = self.get_germline_vcf(normal_capture) normal_capture_str = compose_lib_capture_str(normal_capture) cancer_capture_str = compose_lib_capture_str(cancer_capture) cancer_sample_str = compose_sample_str(cancer_capture) vcfaddsample.samplename = cancer_sample_str vcfaddsample.filter_hom = True vcfaddsample.output = {}/variants/{}-and-{}.germline-variants-with-somatic-afs.vcf.gz .format( self.outdir, normal_capture_str, cancer_capture_str) vcfaddsample.jobname = vcf-add-sample-{} .format(cancer_capture_str) self.add(vcfaddsample) self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].vcf_addsample_output = \\ vcfaddsample.output configure_msi_sensor Configure MSI sensor in this pipeline, for a specified pairing of normal and cancer library capture events. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_msi_sensor(self, normal_capture, cancer_capture): # Configure MSI sensor: msisensor = MsiSensor() cancer_capture_name = self.get_capture_name(cancer_capture.capture_kit_id) msisensor.msi_sites = self.refdata['targets'][cancer_capture_name]['msisites'] msisensor.input_normal_bam = self.get_capture_bam(normal_capture) msisensor.input_tumor_bam = self.get_capture_bam(cancer_capture) normal_capture_str = compose_lib_capture_str(normal_capture) cancer_capture_str = compose_lib_capture_str(cancer_capture) msisensor.output = {}/msisensor-{}-{}.tsv .format( self.outdir, normal_capture_str, cancer_capture_str) msisensor.threads = self.maxcores msisensor.jobname = msisensor-{}-{} .format(normal_capture_str, cancer_capture_str) self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].msi_output = \\ msisensor.output self.add(msisensor) configure_msings Configure msings analysis, which operates on a cancer capture bam input file. param cancer_capture: Named tuple indicating cancer library capture. def configure_msings(self, cancer_capture): # Configure Msings: msings = Msings() cancer_capture_name = self.get_capture_name(cancer_capture.capture_kit_id) msings.input_fasta = self.refdata['reference_genome'] try: msings.msings_baseline = self.refdata['targets'][cancer_capture_name]['msings-baseline'] msings.msings_bed = self.refdata['targets'][cancer_capture_name]['msings-bed'] msings.msings_intervals = self.refdata['targets'][cancer_capture_name]['msings-msi_intervals'] except KeyError: raise InvalidRefDataException( Missing msings data. ) # FIXME: The above logic could be cleaned up. if not msings.msings_bed: raise InvalidRefDataException( Missing msings data. ) msings.input_bam = self.get_capture_bam(cancer_capture) cancer_capture_str = compose_lib_capture_str(cancer_capture) msings.outdir = {}/msings-{} .format( self.outdir, cancer_capture_str) # FIXME: This is nasty: bam_name = os.path.splitext(os.path.basename(msings.input_bam))[0] msings.output = {}/{}/{}.MSI_Analysis.txt .format( msings.outdir, bam_name, bam_name) msings.threads = self.maxcores msings.jobname = msings-{} .format(cancer_capture_str) self.capture_to_results[cancer_capture].msings_output = msings.output self.add(msings) configure_hz_conc Configure heterozygote concordance calculation in this pipeline, for a specified pairing of normal and cancer library capture events. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_hz_conc(self, normal_capture, cancer_capture): # Configure heterozygote concordance: hzconcordance = HeterzygoteConcordance() hzconcordance.input_vcf = self.get_germline_vcf(normal_capture) hzconcordance.input_bam = self.get_capture_bam(cancer_capture) hzconcordance.reference_sequence = self.refdata['reference_genome'] cancer_capture_name = self.get_capture_name(cancer_capture.capture_kit_id) hzconcordance.target_regions = \\ self.refdata['targets'][cancer_capture_name]['targets-interval_list-slopped20'] hzconcordance.normalid = compose_sample_str(normal_capture) hzconcordance.filter_reads_with_N_cigar = True hzconcordance.jobname = hzconcordance-{} .format(compose_lib_capture_str(cancer_capture)) hzconcordance.output = {}/bams/{}-{}-hzconcordance.txt .format( self.outdir, compose_lib_capture_str(cancer_capture), compose_lib_capture_str(normal_capture)) self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].hzconcordance_output = \\ hzconcordance.output self.add(hzconcordance) configure_contest_vcf_generation Configure generation of a contest VCF input file in this pipeline, for a specified pairing of normal and cancer library capture events. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_contest_vcf_generation(self, normal_capture, cancer_capture): contest_vcf_generation = CreateContestVCFs() normal_capture_name = self.get_capture_name(normal_capture.capture_kit_id) cancer_capture_name = self.get_capture_name(cancer_capture.capture_kit_id) normal_targets = self.refdata['targets'][normal_capture_name]['targets-bed-slopped20'] cancer_targets = self.refdata['targets'][cancer_capture_name]['targets-bed-slopped20'] contest_vcf_generation.input_target_regions_bed_1 = normal_targets contest_vcf_generation.input_target_regions_bed_2 = cancer_targets contest_vcf_generation.input_population_vcf = self.refdata[ swegene_common ] normal_capture_str = compose_lib_capture_str(normal_capture) cancer_capture_str = compose_lib_capture_str(cancer_capture) contest_vcf_generation.output = {}/contamination/pop_vcf_{}-{}.vcf .format( self.outdir, normal_capture_str, cancer_capture_str) contest_vcf_generation.jobname = contest_pop_vcf_{}-{} .format( normal_capture_str, cancer_capture_str) self.add(contest_vcf_generation) return contest_vcf_generation.output configure_contest Configure running of ContEst in this pipeline, for a specified pairing of library capture events. Estimates contamination in the bam file for the first library capture, using the bam file for the second library capture as a reference comparison. param library_capture_1: Named tuple indicating first library capture. param library_capture_2: Named tuple indicating second library capture. param contest_vcf: Contest population allele frequency VCF input file. def configure_contest(self, library_capture_1, library_capture_2, contest_vcf): contest = ContEst() contest.reference_genome = self.refdata['reference_genome'] contest.input_eval_bam = self.get_capture_bam(library_capture_1) contest.input_genotype_bam = self.get_capture_bam(library_capture_2) contest.input_population_af_vcf = contest_vcf # TODO: Is it necessary to create the output subdir contamination somewhere? Check how it's done for e.g. cnvkit. contest.output = {}/contamination/{}.contest.txt .format(self.outdir, compose_lib_capture_str(library_capture_1)) # TODO: Should the analysis id also be in name of out file? contest.jobname = contest_tumor/{} .format(compose_lib_capture_str(library_capture_1)) # TODO: Is it ok that the job name does not contain analysis id, i.e. may not be unique? self.add(contest) return contest.output configure_contam_qc_call Configure generation of a contamination QC call in this pipeline, based on the specified contest output. Returns the resulting QC output filename. param contest_output: ContEst output filename. param library_capture: Named tuple identifying a unique library capture. def configure_contam_qc_call(self, contest_output, library_capture): process_contest = ContEstToContamCaveat() process_contest.input_contest_results = contest_output process_contest.output = {}/qc/{}-contam-qc-call.json .format( self.outdir, compose_lib_capture_str(library_capture)) self.add(process_contest) return process_contest.output configure_contamination_estimate Configure contamination estimatates for a given normal, cancer library capture pairing. param normal_capture: Namedtuple indicating a normal library capture. param cancer_capture: Namedtuple indicating a cancer library capture. def configure_contamination_estimate(self, normal_capture, cancer_capture): # Configure generation of the contest VCF input file: intersection_contest_vcf = \\ self.configure_contest_vcf_generation(normal_capture, cancer_capture) # Configure contest for calculating contamination in the cancer sample: cancer_vs_normal_contest_output = \\ self.configure_contest(cancer_capture, normal_capture, intersection_contest_vcf) # Configure contest for calculating contamination in the normal sample: normal_vs_cancer_contest_output = \\ self.configure_contest(normal_capture, cancer_capture, intersection_contest_vcf) # Configure cancer sample contamination QC call: cancer_contam_call = self.configure_contam_qc_call(cancer_vs_normal_contest_output, cancer_capture) # Register the outputs of running contest: self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].normal_contest_output = \\ normal_vs_cancer_contest_output self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].cancer_contest_output = \\ cancer_vs_normal_contest_output self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].cancer_contam_call = \\ cancer_contam_call configure_purecn Configure PureCN, and also configure the custom run of VarDict, which is required for PureCN. :param normal_capture: A unique normal sample library capture :param cancer_capture: A unique cancer sample library capture def configure_purecn(self, normal_capture, cancer_capture): cancer_bam = self.get_capture_bam(cancer_capture) normal_bam = self.get_capture_bam(normal_capture) target_name = self.get_capture_name(cancer_capture.capture_kit_id) cancer_capture_str = compose_lib_capture_str(cancer_capture) capture_name = self.get_capture_name(cancer_capture.capture_kit_id) normal_capture_str = compose_lib_capture_str(normal_capture) # Configure the PureCN-specific VarDict job: vardict_pureCN = VarDictForPureCN() vardict_pureCN.input_tumor = cancer_bam vardict_pureCN.input_normal = normal_bam vardict_pureCN.tumorid = cancer_capture_str vardict_pureCN.normalid = normal_capture_str vardict_pureCN.reference_sequence = self.refdata['reference_genome'] vardict_pureCN.reference_dict = self.refdata['reference_dict'] vardict_pureCN.target_bed = self.refdata['targets'][target_name]['targets-bed-slopped20'] vardict_pureCN.dbsnp = self.refdata[ dbSNP ] vardict_pureCN.output = {}/variants/{}-{}.vardict-somatic-purecn.vcf.gz .format( self.outdir, cancer_capture_str, normal_capture_str) vardict_pureCN.jobname = vardict_purecn/{}-{} .format(cancer_capture_str, normal_capture_str) self.add(vardict_pureCN) # Retrieve the relevant seg-format file: seg_filename = self.capture_to_results[cancer_capture].seg # Configure PureCN itself: # FIXME: NOTE: The Job params *must* be specified as arguments in the case of PureCN: pureCN = PureCN( input_seg=seg_filename, input_vcf=vardict_pureCN.output, tumorid=cancer_capture_str, gcgene_file=self.refdata['targets'][capture_name]['purecn_targets'], outdir= {}/purecn .format(self.outdir), postopt=True, ) self.add(pureCN) # FIXME: It seems like a nasty hack to include a dictionary here, perhaps this belongs elsewhere? self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].pureCN_outputs = { csv : {}/{}.csv .format(pureCN.outdir, pureCN.tumorid), genes_csv : {}/{}_genes.csv .format(pureCN.outdir, pureCN.tumorid), loh_csv : {}/{}_loh.csv .format(pureCN.outdir, pureCN.tumorid), variants_csv : {}/{}_variants.csv .format(pureCN.outdir, pureCN.tumorid), } configure_panel_analysis_cancer_vs_normal Configures standard paired cancer vs normal panel analyses for the specified unique normal and cancer library captures. Comprises the following analyses: - Somatic variant calling - Running VEP on the resulting somatic VCF - Updating of the germline VCF to take into consideration the cancer sample - MSI sensor - Heterozygote concordance of the sample pair - Contamination estimate of cancer compared with normal and vice versa param normal_capture: A unique normal sample library capture param cancer_capture: A unique cancer sample library capture def configure_panel_analysis_cancer_vs_normal(self, normal_capture, cancer_capture): self.configure_somatic_calling(normal_capture, cancer_capture) if self.vep_data_is_available(): self.configure_vep(normal_capture, cancer_capture) self.configure_vcf_add_sample(normal_capture, cancer_capture) self.configure_make_allelic_fraction_track(normal_capture, cancer_capture) self.configure_msi_sensor(normal_capture, cancer_capture) self.configure_hz_conc(normal_capture, cancer_capture) self.configure_contamination_estimate(normal_capture, cancer_capture) configure_all_lowpass_qcs Configure QC checks for all low-pass whole genome data in this pipeline. def configure_all_lowpass_qcs(self): for unique_wgs in self.get_mapped_captures_only_wgs(): self.qc_files += \\ self.configure_wgs_qc(unique_wgs) configure_wgs_qc Configure QC checks for the specified unique WGS item in this pipeline. param unique_wgs: A named tuple identifying a single unique WGS item. return: QC files output files resulting from the QC analysis configuration. def configure_wgs_qc(self, unique_wgs): bam = self.get_capture_bam(unique_wgs) wgs_name = compose_lib_capture_str(unique_wgs) qc_files = [] isize = PicardCollectInsertSizeMetrics() isize.input = bam isize.jobname = picard-isize-{} .format(wgs_name) isize.output_metrics = {}/qc/picard/wgs/{}.picard-insertsize.txt .format(self.outdir, wgs_name) self.add(isize) wgsmetrics = PicardCollectWgsMetrics() wgsmetrics.input = bam wgsmetrics.reference_sequence = self.refdata['reference_genome'] wgsmetrics.output_metrics = {}/qc/picard/wgs/{}.picard-wgsmetrics.txt .format(self.outdir, wgs_name) wgsmetrics.jobname = picard-wgsmetrics-{} .format(wgs_name) self.add(wgsmetrics) qc_files += [isize.output_metrics, wgsmetrics.output_metrics] return qc_files configure_all_panel_qcs Configures QC checks for all panel data (not including WGS data) in this pipeline. def configure_all_panel_qcs(self): for unique_capture in self.get_mapped_captures_no_wgs(): self.qc_files += \\ self.configure_panel_qc(unique_capture) configure_multi_qc Configures MultiQC for this pipeline. self.qc_files must be fully populated in order for MultiQC to use all relevant input files. def configure_multi_qc(self): multiqc = MultiQC() multiqc.input_files = self.qc_files multiqc.search_dir = self.outdir multiqc.output = {}/multiqc/{}-multiqc .format(self.outdir, self.analysis_id) multiqc.jobname = multiqc-{} .format(self.sampledata['sdid']) self.add(multiqc) get_coverage_bed Retrieve the targets bed file to use for calculating coverage, given the specified targets name. param targets: Target capture name return: bed file name def get_coverage_bed(self, targets): return self.refdata['targets'][targets]['targets-bed-slopped20'] configure_panel_qc Configure QC analyses for a given library capture. param unique_capture: Named tuple identifying a sample library capture. return: list of QC output files for this capture. def configure_panel_qc(self, unique_capture): bam = self.get_capture_bam(unique_capture) targets = self.get_capture_name(unique_capture.capture_kit_id) logging.debug( Adding QC jobs for {} .format(bam)) capture_str = compose_lib_capture_str(unique_capture) isize = PicardCollectInsertSizeMetrics() isize.input = bam isize.output_metrics = {}/qc/picard/{}/{}.picard-insertsize.txt .format( self.outdir, unique_capture.capture_kit_id, capture_str) isize.jobname = picard-isize-{} .format(capture_str) self.add(isize) oxog = PicardCollectOxoGMetrics() oxog.input = bam oxog.reference_sequence = self.refdata['reference_genome'] oxog.output_metrics = {}/qc/picard/{}/{}.picard-oxog.txt .format( self.outdir, unique_capture.capture_kit_id, capture_str) oxog.jobname = picard-oxog-{} .format(capture_str) self.add(oxog) hsmetrics = PicardCollectHsMetrics() hsmetrics.input = bam hsmetrics.reference_sequence = self.refdata['reference_genome'] hsmetrics.target_regions = self.refdata['targets'][targets][ 'targets-interval_list-slopped20'] hsmetrics.bait_regions = self.refdata['targets'][targets][ 'targets-interval_list-slopped20'] hsmetrics.bait_name = targets hsmetrics.output_metrics = {}/qc/picard/{}/{}.picard-hsmetrics.txt .format( self.outdir, unique_capture.capture_kit_id, capture_str) hsmetrics.jobname = picard-hsmetrics-{} .format(capture_str) self.add(hsmetrics) sambamba = SambambaDepth() sambamba.targets_bed = self.refdata['targets'][targets]['targets-bed-slopped20'] sambamba.input = bam sambamba.output = {}/qc/sambamba/{}.sambamba-depth-targets.txt .format( self.outdir, capture_str) sambamba.jobname = sambamba-depth-{} .format(capture_str) self.add(sambamba) coverage_hist = CoverageHistogram() # FIXME: Ugly temporary solution to allow the alascca pipeline to use a specific # targets file: coverage_hist.input_bed = self.get_coverage_bed(targets) coverage_hist.input_bam = bam coverage_hist.output = {}/qc/{}.coverage-histogram.txt .format( self.outdir, capture_str) coverage_hist.jobname = alascca-coverage-hist/{} .format(capture_str) self.add(coverage_hist) coverage_qc_call = CoverageCaveat() coverage_qc_call.low_thresh_fraction = self.get_job_param('cov-low-thresh-fraction') coverage_qc_call.low_thresh_fold_cov = self.get_job_param('cov-low-thresh-fold-cov') coverage_qc_call.input_histogram = coverage_hist.output coverage_qc_call.output = {}/qc/{}.coverage-qc-call.json .format(self.outdir, capture_str) coverage_qc_call.jobname = coverage-qc-call/{} .format(capture_str) self.add(coverage_qc_call) self.capture_to_results[unique_capture].cov_qc_call = coverage_qc_call.output return [isize.output_metrics, oxog.output_metrics, hsmetrics.output_metrics, sambamba.output, coverage_hist.output, coverage_qc_call.output]","title":"Clinseq"},{"location":"lld/clinseq/#clinseqpipeline","text":"A pipeline for processing clinseq cancer genomics. sampledata: A dictionary specifying the clinseq barcodes of samples of different types. refdata: A dictionary specifying the reference data used for configuring the pipeline jobs. outdir: Output folder location string. libdir: String specifying location of the library fastq files. maxcores: Maximum number of cores to use concurrently in this analysis. scratch: String indicating folder in which jobs should output all temporary files. kwargs: Additional key-word arguments. ClinseqPipeline is backbone of autoseq, most of the funtions and file manipulations are written with in this class. class ClinseqPipeline(PypedreamPipeline): def __init__(self, sampledata, refdata, job_params, outdir, libdir, maxcores=1, scratch= /scratch/tmp/tmp , analysis_id=None, **kwargs): PypedreamPipeline.__init__(self, normpath(outdir), **kwargs) self.sampledata = sampledata self.refdata = refdata # FIXME: Introduced a simple dictionary for configuring various pipeline job parameters. # However, this solution is still not elegant: self.job_params = job_params self.maxcores = maxcores self.libdir = libdir self.qc_files = [] self.scratch = scratch self.analysis_id = analysis_id # Set up default job parameters: self.default_job_params = { cov-high-thresh-fraction : 0.95, cov-high-thresh-fold-cov : 100, cov-low-thresh-fraction : 0.95, cov-low-thresh-fold-cov : 50, vardict-min-alt-frac : 0.02, vardict-min-num-reads : None, vep-additional-options : } # Dictionary linking unique captures to corresponding generic single panel # analysis results (SinglePanelResults objects as values): self.capture_to_results = collections.defaultdict(SinglePanelResults) # Dictionary linking unique normal library capture items to their corresponding # germline VCF filenames: self.normal_capture_to_vcf = {} # Dictionary linking (normal capture, cancer capture) pairings to corresponding # cancer library capture analysis results (CancerPanelResults objects as values): self.normal_cancer_pair_to_results = collections.defaultdict(CancerVsNormalPanelResults)","title":"ClinseqPipeline"},{"location":"lld/clinseq/#get_job_param","text":"Retrieve the parameter of the specified name from the job parameters, or return a default value if it is not found. param param_name: String defining the parameter to retrieve return: The parameter value to use. def get_job_param(self, param_name): if param_name in self.job_params.keys(): return self.job_params[param_name] else: return self.default_job_params[param_name]","title":"get_job_param"},{"location":"lld/clinseq/#set_germline_vcf","text":"Registers the specified vcf filename for the specified normal capture item, for this analysis. normal_capture: Normal panel capture identifier. vcfs: Tuple specifying (germline_variants_vcf, vepped_germline_variants_vcf) def set_germline_vcf(self, normal_capture, vcfs): self.normal_capture_to_vcf[normal_capture] = vcfs","title":"set_germline_vcf"},{"location":"lld/clinseq/#get_germline_vcf","text":"Obtain the germline VCF (original, un-vepped) for the given normal sample capture item. param normal_capture: Named tuple indicating a unique library capture. return: Original (un-vepped) germline VCF if available, otherwise None. def get_germline_vcf(self, normal_capture): if normal_capture in self.normal_capture_to_vcf: return self.normal_capture_to_vcf[normal_capture][0] else: return None","title":"get_germline_vcf"},{"location":"lld/clinseq/#get_vepped_germline_vcf","text":"Obtain the VEPped germline VCFs (original) for the given normal sample capture item. normal_capture: Named tuple indicating a unique library capture. return: VEPped germline VCF if available, otherwise None. def get_vepped_germline_vcf(self, normal_capture): if normal_capture in self.normal_capture_to_vcf: return self.normal_capture_to_vcf[normal_capture][1] else: return None","title":"get_vepped_germline_vcf"},{"location":"lld/clinseq/#set_capture_bam","text":"Set the bam file corresponding to the specified unique_capture in this analysis. unique_capture: A UniqueCapture item. bam: The bam filename. def set_capture_bam(self, unique_capture, bam): self.capture_to_results[unique_capture].merged_bamfile = bam","title":"set_capture_bam"},{"location":"lld/clinseq/#set_capture_sveffect","text":"Record the structural variants effect prediction. unique_capture: Named tuple indicating unique library capture. effects_json: String indicating JSON file of predicted effects def set_capture_sveffect(self, unique_capture, effects_json): self.capture_to_results[unique_capture].sv_effects = effects_json","title":"set_capture_sveffect"},{"location":"lld/clinseq/#set_capture_svs","text":"Record the structural variants results for the given library capture and event type. unique_capture: Named tuple indicating unique library capture. event_type: String indicating structural variant event type svs_tup: Tuple containing (bam_filename, gtf_filename) def set_capture_svs(self, unique_capture, event_type, svs_tup): self.capture_to_results[unique_capture].svs[event_type] = svs_tup","title":"set_capture_svs"},{"location":"lld/clinseq/#set_capture_cnr","text":"Record the CNR copy number information (CNV kit output) for the given library capture. unique_capture: Named tuple indicating unique library capture. cnr: CNR output filename. def set_capture_cnr(self, unique_capture, cnr): self.capture_to_results[unique_capture].cnr = cnr","title":"set_capture_cnr"},{"location":"lld/clinseq/#set_capture_cns","text":"Record the CNS copy number information (CNV kit output) for the given library capture. unique_capture: Named tuple indicating unique library capture. cnr: CNS output filename. def set_capture_cns(self, unique_capture, cns): self.capture_to_results[unique_capture].cns = cns","title":"set_capture_cns"},{"location":"lld/clinseq/#set_capture_seg","text":"Record the seg file (converted CNV kit output) for the given library capture. unique_capture: Named tuple indicating unique library capture. cnr: CNS output filename. def set_capture_seg(self, unique_capture, seg): self.capture_to_results[unique_capture].seg = seg","title":"set_capture_seg"},{"location":"lld/clinseq/#get_capture_bam","text":"Retrieve the bam file corresponding to the specified unique_capture in this analysis. unique_capture: Named tuple indicating unique library capture. return: The corresponding bam filename, or None if it has not been configured. def get_capture_bam(self, unique_capture): if unique_capture in self.get_mapped_captures_all(): return self.capture_to_results[unique_capture].merged_bamfile else: return None","title":"get_capture_bam"},{"location":"lld/clinseq/#check_sampledata","text":"Check this pipeline for validity of the sample data. In particular, check that each clinseq barcode has a corresponding fastq file, and if not, then modify the pipeline's sampledata by removing that clinseq barcode from the analysis. def check_sampledata(self): for sample_type in ['N', 'T', 'CFDNA']: clinseq_barcodes_with_data = [] for clinseq_barcode in self.sampledata[sample_type]: if data_available_for_clinseq_barcode(self.libdir, clinseq_barcode): clinseq_barcodes_with_data.append(clinseq_barcode) self.sampledata[sample_type] = clinseq_barcodes_with_data","title":"check_sampledata"},{"location":"lld/clinseq/#vep_data_is_available","text":"Indicates whether the VEP folder has been set for this analysis. return: Boolean. def vep_data_is_available(self): return self.refdata['vep_dir'] != None","title":"vep_data_is_available"},{"location":"lld/clinseq/#get_mapped_captures_all","text":"Obtain all unique sample library captures in this pipeline instance (including library - WGS items). return: List of unique capture named tuples. def get_mapped_captures_all(self): return self.capture_to_results.keys()","title":"get_mapped_captures_all"},{"location":"lld/clinseq/#get_mapped_captures_no_wgs","text":"Obtain all unique sample library captures in this pipeline instance (excluding library - WGS items). return: List of unique capture named tuples. def get_mapped_captures_no_wgs(self): return [capture for capture in self.capture_to_results.keys() if capture.capture_kit_id != WG ]","title":"get_mapped_captures_no_wgs"},{"location":"lld/clinseq/#get_mapped_captures_only_wgs","text":"Obtain all unique sample library captures in this pipeline instance (only including library - WGS items). return: List of unique capture named tuples. def get_mapped_captures_only_wgs(self): return [capture for capture in self.capture_to_results.keys() if capture.capture_kit_id == WG ]","title":"get_mapped_captures_only_wgs"},{"location":"lld/clinseq/#get_mapped_captures_normal","text":"Obtain tuples for all unique normal sample library captures in this pipeline instance - not including \"WGS\" (no) capture items. return: List of named tuples. def get_mapped_captures_normal(self): non_wgs_unique_captures = self.get_mapped_captures_no_wgs() return filter(lambda unique_capture: unique_capture.sample_type == N , non_wgs_unique_captures)","title":"get_mapped_captures_normal"},{"location":"lld/clinseq/#get_mapped_captures_cancer","text":"Obtain all unique cancer sample library captures items in this pipeline instance - not including \"WGS\" (no) capture items. return: List of named tuples. def get_mapped_captures_cancer(self): non_wgs_unique_captures = self.get_mapped_captures_no_wgs() return filter(lambda unique_capture: unique_capture.sample_type != N , non_wgs_unique_captures)","title":"get_mapped_captures_cancer"},{"location":"lld/clinseq/#get_prep_kit_name","text":"Convert a two-letter library kit code to the corresponding library kit name. param prep_kit_code: Two-letter library prep code. return: The library prep kit name. def get_prep_kit_name(self, prep_kit_code): # FIXME: Move this information to a config JSON file. prep_kit_lookup = { BN : BIOO_NEXTFLEX , KH : KAPA_HYPERPREP , TD : THRUPLEX_DNASEQ , TP : THRUPLEX_PLASMASEQ , TF : THRUPLEX_FD , TS : TRUSEQ_RNA , NN : NEBNEXT_RNA , VI : VILO_RNA } return prep_kit_lookup[prep_kit_code]","title":"get_prep_kit_name"},{"location":"lld/clinseq/#get_capture_name","text":"Convert a two-letter capture kit code to the corresponding capture kit name. param capture_kit_code: The two-letter capture kit code. return: The capture-kit name. def get_capture_name(self, capture_kit_code): # FIXME: Move this information to a config JSON file. capture_kit_loopkup = { CS : clinseq_v3_targets , CZ : clinseq_v4 , EX : EXOMEV3 , EO : EXOMEV1 , RF : fusion_v1 , CC : core_design , CD : discovery_coho , CB : big_design , AL : alascca_targets , TT : test-regions , CP : progression , CM : monitor } if capture_kit_code == 'WG': return 'lowpass_wgs' else: return capture_kit_loopkup[capture_kit_code]","title":"get_capture_name"},{"location":"lld/clinseq/#get_all_clinseq_barcodes","text":"return: All clinseq barcodes included in this clinseq analysis pipeline's panel data. def get_all_clinseq_barcodes(self): all_clinseq_barcodes = \\ self.sampledata['T'] + \\ self.sampledata['N'] + \\ self.sampledata['CFDNA'] return filter(lambda bc: bc != None, all_clinseq_barcodes)","title":"get_all_clinseq_barcodes"},{"location":"lld/clinseq/#get_unique_capture_to_clinseq_barcodes","text":"Retrieves all clinseq barcodes for this clinseq analysis, and organises them according to unique library captures. return: A dictionary with tuples indicating unique library captures as keys, and barcode lists as values. def get_unique_capture_to_clinseq_barcodes(self): capture_to_barcodes = collections.defaultdict(list) for clinseq_barcode in self.get_all_clinseq_barcodes(): unique_capture = extract_unique_capture(clinseq_barcode) capture_to_barcodes[unique_capture].append(clinseq_barcode) return capture_to_barcodes","title":"get_unique_capture_to_clinseq_barcodes"},{"location":"lld/clinseq/#merge_and_rm_dup","text":"Configures Picard merging and duplicate marking, for the specified group input bams, which should all correspond to the specified sample library capture. Registers the final output bam file for this library capture in this analysis. param unique_capture: A unique library capture specification param input_bams: The bam filenames for which to do merging and duplicate marking def merge_and_rm_dup(self, unique_capture, input_bams): # Strings indicating the sample and capture, for use in output file names below: capture_str = compose_lib_capture_str(unique_capture) #sample_str = {}-{} .format(unique_capture.sample_type, unique_capture.sample_id) #capture_str = {}-{}-{} .format(sample_str, unique_capture.library_kit_id, unique_capture.capture_kit_id) # Configure merging: merged_bam_filename = \\ {}/bams/{}/{}.bam .format(self.outdir, unique_capture.capture_kit_id, capture_str) merge_bams = PicardMergeSamFiles(input_bams, merged_bam_filename) merge_bams.is_intermediate = True merge_bams.jobname = picard-mergebams-{} .format(capture_str) self.add(merge_bams) # Configure duplicate marking: mark_dups_bam_filename = \\ {}/bams/{}/{}-nodups.bam .format(self.outdir, unique_capture.capture_kit_id, capture_str) mark_dups_metrics_filename = \\ {}/qc/picard/{}/{}-markdups-metrics.txt .format( self.outdir, unique_capture.capture_kit_id, capture_str) markdups = PicardMarkDuplicates( merge_bams.output_bam, mark_dups_bam_filename, mark_dups_metrics_filename) markdups.is_intermediate = False self.add(markdups) self.set_capture_bam(unique_capture, markdups.output_bam) self.qc_files.append(markdups.output_metrics)","title":"merge_and_rm_dup"},{"location":"lld/clinseq/#configure_fastq_qcs","text":"Configure QC on all fastq files that exist for this pipeline instance. return: List of qc output filenames. def configure_fastq_qcs(self): for clinseq_barcode in self.get_all_clinseq_barcodes(): curr_fqs = reduce(lambda l1, l2: l1 + l2, find_fastqs(clinseq_barcode, self.libdir)) for fq in curr_fqs: fastqc = FastQC() fastqc.input = fq fastqc.outdir = {}/qc/fastqc/ .format(self.outdir) fastqc.output = {}/qc/fastqc/{}_fastqc.zip .format( self.outdir, clinseq_barcode) fastqc.jobname = fastqc-{} .format(clinseq_barcode) self.qc_files.append(fastqc.output) self.add(fastqc)","title":"configure_fastq_qcs"},{"location":"lld/clinseq/#configure_align_and_merge","text":"Configure the aligning of the fastq files for all clinseq barcodes in this pipeline, and configure merging of the resulting bam files organised according to unique sample library captures (including \"WGS\" captures - i.e. no capture). def configure_align_and_merge(self): capture_to_barcodes = self.get_unique_capture_to_clinseq_barcodes() for unique_capture in capture_to_barcodes.keys(): curr_bamfiles = [] capture_kit = unique_capture.capture_kit_id for clinseq_barcode in capture_to_barcodes[unique_capture]: curr_bamfiles.append( align_library(self, fq1_files=find_fastqs(clinseq_barcode, self.libdir)[0], fq2_files=find_fastqs(clinseq_barcode, self.libdir)[1], clinseq_barcode=clinseq_barcode, ref=self.refdata['bwaIndex'], outdir= {}/bams/{} .format(self.outdir, capture_kit), maxcores=self.maxcores, remove_duplicates=True)) self.merge_and_rm_dup(unique_capture, curr_bamfiles)","title":"configure_align_and_merge"},{"location":"lld/clinseq/#call_germline_variants","text":"Configure calling of germline variants for a normal sample library capture, and configure VEP if specified in the analysis. param normal_capture: The normal sample library capture identifier. param bam: Bam filename input to variant calling. def call_germline_variants(self, normal_capture, bam): targets = self.get_capture_name(normal_capture.capture_kit_id) capture_str = compose_lib_capture_str(normal_capture) freebayes = Freebayes() freebayes.input_bams = [bam] freebayes.somatic_only = False freebayes.params = None freebayes.reference_sequence = self.refdata['reference_genome'] freebayes.target_bed = self.refdata['targets'][targets]['targets-bed-slopped20'] freebayes.threads = self.maxcores freebayes.scratch = self.scratch freebayes.output = {}/variants/{}.freebayes-germline.vcf.gz .format(self.outdir, capture_str) freebayes.jobname = freebayes-germline-{} .format(capture_str) self.add(freebayes) vepped_vcf = None if self.vep_data_is_available(): vep_freebayes = VEP() vep_freebayes.input_vcf = freebayes.output vep_freebayes.threads = self.maxcores vep_freebayes.reference_sequence = self.refdata['reference_genome'] vep_freebayes.vep_dir = self.refdata['vep_dir'] vep_freebayes.output_vcf = {}/variants/{}.freebayes-germline.vep.vcf.gz .format(self.outdir, capture_str) vep_freebayes.jobname = vep-freebayes-germline-{} .format(capture_str) self.add(vep_freebayes) vepped_vcf = vep_freebayes.output_vcf self.set_germline_vcf(normal_capture, (freebayes.output, vepped_vcf))","title":"call_germline_variants"},{"location":"lld/clinseq/#configure_panel_analysis_with_normal","text":"Configure panel analyses focused on a specific unique normal library capture. Configure germline variant calling: For each unique cancer library capture, configure a comparative analysis against this normal capture: def configure_panel_analysis_with_normal(self, normal_capture): if normal_capture.sample_type != N : raise ValueError( Invalid input capture: + compose_sample_str(normal_capture)) normal_bam = self.get_capture_bam(normal_capture) self.call_germline_variants(normal_capture, normal_bam) for cancer_capture in self.get_mapped_captures_cancer(): self.configure_panel_analysis_cancer_vs_normal( normal_capture, cancer_capture)","title":"configure_panel_analysis_with_normal"},{"location":"lld/clinseq/#configure_make_cnvkit_tracks","text":"def configure_make_cnvkit_tracks(self, unique_capture): input_cnr = self.capture_to_results[unique_capture].cnr input_cns = self.capture_to_results[unique_capture].cns sample_str = compose_lib_capture_str(unique_capture) make_cnvkit_tracks = MakeCNVkitTracks() make_cnvkit_tracks.input_cnr = input_cnr make_cnvkit_tracks.input_cns = input_cns make_cnvkit_tracks.output_profile_bedgraph = {}/cnv/{}_profile.bedGraph .format( self.outdir, sample_str) make_cnvkit_tracks.output_segments_bedgraph = {}/cnv/{}_segments.bedGraph .format( self.outdir, sample_str) self.add(make_cnvkit_tracks)","title":"configure_make_cnvkit_tracks"},{"location":"lld/clinseq/#configure_fix_cnvkit","text":"Configure a job to fix the cnvkit output for the specified unique capture. param unique_capture: Named tuple identifying a sample library capture. param cnr: String indicating unfixed cnr file location param cns: String indicating unfixed cns file location param cnvkit_fix_filename: File containing table of data used to fix the outputs. def configure_fix_cnvkit(self, unique_capture, cnr, cns, cnvkit_fix_filename): sample_str = compose_lib_capture_str(unique_capture) cnvkit_fix = CNVkitFix(input_cnr=cnr, input_cns=cns, input_ref=cnvkit_fix_filename, output_cnr= {}/cnv/{}-fixed.cnr .format(self.outdir, sample_str), output_cns= {}/cnv/{}-fixed.cns .format(self.outdir, sample_str)) self.set_capture_cnr(unique_capture, cnvkit_fix.output_cnr) self.set_capture_cns(unique_capture, cnvkit_fix.output_cns) self.add(cnvkit_fix)","title":"configure_fix_cnvkit"},{"location":"lld/clinseq/#configure_single_capture_analysis","text":"Configure all general analyses to perform given a single sample library capture. def configure_single_capture_analysis(self, unique_capture): input_bam = self.get_capture_bam(unique_capture) sample_str = compose_lib_capture_str(unique_capture) capture_kit_name = self.get_capture_name(unique_capture.capture_kit_id) library_kit_name = self.get_prep_kit_name(unique_capture.library_kit_id) sample_type = unique_capture.sample_type # Configure CNV kit analysis: cnvkit = CNVkit(input_bam=input_bam, output_cnr= {}/cnv/{}.cnr .format(self.outdir, sample_str), output_cns= {}/cnv/{}.cns .format(self.outdir, sample_str), scratch=self.scratch) # FIXME: Improve this messy code for extracting the relevant cnvkit reference from self.refdata: cnvkit.reference = None if 'cnvkit-ref' in self.refdata['targets'][capture_kit_name]: # Retrieve the first (in arbitrary order) reference available for this capture kit, # as a fall-back: cnvkit.reference = self.refdata['targets'][capture_kit_name]['cnvkit-ref'].values()[0].values()[0] try: # Try to get a more specific reference, if it available: cnvkit.reference = self.refdata['targets'][capture_kit_name]['cnvkit-ref'][library_kit_name][sample_type] except KeyError: pass if not cnvkit.reference: cnvkit.targets_bed = self.refdata['targets'][capture_kit_name]['targets-bed-slopped20'] cnvkit.fasta = self.refdata[ reference_genome ] cnvkit.jobname = cnvkit/{} .format(sample_str) # Register the result of this analysis: self.set_capture_cnr(unique_capture, cnvkit.output_cnr) self.set_capture_cns(unique_capture, cnvkit.output_cns) # FIXME: This extra step (fixing the cnv kit output) should perhaps go elsewhere. try: # Only fix the CNV-kit output if the required file is available: cnvkit_fix_filename = \\ self.refdata['targets'][capture_kit_name][ cnvkit-fix ][library_kit_name][sample_type] self.configure_fix_cnvkit(unique_capture, cnvkit.output_cnr, cnvkit.output_cns, cnvkit_fix_filename) except KeyError: pass self.add(cnvkit) # Configure conversion of CNV kit output to seg format: seg_filename = {}/cnv/{}.seg .format( self.outdir, sample_str) cns2seg = Cns2Seg(self.capture_to_results[unique_capture].cns, seg_filename) self.add(cns2seg) self.set_capture_seg(unique_capture, cns2seg.output_seg)","title":"configure_single_capture_analysis"},{"location":"lld/clinseq/#configure_lowpass_analyses","text":"Configure generic analyses of all low-pass whole-genome sequencing data for this clinseq pipeline, under the assumption that alignment and bam file merging has already been performed. def configure_lowpass_analyses(self): for unique_wgs in self.get_mapped_captures_only_wgs(): self.configure_single_wgs_analyses(unique_wgs)","title":"configure_lowpass_analyses"},{"location":"lld/clinseq/#configure_make_qdnaseq_tracks","text":"def configure_make_qdnaseq_tracks(self, qdnaseq_output, sample_str): make_qdnaseq_tracks = MakeQDNAseqTracks() make_qdnaseq_tracks.input_qdnaseq_file = qdnaseq_output make_qdnaseq_tracks.output_segments_bedgraph = {}/cnv/{}_qdnaseq_segments.bedGraph .format( self.outdir, sample_str) make_qdnaseq_tracks.output_copynumber_tdf = {}/cnv/{}_qdnaseq_copynumber.tdf .format( self.outdir, sample_str) make_qdnaseq_tracks.output_readcount_tdf = {}/cnv/{}_qdnaseq_readcount.tdf .format( self.outdir, sample_str) self.add(make_qdnaseq_tracks)","title":"configure_make_qdnaseq_tracks"},{"location":"lld/clinseq/#configure_single_wgs_analyses","text":"Configure generic analyses of a single WGS item in the pipeline. param unique_wgs: An identifier for a single unique library WGS. def configure_single_wgs_analyses(self, unique_wgs): input_bam = self.get_capture_bam(unique_wgs) sample_str = compose_lib_capture_str(unique_wgs) qdnaseq = QDNASeq(input_bam, output_segments= {}/cnv/{}-qdnaseq.segments.txt .format( self.outdir, sample_str), background=None) self.configure_make_qdnaseq_tracks(qdnaseq.output, sample_str) self.add(qdnaseq)","title":"configure_single_wgs_analyses"},{"location":"lld/clinseq/#run_wgs_bam_qc","text":"Run QC on wgs bams param bams: list of bams return: list of generated files def run_wgs_bam_qc(self, bams): qc_files = [] logging.debug( bams are {} .format(bams)) for bam in bams: basefn = stripsuffix(os.path.basename(bam), .bam ) isize = PicardCollectInsertSizeMetrics() isize.input = bam isize.jobname = picard-isize-{} .format(basefn) isize.output_metrics = {}/qc/picard/wgs/{}.picard-insertsize.txt .format(self.outdir, basefn) self.add(isize) wgsmetrics = PicardCollectWgsMetrics() wgsmetrics.input = bam wgsmetrics.reference_sequence = self.refdata['reference_genome'] wgsmetrics.output_metrics = {}/qc/picard/wgs/{}.picard-wgsmetrics.txt .format(self.outdir, basefn) wgsmetrics.jobname = picard-wgsmetrics-{} .format(basefn) self.add(wgsmetrics) qc_files += [isize.output_metrics, wgsmetrics.output_metrics] return qc_files","title":"run_wgs_bam_qc"},{"location":"lld/clinseq/#configure_panel_analyses","text":"Configure generic analyses of all panel data for this clinseq pipeline, assuming that alignment and bam file merging has been performed. def configure_panel_analyses(self): # Configure analyses to be run on all unique panel captures individually: for unique_capture in self.get_mapped_captures_no_wgs(): self.configure_single_capture_analysis(unique_capture) self.configure_make_cnvkit_tracks(unique_capture) # Configure a separate group of analyses for each unique normal library capture: for normal_capture in self.get_mapped_captures_normal(): self.configure_panel_analysis_with_normal(normal_capture)","title":"configure_panel_analyses"},{"location":"lld/clinseq/#configure_panel_msings_analyses","text":"Configure msings analyses for all unique captures for which this is possible. def configure_panel_msings_analyses(self): for unique_capture in self.get_mapped_captures_cancer(): try: self.configure_msings(unique_capture) except InvalidRefDataException: # This indicates the reference data does not support configuring # msings for this cancer capture = Ignore this and proceed to the next: pass","title":"configure_panel_msings_analyses"},{"location":"lld/clinseq/#configure_somatic_calling","text":"Configure somatic variant calling in this pipeline, for a specified pairing of normal and cancer library capture events. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_somatic_calling(self, normal_capture, cancer_capture): cancer_bam = self.get_capture_bam(cancer_capture) normal_bam = self.get_capture_bam(normal_capture) target_name = self.get_capture_name(cancer_capture.capture_kit_id) # FIXME: Need to fix the configuration of the min_alt_frac threshold, rather than hard-coding it here: somatic_variants = call_somatic_variants( self, cancer_bam=cancer_bam, normal_bam=normal_bam, cancer_capture=cancer_capture, normal_capture=normal_capture, target_name=target_name, outdir=self.outdir, callers=['vardict'], min_alt_frac=self.get_job_param('vardict-min-alt-frac'), min_num_reads=self.get_job_param('vardict-min-num-reads')) self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].somatic_vcf = \\ somatic_variants.values()[0]","title":"configure_somatic_calling"},{"location":"lld/clinseq/#configure_vep","text":"def configure_vep(self, normal_capture, cancer_capture): if not self.vep_data_is_available(): raise ValueError( Invalid call to configure_vep: No vep data available. ) somatic_vcf = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].somatic_vcf cancer_capture_str = compose_lib_capture_str(cancer_capture) normal_capture_str = compose_lib_capture_str(normal_capture) vep = VEP() vep.input_vcf = somatic_vcf vep.threads = self.maxcores vep.reference_sequence = self.refdata['reference_genome'] vep.vep_dir = self.refdata['vep_dir'] vep.output_vcf = {}/variants/{}-{}.somatic.vep.vcf.gz .format( self.outdir, cancer_capture_str, normal_capture_str) vep.jobname = vep-freebayes-somatic/{} .format(cancer_capture_str) vep.additional_options = self.get_job_param( vep-additional-options ) self.add(vep) self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].vepped_vcf = \\ vep.output_vcf","title":"configure_vep"},{"location":"lld/clinseq/#configure_make_allelic_fraction_track","text":"Configure a small job for converting the germline variant somatic allelic fraction information into tracks for displaying in IGV. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_make_allelic_fraction_track(self, normal_capture, cancer_capture): vcf = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].vcf_addsample_output make_allelic_fraction_track = MakeAllelicFractionTrack() make_allelic_fraction_track.input_vcf = vcf normal_capture_str = compose_lib_capture_str(normal_capture) cancer_capture_str = compose_lib_capture_str(cancer_capture) make_allelic_fraction_track.output_bedgraph = \\ {}/variants/{}-and-{}.germline-variants-somatic-afs.bedGraph .format( self.outdir, normal_capture_str, cancer_capture_str) self.add(make_allelic_fraction_track)","title":"configure_make_allelic_fraction_track"},{"location":"lld/clinseq/#configure_vcf_add_sample","text":"Configure VCF updating in this pipeline, for a specified pairing of normal and cancer library capture events. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_vcf_add_sample(self, normal_capture, cancer_capture): # Configure VCF add sample: vcfaddsample = VcfAddSample() vcfaddsample.input_bam = self.get_capture_bam(cancer_capture) vcfaddsample.input_vcf = self.get_germline_vcf(normal_capture) normal_capture_str = compose_lib_capture_str(normal_capture) cancer_capture_str = compose_lib_capture_str(cancer_capture) cancer_sample_str = compose_sample_str(cancer_capture) vcfaddsample.samplename = cancer_sample_str vcfaddsample.filter_hom = True vcfaddsample.output = {}/variants/{}-and-{}.germline-variants-with-somatic-afs.vcf.gz .format( self.outdir, normal_capture_str, cancer_capture_str) vcfaddsample.jobname = vcf-add-sample-{} .format(cancer_capture_str) self.add(vcfaddsample) self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].vcf_addsample_output = \\ vcfaddsample.output","title":"configure_vcf_add_sample"},{"location":"lld/clinseq/#configure_msi_sensor","text":"Configure MSI sensor in this pipeline, for a specified pairing of normal and cancer library capture events. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_msi_sensor(self, normal_capture, cancer_capture): # Configure MSI sensor: msisensor = MsiSensor() cancer_capture_name = self.get_capture_name(cancer_capture.capture_kit_id) msisensor.msi_sites = self.refdata['targets'][cancer_capture_name]['msisites'] msisensor.input_normal_bam = self.get_capture_bam(normal_capture) msisensor.input_tumor_bam = self.get_capture_bam(cancer_capture) normal_capture_str = compose_lib_capture_str(normal_capture) cancer_capture_str = compose_lib_capture_str(cancer_capture) msisensor.output = {}/msisensor-{}-{}.tsv .format( self.outdir, normal_capture_str, cancer_capture_str) msisensor.threads = self.maxcores msisensor.jobname = msisensor-{}-{} .format(normal_capture_str, cancer_capture_str) self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].msi_output = \\ msisensor.output self.add(msisensor)","title":"configure_msi_sensor"},{"location":"lld/clinseq/#configure_msings","text":"Configure msings analysis, which operates on a cancer capture bam input file. param cancer_capture: Named tuple indicating cancer library capture. def configure_msings(self, cancer_capture): # Configure Msings: msings = Msings() cancer_capture_name = self.get_capture_name(cancer_capture.capture_kit_id) msings.input_fasta = self.refdata['reference_genome'] try: msings.msings_baseline = self.refdata['targets'][cancer_capture_name]['msings-baseline'] msings.msings_bed = self.refdata['targets'][cancer_capture_name]['msings-bed'] msings.msings_intervals = self.refdata['targets'][cancer_capture_name]['msings-msi_intervals'] except KeyError: raise InvalidRefDataException( Missing msings data. ) # FIXME: The above logic could be cleaned up. if not msings.msings_bed: raise InvalidRefDataException( Missing msings data. ) msings.input_bam = self.get_capture_bam(cancer_capture) cancer_capture_str = compose_lib_capture_str(cancer_capture) msings.outdir = {}/msings-{} .format( self.outdir, cancer_capture_str) # FIXME: This is nasty: bam_name = os.path.splitext(os.path.basename(msings.input_bam))[0] msings.output = {}/{}/{}.MSI_Analysis.txt .format( msings.outdir, bam_name, bam_name) msings.threads = self.maxcores msings.jobname = msings-{} .format(cancer_capture_str) self.capture_to_results[cancer_capture].msings_output = msings.output self.add(msings)","title":"configure_msings"},{"location":"lld/clinseq/#configure_hz_conc","text":"Configure heterozygote concordance calculation in this pipeline, for a specified pairing of normal and cancer library capture events. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_hz_conc(self, normal_capture, cancer_capture): # Configure heterozygote concordance: hzconcordance = HeterzygoteConcordance() hzconcordance.input_vcf = self.get_germline_vcf(normal_capture) hzconcordance.input_bam = self.get_capture_bam(cancer_capture) hzconcordance.reference_sequence = self.refdata['reference_genome'] cancer_capture_name = self.get_capture_name(cancer_capture.capture_kit_id) hzconcordance.target_regions = \\ self.refdata['targets'][cancer_capture_name]['targets-interval_list-slopped20'] hzconcordance.normalid = compose_sample_str(normal_capture) hzconcordance.filter_reads_with_N_cigar = True hzconcordance.jobname = hzconcordance-{} .format(compose_lib_capture_str(cancer_capture)) hzconcordance.output = {}/bams/{}-{}-hzconcordance.txt .format( self.outdir, compose_lib_capture_str(cancer_capture), compose_lib_capture_str(normal_capture)) self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].hzconcordance_output = \\ hzconcordance.output self.add(hzconcordance)","title":"configure_hz_conc"},{"location":"lld/clinseq/#configure_contest_vcf_generation","text":"Configure generation of a contest VCF input file in this pipeline, for a specified pairing of normal and cancer library capture events. param normal_capture: Named tuple indicating normal library capture. param cancer_capture: Named tuple indicating cancer library capture. def configure_contest_vcf_generation(self, normal_capture, cancer_capture): contest_vcf_generation = CreateContestVCFs() normal_capture_name = self.get_capture_name(normal_capture.capture_kit_id) cancer_capture_name = self.get_capture_name(cancer_capture.capture_kit_id) normal_targets = self.refdata['targets'][normal_capture_name]['targets-bed-slopped20'] cancer_targets = self.refdata['targets'][cancer_capture_name]['targets-bed-slopped20'] contest_vcf_generation.input_target_regions_bed_1 = normal_targets contest_vcf_generation.input_target_regions_bed_2 = cancer_targets contest_vcf_generation.input_population_vcf = self.refdata[ swegene_common ] normal_capture_str = compose_lib_capture_str(normal_capture) cancer_capture_str = compose_lib_capture_str(cancer_capture) contest_vcf_generation.output = {}/contamination/pop_vcf_{}-{}.vcf .format( self.outdir, normal_capture_str, cancer_capture_str) contest_vcf_generation.jobname = contest_pop_vcf_{}-{} .format( normal_capture_str, cancer_capture_str) self.add(contest_vcf_generation) return contest_vcf_generation.output","title":"configure_contest_vcf_generation"},{"location":"lld/clinseq/#configure_contest","text":"Configure running of ContEst in this pipeline, for a specified pairing of library capture events. Estimates contamination in the bam file for the first library capture, using the bam file for the second library capture as a reference comparison. param library_capture_1: Named tuple indicating first library capture. param library_capture_2: Named tuple indicating second library capture. param contest_vcf: Contest population allele frequency VCF input file. def configure_contest(self, library_capture_1, library_capture_2, contest_vcf): contest = ContEst() contest.reference_genome = self.refdata['reference_genome'] contest.input_eval_bam = self.get_capture_bam(library_capture_1) contest.input_genotype_bam = self.get_capture_bam(library_capture_2) contest.input_population_af_vcf = contest_vcf # TODO: Is it necessary to create the output subdir contamination somewhere? Check how it's done for e.g. cnvkit. contest.output = {}/contamination/{}.contest.txt .format(self.outdir, compose_lib_capture_str(library_capture_1)) # TODO: Should the analysis id also be in name of out file? contest.jobname = contest_tumor/{} .format(compose_lib_capture_str(library_capture_1)) # TODO: Is it ok that the job name does not contain analysis id, i.e. may not be unique? self.add(contest) return contest.output","title":"configure_contest"},{"location":"lld/clinseq/#configure_contam_qc_call","text":"Configure generation of a contamination QC call in this pipeline, based on the specified contest output. Returns the resulting QC output filename. param contest_output: ContEst output filename. param library_capture: Named tuple identifying a unique library capture. def configure_contam_qc_call(self, contest_output, library_capture): process_contest = ContEstToContamCaveat() process_contest.input_contest_results = contest_output process_contest.output = {}/qc/{}-contam-qc-call.json .format( self.outdir, compose_lib_capture_str(library_capture)) self.add(process_contest) return process_contest.output","title":"configure_contam_qc_call"},{"location":"lld/clinseq/#configure_contamination_estimate","text":"Configure contamination estimatates for a given normal, cancer library capture pairing. param normal_capture: Namedtuple indicating a normal library capture. param cancer_capture: Namedtuple indicating a cancer library capture. def configure_contamination_estimate(self, normal_capture, cancer_capture): # Configure generation of the contest VCF input file: intersection_contest_vcf = \\ self.configure_contest_vcf_generation(normal_capture, cancer_capture) # Configure contest for calculating contamination in the cancer sample: cancer_vs_normal_contest_output = \\ self.configure_contest(cancer_capture, normal_capture, intersection_contest_vcf) # Configure contest for calculating contamination in the normal sample: normal_vs_cancer_contest_output = \\ self.configure_contest(normal_capture, cancer_capture, intersection_contest_vcf) # Configure cancer sample contamination QC call: cancer_contam_call = self.configure_contam_qc_call(cancer_vs_normal_contest_output, cancer_capture) # Register the outputs of running contest: self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].normal_contest_output = \\ normal_vs_cancer_contest_output self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].cancer_contest_output = \\ cancer_vs_normal_contest_output self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].cancer_contam_call = \\ cancer_contam_call","title":"configure_contamination_estimate"},{"location":"lld/clinseq/#configure_purecn","text":"Configure PureCN, and also configure the custom run of VarDict, which is required for PureCN. :param normal_capture: A unique normal sample library capture :param cancer_capture: A unique cancer sample library capture def configure_purecn(self, normal_capture, cancer_capture): cancer_bam = self.get_capture_bam(cancer_capture) normal_bam = self.get_capture_bam(normal_capture) target_name = self.get_capture_name(cancer_capture.capture_kit_id) cancer_capture_str = compose_lib_capture_str(cancer_capture) capture_name = self.get_capture_name(cancer_capture.capture_kit_id) normal_capture_str = compose_lib_capture_str(normal_capture) # Configure the PureCN-specific VarDict job: vardict_pureCN = VarDictForPureCN() vardict_pureCN.input_tumor = cancer_bam vardict_pureCN.input_normal = normal_bam vardict_pureCN.tumorid = cancer_capture_str vardict_pureCN.normalid = normal_capture_str vardict_pureCN.reference_sequence = self.refdata['reference_genome'] vardict_pureCN.reference_dict = self.refdata['reference_dict'] vardict_pureCN.target_bed = self.refdata['targets'][target_name]['targets-bed-slopped20'] vardict_pureCN.dbsnp = self.refdata[ dbSNP ] vardict_pureCN.output = {}/variants/{}-{}.vardict-somatic-purecn.vcf.gz .format( self.outdir, cancer_capture_str, normal_capture_str) vardict_pureCN.jobname = vardict_purecn/{}-{} .format(cancer_capture_str, normal_capture_str) self.add(vardict_pureCN) # Retrieve the relevant seg-format file: seg_filename = self.capture_to_results[cancer_capture].seg # Configure PureCN itself: # FIXME: NOTE: The Job params *must* be specified as arguments in the case of PureCN: pureCN = PureCN( input_seg=seg_filename, input_vcf=vardict_pureCN.output, tumorid=cancer_capture_str, gcgene_file=self.refdata['targets'][capture_name]['purecn_targets'], outdir= {}/purecn .format(self.outdir), postopt=True, ) self.add(pureCN) # FIXME: It seems like a nasty hack to include a dictionary here, perhaps this belongs elsewhere? self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].pureCN_outputs = { csv : {}/{}.csv .format(pureCN.outdir, pureCN.tumorid), genes_csv : {}/{}_genes.csv .format(pureCN.outdir, pureCN.tumorid), loh_csv : {}/{}_loh.csv .format(pureCN.outdir, pureCN.tumorid), variants_csv : {}/{}_variants.csv .format(pureCN.outdir, pureCN.tumorid), }","title":"configure_purecn"},{"location":"lld/clinseq/#configure_panel_analysis_cancer_vs_normal","text":"Configures standard paired cancer vs normal panel analyses for the specified unique normal and cancer library captures. Comprises the following analyses: - Somatic variant calling - Running VEP on the resulting somatic VCF - Updating of the germline VCF to take into consideration the cancer sample - MSI sensor - Heterozygote concordance of the sample pair - Contamination estimate of cancer compared with normal and vice versa param normal_capture: A unique normal sample library capture param cancer_capture: A unique cancer sample library capture def configure_panel_analysis_cancer_vs_normal(self, normal_capture, cancer_capture): self.configure_somatic_calling(normal_capture, cancer_capture) if self.vep_data_is_available(): self.configure_vep(normal_capture, cancer_capture) self.configure_vcf_add_sample(normal_capture, cancer_capture) self.configure_make_allelic_fraction_track(normal_capture, cancer_capture) self.configure_msi_sensor(normal_capture, cancer_capture) self.configure_hz_conc(normal_capture, cancer_capture) self.configure_contamination_estimate(normal_capture, cancer_capture)","title":"configure_panel_analysis_cancer_vs_normal"},{"location":"lld/clinseq/#configure_all_lowpass_qcs","text":"Configure QC checks for all low-pass whole genome data in this pipeline. def configure_all_lowpass_qcs(self): for unique_wgs in self.get_mapped_captures_only_wgs(): self.qc_files += \\ self.configure_wgs_qc(unique_wgs)","title":"configure_all_lowpass_qcs"},{"location":"lld/clinseq/#configure_wgs_qc","text":"Configure QC checks for the specified unique WGS item in this pipeline. param unique_wgs: A named tuple identifying a single unique WGS item. return: QC files output files resulting from the QC analysis configuration. def configure_wgs_qc(self, unique_wgs): bam = self.get_capture_bam(unique_wgs) wgs_name = compose_lib_capture_str(unique_wgs) qc_files = [] isize = PicardCollectInsertSizeMetrics() isize.input = bam isize.jobname = picard-isize-{} .format(wgs_name) isize.output_metrics = {}/qc/picard/wgs/{}.picard-insertsize.txt .format(self.outdir, wgs_name) self.add(isize) wgsmetrics = PicardCollectWgsMetrics() wgsmetrics.input = bam wgsmetrics.reference_sequence = self.refdata['reference_genome'] wgsmetrics.output_metrics = {}/qc/picard/wgs/{}.picard-wgsmetrics.txt .format(self.outdir, wgs_name) wgsmetrics.jobname = picard-wgsmetrics-{} .format(wgs_name) self.add(wgsmetrics) qc_files += [isize.output_metrics, wgsmetrics.output_metrics] return qc_files","title":"configure_wgs_qc"},{"location":"lld/clinseq/#configure_all_panel_qcs","text":"Configures QC checks for all panel data (not including WGS data) in this pipeline. def configure_all_panel_qcs(self): for unique_capture in self.get_mapped_captures_no_wgs(): self.qc_files += \\ self.configure_panel_qc(unique_capture)","title":"configure_all_panel_qcs"},{"location":"lld/clinseq/#configure_multi_qc","text":"Configures MultiQC for this pipeline. self.qc_files must be fully populated in order for MultiQC to use all relevant input files. def configure_multi_qc(self): multiqc = MultiQC() multiqc.input_files = self.qc_files multiqc.search_dir = self.outdir multiqc.output = {}/multiqc/{}-multiqc .format(self.outdir, self.analysis_id) multiqc.jobname = multiqc-{} .format(self.sampledata['sdid']) self.add(multiqc)","title":"configure_multi_qc"},{"location":"lld/clinseq/#get_coverage_bed","text":"Retrieve the targets bed file to use for calculating coverage, given the specified targets name. param targets: Target capture name return: bed file name def get_coverage_bed(self, targets): return self.refdata['targets'][targets]['targets-bed-slopped20']","title":"get_coverage_bed"},{"location":"lld/clinseq/#configure_panel_qc","text":"Configure QC analyses for a given library capture. param unique_capture: Named tuple identifying a sample library capture. return: list of QC output files for this capture. def configure_panel_qc(self, unique_capture): bam = self.get_capture_bam(unique_capture) targets = self.get_capture_name(unique_capture.capture_kit_id) logging.debug( Adding QC jobs for {} .format(bam)) capture_str = compose_lib_capture_str(unique_capture) isize = PicardCollectInsertSizeMetrics() isize.input = bam isize.output_metrics = {}/qc/picard/{}/{}.picard-insertsize.txt .format( self.outdir, unique_capture.capture_kit_id, capture_str) isize.jobname = picard-isize-{} .format(capture_str) self.add(isize) oxog = PicardCollectOxoGMetrics() oxog.input = bam oxog.reference_sequence = self.refdata['reference_genome'] oxog.output_metrics = {}/qc/picard/{}/{}.picard-oxog.txt .format( self.outdir, unique_capture.capture_kit_id, capture_str) oxog.jobname = picard-oxog-{} .format(capture_str) self.add(oxog) hsmetrics = PicardCollectHsMetrics() hsmetrics.input = bam hsmetrics.reference_sequence = self.refdata['reference_genome'] hsmetrics.target_regions = self.refdata['targets'][targets][ 'targets-interval_list-slopped20'] hsmetrics.bait_regions = self.refdata['targets'][targets][ 'targets-interval_list-slopped20'] hsmetrics.bait_name = targets hsmetrics.output_metrics = {}/qc/picard/{}/{}.picard-hsmetrics.txt .format( self.outdir, unique_capture.capture_kit_id, capture_str) hsmetrics.jobname = picard-hsmetrics-{} .format(capture_str) self.add(hsmetrics) sambamba = SambambaDepth() sambamba.targets_bed = self.refdata['targets'][targets]['targets-bed-slopped20'] sambamba.input = bam sambamba.output = {}/qc/sambamba/{}.sambamba-depth-targets.txt .format( self.outdir, capture_str) sambamba.jobname = sambamba-depth-{} .format(capture_str) self.add(sambamba) coverage_hist = CoverageHistogram() # FIXME: Ugly temporary solution to allow the alascca pipeline to use a specific # targets file: coverage_hist.input_bed = self.get_coverage_bed(targets) coverage_hist.input_bam = bam coverage_hist.output = {}/qc/{}.coverage-histogram.txt .format( self.outdir, capture_str) coverage_hist.jobname = alascca-coverage-hist/{} .format(capture_str) self.add(coverage_hist) coverage_qc_call = CoverageCaveat() coverage_qc_call.low_thresh_fraction = self.get_job_param('cov-low-thresh-fraction') coverage_qc_call.low_thresh_fold_cov = self.get_job_param('cov-low-thresh-fold-cov') coverage_qc_call.input_histogram = coverage_hist.output coverage_qc_call.output = {}/qc/{}.coverage-qc-call.json .format(self.outdir, capture_str) coverage_qc_call.jobname = coverage-qc-call/{} .format(capture_str) self.add(coverage_qc_call) self.capture_to_results[unique_capture].cov_qc_call = coverage_qc_call.output return [isize.output_metrics, oxog.output_metrics, hsmetrics.output_metrics, sambamba.output, coverage_hist.output, coverage_qc_call.output]","title":"configure_panel_qc"},{"location":"lld/contest/","text":"CreateContestVCFs Runs create_contest_vcfs.py to generate a ContEst population allele frequency VCF input file, given an overall population VCF file and a pair of target region bed files. class CreateContestVCFs(Job): def __init__(self): Job.__init__(self) self.input_target_regions_bed_1 = None self.input_target_regions_bed_2 = None self.input_population_vcf = None self.output = None self.jobname = create_contest_vcfs def command(self): return create_contest_vcfs.py + \\ required( , self.input_target_regions_bed_1) + \\ required( , self.input_target_regions_bed_2) + \\ required( , self.input_population_vcf) + \\ required( --output-filename , self.output) ConEst Runs ContEst to estimate contamination level in bam file \"input_eval_bam\". class ContEst(Job): def __init__(self): Job.__init__(self) self.reference_genome = None self.input_eval_bam = None self.input_genotype_bam = None self.input_population_af_vcf = None self.output = None self.jobname = contest def command(self): min_genotype_ratio = 0.95 return java -Xmx15g -jar /nfs/ALASCCA/autoseq-scripts/GenomeAnalysisTK-3.5.jar -T ContEst + \\ required( -R , self.reference_genome) + \\ required( -I:eval , self.input_eval_bam) + \\ required( -I:genotype , self.input_genotype_bam) + \\ required( --popfile , self.input_population_af_vcf) + \\ required( --min_genotype_ratio , min_genotype_ratio) + \\ required( -o , self.output) ContEstToContamCaveat Runs script to convert ContEst output to JSON file with contamination QC estimatimate. class ContEstToContamCaveat(Job): def __init__(self): Job.__init__(self) self.input_contest_results = None self.output = None self.jobname = contest_to_contam_qc def command(self): return contest_to_contam_caveat.py + \\ required( , self.input_contest_results) + \\ required( , self.output)","title":"Contest"},{"location":"lld/contest/#createcontestvcfs","text":"Runs create_contest_vcfs.py to generate a ContEst population allele frequency VCF input file, given an overall population VCF file and a pair of target region bed files. class CreateContestVCFs(Job): def __init__(self): Job.__init__(self) self.input_target_regions_bed_1 = None self.input_target_regions_bed_2 = None self.input_population_vcf = None self.output = None self.jobname = create_contest_vcfs def command(self): return create_contest_vcfs.py + \\ required( , self.input_target_regions_bed_1) + \\ required( , self.input_target_regions_bed_2) + \\ required( , self.input_population_vcf) + \\ required( --output-filename , self.output)","title":"CreateContestVCFs"},{"location":"lld/contest/#conest","text":"Runs ContEst to estimate contamination level in bam file \"input_eval_bam\". class ContEst(Job): def __init__(self): Job.__init__(self) self.reference_genome = None self.input_eval_bam = None self.input_genotype_bam = None self.input_population_af_vcf = None self.output = None self.jobname = contest def command(self): min_genotype_ratio = 0.95 return java -Xmx15g -jar /nfs/ALASCCA/autoseq-scripts/GenomeAnalysisTK-3.5.jar -T ContEst + \\ required( -R , self.reference_genome) + \\ required( -I:eval , self.input_eval_bam) + \\ required( -I:genotype , self.input_genotype_bam) + \\ required( --popfile , self.input_population_af_vcf) + \\ required( --min_genotype_ratio , min_genotype_ratio) + \\ required( -o , self.output)","title":"ConEst"},{"location":"lld/contest/#contesttocontamcaveat","text":"Runs script to convert ContEst output to JSON file with contamination QC estimatimate. class ContEstToContamCaveat(Job): def __init__(self): Job.__init__(self) self.input_contest_results = None self.output = None self.jobname = contest_to_contam_qc def command(self): return contest_to_contam_caveat.py + \\ required( , self.input_contest_results) + \\ required( , self.output)","title":"ContEstToContamCaveat"},{"location":"lld/generateref/","text":"GenerateRefFilesPipeline To generate reference files from class GenerateRefFilesPipeline(PypedreamPipeline): outdir = None maxcores = None def __init__(self, genome_resources, outdir, maxcores=1, runner=Shellrunner()): PypedreamPipeline.__init__(self, normpath(outdir), runner=runner) self.genome_resources = genome_resources self.input_reference_sequence = {}/human_g1k_v37_decoy.fasta.gz .format(genome_resources) self.cosmic_vcf = {}/CosmicCodingMuts_v71.vcf.gz .format(genome_resources) self.qdnaseq_background = {}/qdnaseq_background.Rdata .format(genome_resources) self.swegene_common_vcf = {}/swegen_common.vcf.gz .format(genome_resources) self.outdir = outdir self.maxcores = maxcores self.reference_data = dict() self.exac_remote = ftp://ftp.broadinstitute.org/pub/ExAC_release/release0.3.1/ExAC.r0.3.1.sites.vep.vcf.gz self.dbsnp_remote = ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b149_GRCh37p13/VCF/All_20161121.vcf.gz self.clinvar_remote = ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh37/archive_1.0/2016/clinvar_20160203.vcf.gz self.icgc_somatic_remote = https://dcc.icgc.org/api/v1/download?fn=/release_20/Summary/simple_somatic_mutation.aggregated.vcf.gz self.ensembl_version = 75 self.ensembl_gtf_remote = ftp://ftp.ensembl.org/pub/release- + self.ensembl_version + \\ /gtf/homo_sapiens/Homo_sapiens.GRCh37. + self.ensembl_version + .gtf.gz self.mitranscriptome_remote = http://mitranscriptome.org/download/mitranscriptome.gtf.tar.gz self.prepare_reference_genome() self.prepare_genes() self.prepare_sveffect_regions() self.prepare_intervals() self.prepare_variants() fetch_vep_cache = InstallVep() fetch_vep_cache.output_dir = {}/vep/ .format(self.outdir) self.add(fetch_vep_cache) self.reference_data['vep_dir'] = fetch_vep_cache.output_dir self.make_ref_paths_relative() with open( {}/autoseq-genome.json .format(self.outdir), w ) as output_file: json.dump(self.reference_data, output_file, indent=4, sort_keys=True) prepare_sveffect_regions def prepare_sveffect_regions(self): for regions_name in [ ar_regions , ts_regions , fusion_regions ]: file_full_path = {}/{}.bed .format( self.genome_resources, regions_name, ) copy_regions = Copy(input_file=file_full_path, output_file= {}/intervals/{} .format( self.outdir, os.path.basename(file_full_path), )) self.reference_data[regions_name] = copy_regions.output self.add(copy_regions) prepare_variants def prepare_variants(self): curl_dbsnp = CurlSplitAndLeftAlign() curl_dbsnp.input_reference_sequence = self.reference_data['reference_genome'] curl_dbsnp.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_dbsnp.remote = self.dbsnp_remote curl_dbsnp.output = {}/variants/{} .format(self.outdir, os.path.basename(self.dbsnp_remote)) curl_dbsnp.is_intermediate = True self.add(curl_dbsnp) filter_dbsnp = VcfFilter() filter_dbsnp.input = curl_dbsnp.output filter_dbsnp.filter = \\ ! ( SAO = 3 | SAO = 2 )\\ filter_dbsnp.output = {}/variants/dbsnp142-germline-only.vcf.gz .format(self.outdir) self.add(filter_dbsnp) curl_cosmic = CurlSplitAndLeftAlign() curl_cosmic.input_reference_sequence = self.reference_data['reference_genome'] curl_cosmic.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_cosmic.remote = file:// + self.cosmic_vcf curl_cosmic.output = {}/variants/{} .format(self.outdir, os.path.basename(self.cosmic_vcf)) self.add(curl_cosmic) curl_clinvar = CurlSplitAndLeftAlign() curl_clinvar.input_reference_sequence = self.reference_data['reference_genome'] curl_clinvar.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_clinvar.remote = self.clinvar_remote curl_clinvar.output = {}/variants/{} .format(self.outdir, os.path.basename(self.clinvar_remote)) self.add(curl_clinvar) curl_exac = CurlSplitAndLeftAlign() curl_exac.input_reference_sequence = self.reference_data['reference_genome'] curl_exac.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_exac.remote = self.exac_remote curl_exac.output = {}/variants/{} .format(self.outdir, os.path.basename(self.exac_remote)) self.add(curl_exac) curl_icgc = CurlSplitAndLeftAlign() curl_icgc.input_reference_sequence = self.reference_data['reference_genome'] curl_icgc.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_icgc.remote = self.icgc_somatic_remote curl_icgc.output = {}/variants/{} .format(self.outdir, icgc_release_20_simple_somatic_mutation.aggregated.vcf.gz ) self.add(curl_icgc) curl_swegene = CurlSplitAndLeftAlign() curl_swegene.input_reference_sequence = self.reference_data['reference_genome'] curl_swegene.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_swegene.remote = file:// + self.swegene_common_vcf curl_swegene.output = {}/variants/{} .format(self.outdir, os.path.basename(self.swegene_common_vcf)) self.add(curl_swegene) self.reference_data['dbSNP'] = filter_dbsnp.output self.reference_data['cosmic'] = curl_cosmic.output self.reference_data['exac'] = curl_exac.output self.reference_data['clinvar'] = curl_clinvar.output self.reference_data['icgc'] = curl_icgc.output self.reference_data['swegene_common'] = curl_swegene.output prepare_cnvkit param: cnv_kit_ref_filename: String name of a cnvkit reference file (either .cnn or .cnvkitref.txt ), to be registered in self.ref_data. def prepare_cnvkit(self, cnv_kit_ref_filename): file_full_path = {}/target_intervals/{} .format(self.genome_resources, cnv_kit_ref_filename) # Extract the capture+library+sampletype strings: capture_library_sampletype = cnv_kit_ref_filename.split( . )[:3] copy_cnvkit_ref = Copy(input_file=file_full_path, output_file= {}/intervals/targets/{} .format(self.outdir, os.path.basename(file_full_path)) ) self.add(copy_cnvkit_ref) capture_name = capture_library_sampletype[0] library_kit_name = capture_library_sampletype[1] sample_type = capture_library_sampletype[2] ref_type = cnvkit-fix if cnv_kit_ref_filename.endswith(( cnn )): ref_type = cnvkit-ref # FIXME: Ugly; refactor. This registers the cnvkit reference file copy in the autoseq genome dictionary: if ref_type not in self.reference_data['targets'][capture_name]: self.reference_data['targets'][capture_name][ref_type] = {} if library_kit_name not in self.reference_data['targets'][capture_name][ref_type]: self.reference_data['targets'][capture_name][ref_type][library_kit_name] = {} self.reference_data['targets'][capture_name][ref_type][library_kit_name][sample_type] = \\ copy_cnvkit_ref.output prepare_msings def prepare_msings(self, filename_base, capture_name): for msings_extn in [ baseline , bed , msi_intervals ]: msings_ref_file = filename_base + .msings. + msings_extn if os.path.exists(msings_ref_file): copy_msings_ref = Copy(input_file=msings_ref_file, output_file= {}/intervals/targets/{} .format(self.outdir, os.path.basename( msings_ref_file)) ) self.add(copy_msings_ref) self.reference_data['targets'][capture_name]['msings-' + msings_extn] = copy_msings_ref.output else: self.reference_data['targets'][capture_name]['msings-' + msings_extn] = None # FIXME: The prepare_intervals() method is becoming very unwieldy. Consider refactoring. prepare_intervals def prepare_intervals(self): self.reference_data['targets'] = {} target_intervals_dir = {}/target_intervals/ .format(self.genome_resources) input_files = [f for f in os.listdir(target_intervals_dir) if f.endswith( .interval_list )] scan_for_microsatellites = MsiSensorScan() scan_for_microsatellites.input_fasta = self.reference_data['reference_genome'] scan_for_microsatellites.homopolymers_only = True scan_for_microsatellites.output = {}/intervals/msisensor-microsatellites.tsv .format(self.outdir) self.add(scan_for_microsatellites) for f in input_files: file_full_path = {}/target_intervals/{} .format(self.genome_resources, f) logging.debug( Parsing intervals file {} .format(file_full_path)) capture_name = stripsuffix(f, .interval_list ) self.reference_data['targets'][capture_name] = {} copy_file = Copy(input_file=file_full_path, output_file= {}/intervals/targets/{} .format(self.outdir, os.path.basename(file_full_path))) self.add(copy_file) slop_interval_list = SlopIntervalList() slop_interval_list.input = copy_file.output slop_interval_list.output = stripsuffix(copy_file.output, .interval_list ) + .slopped20.interval_list self.add(slop_interval_list) interval_list_to_bed = IntervalListToBed() interval_list_to_bed.input = slop_interval_list.output interval_list_to_bed.output = stripsuffix(slop_interval_list.output, .interval_list ) + .bed self.add(interval_list_to_bed) intersect_msi = IntersectMsiSites() intersect_msi.input_msi_sites = scan_for_microsatellites.output intersect_msi.target_bed = interval_list_to_bed.output intersect_msi.output_msi_sites = stripsuffix(interval_list_to_bed.output, .bed ) + .msisites.tsv self.add(intersect_msi) self.prepare_msings(stripsuffix(file_full_path, .interval_list ), capture_name) self.reference_data['targets'][capture_name]['blacklist-bed'] = None blacklist_bed = stripsuffix(file_full_path, .interval_list ) + .blacklist.bed if os.path.exists(blacklist_bed): blacklist_copy = Copy(input_file=blacklist_bed, output_file= {}/intervals/targets/{} .format( self.outdir, os.path.basename(blacklist_bed), )) self.add(blacklist_copy) self.reference_data['targets'][capture_name]['blacklist-bed'] = blacklist_copy.output purecn_targets_file = stripsuffix(file_full_path, .interval_list ) + .purecn.txt if os.path.exists(purecn_targets_file): copy_purecn_targets = Copy(input_file=purecn_targets_file, output_file= {}/intervals/targets/{} .format(self.outdir, os.path.basename( purecn_targets_file)) ) self.add(copy_purecn_targets) self.reference_data['targets'][capture_name]['purecn_targets'] = copy_purecn_targets.output else: self.reference_data['targets'][capture_name]['purecn_targets'] = None self.reference_data['targets'][capture_name]['targets-interval_list'] = copy_file.output self.reference_data['targets'][capture_name]['targets-interval_list-slopped20'] = slop_interval_list.output self.reference_data['targets'][capture_name]['targets-bed-slopped20'] = interval_list_to_bed.output self.reference_data['targets'][capture_name]['msisites'] = intersect_msi.output_msi_sites # Find all .cnn files and copy + register them for use in cnv kit: for f in [f for f in os.listdir(target_intervals_dir) if (f.endswith( .cnn ) or cnvkit-fix in f)]: self.prepare_cnvkit(f) prepare_genes def prepare_genes(self): curl_ensembl_gtf = Curl() curl_ensembl_gtf.remote = self.ensembl_gtf_remote curl_ensembl_gtf.output = {}/genes/{} .format(self.outdir, os.path.basename(self.ensembl_gtf_remote)) curl_ensembl_gtf.jobname = curl-ensembl-gtf curl_ensembl_gtf.is_intermediate = True self.add(curl_ensembl_gtf) gunzip_ensembl_gtf = Gunzip() gunzip_ensembl_gtf.input = curl_ensembl_gtf.output gunzip_ensembl_gtf.output = stripsuffix(curl_ensembl_gtf.output, .gz ) gunzip_ensembl_gtf.is_intermediate = True self.add(gunzip_ensembl_gtf) filt_ensembl_gtf_chrs = FilterGTFChromosomes() filt_ensembl_gtf_chrs.input = gunzip_ensembl_gtf.output filt_ensembl_gtf_chrs.output = stripsuffix(gunzip_ensembl_gtf.output, .gtf ) + .filtered.gtf self.add(filt_ensembl_gtf_chrs) gtf2genepred_ensembl = GTF2GenePred() gtf2genepred_ensembl.input = filt_ensembl_gtf_chrs.output gtf2genepred_ensembl.output = stripsuffix(filt_ensembl_gtf_chrs.output, .gtf ) + .genepred self.add(gtf2genepred_ensembl) filt_genes_ensembl_gtf_genes = FilterGTFGenes() filt_genes_ensembl_gtf_genes.input = filt_ensembl_gtf_chrs.output filt_genes_ensembl_gtf_genes.output = stripsuffix(filt_ensembl_gtf_chrs.output, .gtf ) + .genes-only.gtf self.add(filt_genes_ensembl_gtf_genes) self.reference_data['ensemblVersion'] = self.ensembl_version self.reference_data['genesGtf'] = filt_ensembl_gtf_chrs.output self.reference_data['genesGenePred'] = gtf2genepred_ensembl.output self.reference_data['genesGtfGenesOnly'] = filt_genes_ensembl_gtf_genes.output prepare_reference_genome def prepare_reference_genome(self): genome_unzipped = stripsuffix(os.path.basename(self.input_reference_sequence), .gz ) gunzip_ref = Gunzip() gunzip_ref.input = self.input_reference_sequence gunzip_ref.output = {}/genome/{} .format(self.outdir, genome_unzipped) self.add(gunzip_ref) copy_ref_to_bwa = Copy(input_file=gunzip_ref.output, output_file= {}/bwa/{} .format(self.outdir, os.path.basename(gunzip_ref.output))) self.add(copy_ref_to_bwa) bwa_index = BwaIndex() bwa_index.input_fasta = copy_ref_to_bwa.output bwa_index.output = copy_ref_to_bwa.output + .bwt bwa_index.algorithm = bwtsw self.add(bwa_index) create_dict = PicardCreateSequenceDictionary() create_dict.input = gunzip_ref.output create_dict.output_dict = gunzip_ref.output.replace( .fasta , ) + .dict self.add(create_dict) samtools_faidx = SamtoolsFaidx() samtools_faidx.input_fasta = gunzip_ref.output samtools_faidx.output = gunzip_ref.output + .fai self.add(samtools_faidx) create_chrsizes = GenerateChrSizes() create_chrsizes.input_fai = samtools_faidx.output create_chrsizes.output = gunzip_ref.output.replace( .fasta , ) + .chrsizes.txt self.add(create_chrsizes) copy_qdnaseq_bg = Copy(input_file=self.qdnaseq_background, output_file= {}/genome/{} .format(self.outdir, os.path.basename(self.qdnaseq_background))) self.add(copy_qdnaseq_bg) self.reference_data['reference_genome'] = gunzip_ref.output self.reference_data['reference_dict'] = create_dict.output_dict self.reference_data['chrsizes'] = create_chrsizes.output self.reference_data['bwaIndex'] = bwa_index.input_fasta self.reference_data['qdnaseq_background'] = copy_qdnaseq_bg.output make_ref_paths_relative Recursively traverse a given dictionary and make paths relative def make_ref_paths_relative(self): def make_paths_relative(d): for k, v in d.items(): if isinstance(v, dict): make_paths_relative(v) else: if v and self.outdir in v: d[k] = os.path.relpath(v, self.outdir) make_paths_relative(self.reference_data) FilterGTFChromosomes class FilterGTFChromosomes(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = filter-gtf-chrs def command(self): return gtf_filter_unused_chrs.py + \\ required( , self.input) + \\ required( , self.output) FilterGTFGenes class FilterGTFGenes(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = filter-gtf-genes def command(self): return awk '{if($3== gene ){print $0}}' + \\ required( , self.input) + \\ | sort -k1,1 -k4,4n + \\ required( , self.output) VcfFilter class VcfFilter(Job): def __init__(self): Job.__init__(self) self.input = None self.filter = None self.output = None self.jobname = vcffilter def command(self): return zcat + \\ required( , self.input) + \\ | vcffilter + \\ required( -f , self.filter) + \\ | bgzip + required( , self.output) + \\ tabix -p vcf {output} .format(output=self.output) CurlSplitAndLeftAlign class CurlSplitAndLeftAlign(Job): def __init__(self): Job.__init__(self) self.remote = None self.input_reference_sequence = None self.input_reference_sequence_fai = None self.output = None self.jobname = curl-split-leftaln def command(self): required( , self.input_reference_sequence_fai) return curl -L + \\ required( , self.remote) + \\ | gzip -d | + vt_split_and_leftaln(self.input_reference_sequence, allow_ref_mismatches=True) + \\ | bgzip + required( , self.output) + \\ tabix -p vcf {output} .format(output=self.output) InstallVep class InstallVep(Job): def __init__(self): Job.__init__(self) self.output_dir = None self.jobname = fetch-vep-cache def command(self): return vep_install.pl --SPECIES homo_sapiens_vep --AUTO c --ASSEMBLY GRCh37 --NO_HTSLIB + \\ required( --CACHEDIR , self.output_dir) + \\ vep_convert_cache.pl + required( --dir , self.output_dir) + \\ --species homo_sapiens --version 83_GRCh37 GTF2GenePred class GTF2GenePred(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = gtf2genepred def command(self): return gtfToGenePred -genePredExt + required( , self.input) + \\ /dev/stdout |awk '{print $12 \\t $0}'|cut -f 1-11 + \\ required( , self.output) BwaIndex class BwaIndex(Job): def __init__(self): Job.__init__(self) self.input_fasta = None self.algorithm = bwtsw self.output = None self.jobname = bwa-index def command(self): return bwa index + \\ required( -a , self.algorithm) + \\ required( , self.input_fasta) SamtoolsFaidx class SamtoolsFaidx(Job): def __init__(self): Job.__init__(self) self.input_fasta = None self.output = None self.jobname = samtools-faidx def command(self): return samtools faidx + \\ required( , self.input_fasta) GenerateChrSizes class GenerateChrSizes(Job): def __init__(self): Job.__init__(self) self.input_fai = None self.output = None self.jobname = make-chrsizes def command(self): return head -n 25 + \\ required( , self.input_fai) + \\ |cut -f 1-2 + \\ required( , self.output) SlopIntervalList class SlopIntervalList(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = slop-interval-list def command(self): return slopIntervalList.py + \\ required( , self.input) + \\ required( , self.output) IntervalListToBed class IntervalListToBed(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = interval-list-to-bed def command(self): return picard_interval_list_to_bed6_converter.py + \\ required( , self.input) + \\ required( , self.output) class Copy(Job): def __init__(self, input_file, output_file): Job.__init__(self) self.input = input_file self.output = output_file self.jobname = copy def command(self): cmd = cp + \\ required( , self.input) + \\ required( , self.output) if self.input.endswith( .bam ): cmd += samtools index {} .format(self.output) return cmd class Gunzip(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = gunzip def command(self): return gzip -cd + \\ required( , self.input) + \\ required( , self.output) class Curl(Job): def __init__(self): Job.__init__(self) self.remote = None self.output = None self.jobname = curl def command(self): return curl + \\ required( , self.remote) + \\ required( , self.output)","title":"Generateref"},{"location":"lld/generateref/#generatereffilespipeline","text":"To generate reference files from class GenerateRefFilesPipeline(PypedreamPipeline): outdir = None maxcores = None def __init__(self, genome_resources, outdir, maxcores=1, runner=Shellrunner()): PypedreamPipeline.__init__(self, normpath(outdir), runner=runner) self.genome_resources = genome_resources self.input_reference_sequence = {}/human_g1k_v37_decoy.fasta.gz .format(genome_resources) self.cosmic_vcf = {}/CosmicCodingMuts_v71.vcf.gz .format(genome_resources) self.qdnaseq_background = {}/qdnaseq_background.Rdata .format(genome_resources) self.swegene_common_vcf = {}/swegen_common.vcf.gz .format(genome_resources) self.outdir = outdir self.maxcores = maxcores self.reference_data = dict() self.exac_remote = ftp://ftp.broadinstitute.org/pub/ExAC_release/release0.3.1/ExAC.r0.3.1.sites.vep.vcf.gz self.dbsnp_remote = ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b149_GRCh37p13/VCF/All_20161121.vcf.gz self.clinvar_remote = ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh37/archive_1.0/2016/clinvar_20160203.vcf.gz self.icgc_somatic_remote = https://dcc.icgc.org/api/v1/download?fn=/release_20/Summary/simple_somatic_mutation.aggregated.vcf.gz self.ensembl_version = 75 self.ensembl_gtf_remote = ftp://ftp.ensembl.org/pub/release- + self.ensembl_version + \\ /gtf/homo_sapiens/Homo_sapiens.GRCh37. + self.ensembl_version + .gtf.gz self.mitranscriptome_remote = http://mitranscriptome.org/download/mitranscriptome.gtf.tar.gz self.prepare_reference_genome() self.prepare_genes() self.prepare_sveffect_regions() self.prepare_intervals() self.prepare_variants() fetch_vep_cache = InstallVep() fetch_vep_cache.output_dir = {}/vep/ .format(self.outdir) self.add(fetch_vep_cache) self.reference_data['vep_dir'] = fetch_vep_cache.output_dir self.make_ref_paths_relative() with open( {}/autoseq-genome.json .format(self.outdir), w ) as output_file: json.dump(self.reference_data, output_file, indent=4, sort_keys=True)","title":"GenerateRefFilesPipeline"},{"location":"lld/generateref/#prepare_sveffect_regions","text":"def prepare_sveffect_regions(self): for regions_name in [ ar_regions , ts_regions , fusion_regions ]: file_full_path = {}/{}.bed .format( self.genome_resources, regions_name, ) copy_regions = Copy(input_file=file_full_path, output_file= {}/intervals/{} .format( self.outdir, os.path.basename(file_full_path), )) self.reference_data[regions_name] = copy_regions.output self.add(copy_regions)","title":"prepare_sveffect_regions"},{"location":"lld/generateref/#prepare_variants","text":"def prepare_variants(self): curl_dbsnp = CurlSplitAndLeftAlign() curl_dbsnp.input_reference_sequence = self.reference_data['reference_genome'] curl_dbsnp.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_dbsnp.remote = self.dbsnp_remote curl_dbsnp.output = {}/variants/{} .format(self.outdir, os.path.basename(self.dbsnp_remote)) curl_dbsnp.is_intermediate = True self.add(curl_dbsnp) filter_dbsnp = VcfFilter() filter_dbsnp.input = curl_dbsnp.output filter_dbsnp.filter = \\ ! ( SAO = 3 | SAO = 2 )\\ filter_dbsnp.output = {}/variants/dbsnp142-germline-only.vcf.gz .format(self.outdir) self.add(filter_dbsnp) curl_cosmic = CurlSplitAndLeftAlign() curl_cosmic.input_reference_sequence = self.reference_data['reference_genome'] curl_cosmic.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_cosmic.remote = file:// + self.cosmic_vcf curl_cosmic.output = {}/variants/{} .format(self.outdir, os.path.basename(self.cosmic_vcf)) self.add(curl_cosmic) curl_clinvar = CurlSplitAndLeftAlign() curl_clinvar.input_reference_sequence = self.reference_data['reference_genome'] curl_clinvar.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_clinvar.remote = self.clinvar_remote curl_clinvar.output = {}/variants/{} .format(self.outdir, os.path.basename(self.clinvar_remote)) self.add(curl_clinvar) curl_exac = CurlSplitAndLeftAlign() curl_exac.input_reference_sequence = self.reference_data['reference_genome'] curl_exac.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_exac.remote = self.exac_remote curl_exac.output = {}/variants/{} .format(self.outdir, os.path.basename(self.exac_remote)) self.add(curl_exac) curl_icgc = CurlSplitAndLeftAlign() curl_icgc.input_reference_sequence = self.reference_data['reference_genome'] curl_icgc.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_icgc.remote = self.icgc_somatic_remote curl_icgc.output = {}/variants/{} .format(self.outdir, icgc_release_20_simple_somatic_mutation.aggregated.vcf.gz ) self.add(curl_icgc) curl_swegene = CurlSplitAndLeftAlign() curl_swegene.input_reference_sequence = self.reference_data['reference_genome'] curl_swegene.input_reference_sequence_fai = self.reference_data['reference_genome'] + .fai curl_swegene.remote = file:// + self.swegene_common_vcf curl_swegene.output = {}/variants/{} .format(self.outdir, os.path.basename(self.swegene_common_vcf)) self.add(curl_swegene) self.reference_data['dbSNP'] = filter_dbsnp.output self.reference_data['cosmic'] = curl_cosmic.output self.reference_data['exac'] = curl_exac.output self.reference_data['clinvar'] = curl_clinvar.output self.reference_data['icgc'] = curl_icgc.output self.reference_data['swegene_common'] = curl_swegene.output","title":"prepare_variants"},{"location":"lld/generateref/#prepare_cnvkit","text":"param: cnv_kit_ref_filename: String name of a cnvkit reference file (either .cnn or .cnvkitref.txt ), to be registered in self.ref_data. def prepare_cnvkit(self, cnv_kit_ref_filename): file_full_path = {}/target_intervals/{} .format(self.genome_resources, cnv_kit_ref_filename) # Extract the capture+library+sampletype strings: capture_library_sampletype = cnv_kit_ref_filename.split( . )[:3] copy_cnvkit_ref = Copy(input_file=file_full_path, output_file= {}/intervals/targets/{} .format(self.outdir, os.path.basename(file_full_path)) ) self.add(copy_cnvkit_ref) capture_name = capture_library_sampletype[0] library_kit_name = capture_library_sampletype[1] sample_type = capture_library_sampletype[2] ref_type = cnvkit-fix if cnv_kit_ref_filename.endswith(( cnn )): ref_type = cnvkit-ref # FIXME: Ugly; refactor. This registers the cnvkit reference file copy in the autoseq genome dictionary: if ref_type not in self.reference_data['targets'][capture_name]: self.reference_data['targets'][capture_name][ref_type] = {} if library_kit_name not in self.reference_data['targets'][capture_name][ref_type]: self.reference_data['targets'][capture_name][ref_type][library_kit_name] = {} self.reference_data['targets'][capture_name][ref_type][library_kit_name][sample_type] = \\ copy_cnvkit_ref.output","title":"prepare_cnvkit"},{"location":"lld/generateref/#prepare_msings","text":"def prepare_msings(self, filename_base, capture_name): for msings_extn in [ baseline , bed , msi_intervals ]: msings_ref_file = filename_base + .msings. + msings_extn if os.path.exists(msings_ref_file): copy_msings_ref = Copy(input_file=msings_ref_file, output_file= {}/intervals/targets/{} .format(self.outdir, os.path.basename( msings_ref_file)) ) self.add(copy_msings_ref) self.reference_data['targets'][capture_name]['msings-' + msings_extn] = copy_msings_ref.output else: self.reference_data['targets'][capture_name]['msings-' + msings_extn] = None # FIXME: The prepare_intervals() method is becoming very unwieldy. Consider refactoring.","title":"prepare_msings"},{"location":"lld/generateref/#prepare_intervals","text":"def prepare_intervals(self): self.reference_data['targets'] = {} target_intervals_dir = {}/target_intervals/ .format(self.genome_resources) input_files = [f for f in os.listdir(target_intervals_dir) if f.endswith( .interval_list )] scan_for_microsatellites = MsiSensorScan() scan_for_microsatellites.input_fasta = self.reference_data['reference_genome'] scan_for_microsatellites.homopolymers_only = True scan_for_microsatellites.output = {}/intervals/msisensor-microsatellites.tsv .format(self.outdir) self.add(scan_for_microsatellites) for f in input_files: file_full_path = {}/target_intervals/{} .format(self.genome_resources, f) logging.debug( Parsing intervals file {} .format(file_full_path)) capture_name = stripsuffix(f, .interval_list ) self.reference_data['targets'][capture_name] = {} copy_file = Copy(input_file=file_full_path, output_file= {}/intervals/targets/{} .format(self.outdir, os.path.basename(file_full_path))) self.add(copy_file) slop_interval_list = SlopIntervalList() slop_interval_list.input = copy_file.output slop_interval_list.output = stripsuffix(copy_file.output, .interval_list ) + .slopped20.interval_list self.add(slop_interval_list) interval_list_to_bed = IntervalListToBed() interval_list_to_bed.input = slop_interval_list.output interval_list_to_bed.output = stripsuffix(slop_interval_list.output, .interval_list ) + .bed self.add(interval_list_to_bed) intersect_msi = IntersectMsiSites() intersect_msi.input_msi_sites = scan_for_microsatellites.output intersect_msi.target_bed = interval_list_to_bed.output intersect_msi.output_msi_sites = stripsuffix(interval_list_to_bed.output, .bed ) + .msisites.tsv self.add(intersect_msi) self.prepare_msings(stripsuffix(file_full_path, .interval_list ), capture_name) self.reference_data['targets'][capture_name]['blacklist-bed'] = None blacklist_bed = stripsuffix(file_full_path, .interval_list ) + .blacklist.bed if os.path.exists(blacklist_bed): blacklist_copy = Copy(input_file=blacklist_bed, output_file= {}/intervals/targets/{} .format( self.outdir, os.path.basename(blacklist_bed), )) self.add(blacklist_copy) self.reference_data['targets'][capture_name]['blacklist-bed'] = blacklist_copy.output purecn_targets_file = stripsuffix(file_full_path, .interval_list ) + .purecn.txt if os.path.exists(purecn_targets_file): copy_purecn_targets = Copy(input_file=purecn_targets_file, output_file= {}/intervals/targets/{} .format(self.outdir, os.path.basename( purecn_targets_file)) ) self.add(copy_purecn_targets) self.reference_data['targets'][capture_name]['purecn_targets'] = copy_purecn_targets.output else: self.reference_data['targets'][capture_name]['purecn_targets'] = None self.reference_data['targets'][capture_name]['targets-interval_list'] = copy_file.output self.reference_data['targets'][capture_name]['targets-interval_list-slopped20'] = slop_interval_list.output self.reference_data['targets'][capture_name]['targets-bed-slopped20'] = interval_list_to_bed.output self.reference_data['targets'][capture_name]['msisites'] = intersect_msi.output_msi_sites # Find all .cnn files and copy + register them for use in cnv kit: for f in [f for f in os.listdir(target_intervals_dir) if (f.endswith( .cnn ) or cnvkit-fix in f)]: self.prepare_cnvkit(f)","title":"prepare_intervals"},{"location":"lld/generateref/#prepare_genes","text":"def prepare_genes(self): curl_ensembl_gtf = Curl() curl_ensembl_gtf.remote = self.ensembl_gtf_remote curl_ensembl_gtf.output = {}/genes/{} .format(self.outdir, os.path.basename(self.ensembl_gtf_remote)) curl_ensembl_gtf.jobname = curl-ensembl-gtf curl_ensembl_gtf.is_intermediate = True self.add(curl_ensembl_gtf) gunzip_ensembl_gtf = Gunzip() gunzip_ensembl_gtf.input = curl_ensembl_gtf.output gunzip_ensembl_gtf.output = stripsuffix(curl_ensembl_gtf.output, .gz ) gunzip_ensembl_gtf.is_intermediate = True self.add(gunzip_ensembl_gtf) filt_ensembl_gtf_chrs = FilterGTFChromosomes() filt_ensembl_gtf_chrs.input = gunzip_ensembl_gtf.output filt_ensembl_gtf_chrs.output = stripsuffix(gunzip_ensembl_gtf.output, .gtf ) + .filtered.gtf self.add(filt_ensembl_gtf_chrs) gtf2genepred_ensembl = GTF2GenePred() gtf2genepred_ensembl.input = filt_ensembl_gtf_chrs.output gtf2genepred_ensembl.output = stripsuffix(filt_ensembl_gtf_chrs.output, .gtf ) + .genepred self.add(gtf2genepred_ensembl) filt_genes_ensembl_gtf_genes = FilterGTFGenes() filt_genes_ensembl_gtf_genes.input = filt_ensembl_gtf_chrs.output filt_genes_ensembl_gtf_genes.output = stripsuffix(filt_ensembl_gtf_chrs.output, .gtf ) + .genes-only.gtf self.add(filt_genes_ensembl_gtf_genes) self.reference_data['ensemblVersion'] = self.ensembl_version self.reference_data['genesGtf'] = filt_ensembl_gtf_chrs.output self.reference_data['genesGenePred'] = gtf2genepred_ensembl.output self.reference_data['genesGtfGenesOnly'] = filt_genes_ensembl_gtf_genes.output","title":"prepare_genes"},{"location":"lld/generateref/#prepare_reference_genome","text":"def prepare_reference_genome(self): genome_unzipped = stripsuffix(os.path.basename(self.input_reference_sequence), .gz ) gunzip_ref = Gunzip() gunzip_ref.input = self.input_reference_sequence gunzip_ref.output = {}/genome/{} .format(self.outdir, genome_unzipped) self.add(gunzip_ref) copy_ref_to_bwa = Copy(input_file=gunzip_ref.output, output_file= {}/bwa/{} .format(self.outdir, os.path.basename(gunzip_ref.output))) self.add(copy_ref_to_bwa) bwa_index = BwaIndex() bwa_index.input_fasta = copy_ref_to_bwa.output bwa_index.output = copy_ref_to_bwa.output + .bwt bwa_index.algorithm = bwtsw self.add(bwa_index) create_dict = PicardCreateSequenceDictionary() create_dict.input = gunzip_ref.output create_dict.output_dict = gunzip_ref.output.replace( .fasta , ) + .dict self.add(create_dict) samtools_faidx = SamtoolsFaidx() samtools_faidx.input_fasta = gunzip_ref.output samtools_faidx.output = gunzip_ref.output + .fai self.add(samtools_faidx) create_chrsizes = GenerateChrSizes() create_chrsizes.input_fai = samtools_faidx.output create_chrsizes.output = gunzip_ref.output.replace( .fasta , ) + .chrsizes.txt self.add(create_chrsizes) copy_qdnaseq_bg = Copy(input_file=self.qdnaseq_background, output_file= {}/genome/{} .format(self.outdir, os.path.basename(self.qdnaseq_background))) self.add(copy_qdnaseq_bg) self.reference_data['reference_genome'] = gunzip_ref.output self.reference_data['reference_dict'] = create_dict.output_dict self.reference_data['chrsizes'] = create_chrsizes.output self.reference_data['bwaIndex'] = bwa_index.input_fasta self.reference_data['qdnaseq_background'] = copy_qdnaseq_bg.output","title":"prepare_reference_genome"},{"location":"lld/generateref/#make_ref_paths_relative","text":"Recursively traverse a given dictionary and make paths relative def make_ref_paths_relative(self): def make_paths_relative(d): for k, v in d.items(): if isinstance(v, dict): make_paths_relative(v) else: if v and self.outdir in v: d[k] = os.path.relpath(v, self.outdir) make_paths_relative(self.reference_data)","title":"make_ref_paths_relative"},{"location":"lld/generateref/#filtergtfchromosomes","text":"class FilterGTFChromosomes(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = filter-gtf-chrs def command(self): return gtf_filter_unused_chrs.py + \\ required( , self.input) + \\ required( , self.output)","title":"FilterGTFChromosomes"},{"location":"lld/generateref/#filtergtfgenes","text":"class FilterGTFGenes(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = filter-gtf-genes def command(self): return awk '{if($3== gene ){print $0}}' + \\ required( , self.input) + \\ | sort -k1,1 -k4,4n + \\ required( , self.output)","title":"FilterGTFGenes"},{"location":"lld/generateref/#vcffilter","text":"class VcfFilter(Job): def __init__(self): Job.__init__(self) self.input = None self.filter = None self.output = None self.jobname = vcffilter def command(self): return zcat + \\ required( , self.input) + \\ | vcffilter + \\ required( -f , self.filter) + \\ | bgzip + required( , self.output) + \\ tabix -p vcf {output} .format(output=self.output)","title":"VcfFilter"},{"location":"lld/generateref/#curlsplitandleftalign","text":"class CurlSplitAndLeftAlign(Job): def __init__(self): Job.__init__(self) self.remote = None self.input_reference_sequence = None self.input_reference_sequence_fai = None self.output = None self.jobname = curl-split-leftaln def command(self): required( , self.input_reference_sequence_fai) return curl -L + \\ required( , self.remote) + \\ | gzip -d | + vt_split_and_leftaln(self.input_reference_sequence, allow_ref_mismatches=True) + \\ | bgzip + required( , self.output) + \\ tabix -p vcf {output} .format(output=self.output)","title":"CurlSplitAndLeftAlign"},{"location":"lld/generateref/#installvep","text":"class InstallVep(Job): def __init__(self): Job.__init__(self) self.output_dir = None self.jobname = fetch-vep-cache def command(self): return vep_install.pl --SPECIES homo_sapiens_vep --AUTO c --ASSEMBLY GRCh37 --NO_HTSLIB + \\ required( --CACHEDIR , self.output_dir) + \\ vep_convert_cache.pl + required( --dir , self.output_dir) + \\ --species homo_sapiens --version 83_GRCh37","title":"InstallVep"},{"location":"lld/generateref/#gtf2genepred","text":"class GTF2GenePred(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = gtf2genepred def command(self): return gtfToGenePred -genePredExt + required( , self.input) + \\ /dev/stdout |awk '{print $12 \\t $0}'|cut -f 1-11 + \\ required( , self.output)","title":"GTF2GenePred"},{"location":"lld/generateref/#bwaindex","text":"class BwaIndex(Job): def __init__(self): Job.__init__(self) self.input_fasta = None self.algorithm = bwtsw self.output = None self.jobname = bwa-index def command(self): return bwa index + \\ required( -a , self.algorithm) + \\ required( , self.input_fasta)","title":"BwaIndex"},{"location":"lld/generateref/#samtoolsfaidx","text":"class SamtoolsFaidx(Job): def __init__(self): Job.__init__(self) self.input_fasta = None self.output = None self.jobname = samtools-faidx def command(self): return samtools faidx + \\ required( , self.input_fasta)","title":"SamtoolsFaidx"},{"location":"lld/generateref/#generatechrsizes","text":"class GenerateChrSizes(Job): def __init__(self): Job.__init__(self) self.input_fai = None self.output = None self.jobname = make-chrsizes def command(self): return head -n 25 + \\ required( , self.input_fai) + \\ |cut -f 1-2 + \\ required( , self.output)","title":"GenerateChrSizes"},{"location":"lld/generateref/#slopintervallist","text":"class SlopIntervalList(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = slop-interval-list def command(self): return slopIntervalList.py + \\ required( , self.input) + \\ required( , self.output)","title":"SlopIntervalList"},{"location":"lld/generateref/#intervallisttobed","text":"class IntervalListToBed(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = interval-list-to-bed def command(self): return picard_interval_list_to_bed6_converter.py + \\ required( , self.input) + \\ required( , self.output) class Copy(Job): def __init__(self, input_file, output_file): Job.__init__(self) self.input = input_file self.output = output_file self.jobname = copy def command(self): cmd = cp + \\ required( , self.input) + \\ required( , self.output) if self.input.endswith( .bam ): cmd += samtools index {} .format(self.output) return cmd class Gunzip(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.jobname = gunzip def command(self): return gzip -cd + \\ required( , self.input) + \\ required( , self.output) class Curl(Job): def __init__(self): Job.__init__(self) self.remote = None self.output = None self.jobname = curl def command(self): return curl + \\ required( , self.remote) + \\ required( , self.output)","title":"IntervalListToBed"},{"location":"lld/liqbio/","text":"LiqBioPipeline Liqbiopipeline comprises, Set the min alt frac value Remove clinseq barcodes for which data is not available: Configure the umi processes from fastq to bam file: Configure alignment and merging of fastq data for all clinseq barcodes Configure all panel analyses Configure liqbio-specific panel analyses Configure additional msings analysis Configure QC of all panel data Configure fastq QCs Configure the low-pass whole genome analysis Configure low-pass whole genome data QC Configure MultiQC class LiqBioPipeline(ClinseqPipeline): def __init__(self, sampledata, refdata, job_params, outdir, libdir, umi, maxcores=1, scratch= /scratch/tmp/tmp , **kwargs): ClinseqPipeline.__init__(self, sampledata, refdata, job_params, outdir, libdir, umi, maxcores, scratch, **kwargs) # Set the min alt frac value: self.default_job_params[ vardict-min-alt-frac ] = 0.01 self.default_job_params[ vardict-min-num-reads ] = None self.default_job_params[ vep-additional-options ] = --pick --filter_common self.check_sampledata() if umi: self.configure_umi_processing() else: self.configure_align_and_merge() self.configure_panel_analyses() self.configure_panel_analyses_liqbio() self.configure_panel_msings_analyses() self.configure_all_panel_qcs() self.configure_fastq_qcs() self.configure_lowpass_analyses() self.configure_all_lowpass_qcs() self.configure_multi_qc() configure_single_capture_analysis_liqbio Configure svcaller analysis for each event type def configure_single_capture_analysis_liqbio(self, unique_capture): input_bam = self.get_capture_bam(unique_capture) sample_str = compose_lib_capture_str(unique_capture) for event_type in [ DEL , DUP , INV , TRA ]: svcaller = Svcaller() svcaller.input_bam = input_bam svcaller.event_type = event_type svcaller.output_bam = {}/svs/{}-{}.bam .format(self.outdir, sample_str, event_type) svcaller.output_gtf = {}/svs/{}-{}.gtf .format(self.outdir, sample_str, event_type) svcaller.reference_sequence = self.refdata[ reference_genome ] svcaller.scratch = self.scratch self.add(svcaller) self.set_capture_svs(unique_capture, event_type, (svcaller.output_bam, svcaller.output_gtf)) sveffect = Sveffect() sveffect.input_del_gtf = self.capture_to_results[unique_capture].svs[ DEL ][1] sveffect.input_dup_gtf = self.capture_to_results[unique_capture].svs[ DUP ][1] sveffect.input_inv_gtf = self.capture_to_results[unique_capture].svs[ INV ][1] sveffect.input_tra_gtf = self.capture_to_results[unique_capture].svs[ TRA ][1] sveffect.ts_regions = self.refdata[ ts_regions ] sveffect.ar_regions = self.refdata[ ar_regions ] sveffect.fusion_regions = self.refdata[ fusion_regions ] sveffect.output_combined_bed = {}/svs/{}_combined.bed .format(self.outdir, sample_str) sveffect.output_effects_json = {}/svs/{}_effects.json .format(self.outdir, sample_str) self.add(sveffect) self.set_capture_sveffect(unique_capture, sveffect.output_effects_json) configure_panel_analyses_liqbio Configure liqbio analyses to be run on all unique panel captures individually and Configure a liqbio analyses for each normal-cancer pairing. def configure_panel_analyses_liqbio(self): for unique_capture in self.get_mapped_captures_no_wgs(): self.configure_single_capture_analysis_liqbio(unique_capture) self.configure_svict(unique_capture) for normal_capture in self.get_mapped_captures_normal(): for cancer_capture in self.get_mapped_captures_cancer(): self.configure_panel_analysis_cancer_vs_normal_liqbio( normal_capture, cancer_capture) configure_svict configuring SViCT which is a computational tool for detecting structural variations from cell free DNA (cfDNA) containing low dilutions of circulating tumor DNA (ctDNA). def configure_svict(self, unique_capture): input_bam = self.get_capture_bam(unique_capture, umi=False) sample_str = compose_lib_capture_str(unique_capture) svict = SViCT() svict.input_bam = input_bam svict.reference_sequence = self.refdata[ reference_genome ] svict.output = {}/svs/{}-svict .format(self.outdir, sample_str) self.add(svict) configure_sv_calling Configure Structural Variant Calling, to identify structural variants in sample, Varinat callers are Svaba, Lumpy. param normal_capture: A unique normal sample library capture param cancer_capture: A unique cancer sample library capture def configure_sv_calling(self, normal_capture, cancer_capture): cancer_bam = self.get_capture_bam(cancer_capture, umi=False) normal_bam = self.get_capture_bam(normal_capture, umi=False) target_name = self.get_capture_name(cancer_capture.capture_kit_id) cancer_capture_str = compose_lib_capture_str(cancer_capture) normal_capture_str = compose_lib_capture_str(normal_capture) svaba = Svaba() svaba.input_normal = normal_bam svaba.input_tumor = cancer_bam svaba.reference_sequence = self.refdata[ bwaIndex ] svaba.threads = self.maxcores svaba.target_bed = self.refdata['targets'][target_name]['targets-bed-slopped20'] svaba.output_sample = {}/svs/svaba/{}-{}-svaba .format(self.outdir, normal_capture_str, cancer_capture_str) self.add(svaba) lumpy = Lumpy() lumpy.input_normal = normal_bam lumpy.input_tumor = cancer_bam lumpy.normal_discordants = {}/svs/lumpy/{}-discordants.bam .format(self.outdir, normal_capture_str) lumpy.tumor_discordants = {}/svs/lumpy/{}-discordants.bam .format(self.outdir, cancer_capture_str) lumpy.normal_splitters = {}/svs/lumpy/{}-splitters.bam .format(self.outdir, normal_capture_str) lumpy.tumor_splitters = {}/svs/lumpy/{}-splitters.bam .format(self.outdir, cancer_capture_str) lumpy.output = {}/svs/lumpy/{}-{}-lumpy.vcf .format(self.outdir, normal_capture_str, cancer_capture_str) lumpy.threads = self.maxcores self.add(lumpy) configure_liqbio_cna def configure_liqbio_cna(self, normal_capture, cancer_capture): tumor_vs_normal_results = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)] tumor_results = self.capture_to_results[cancer_capture] pureCN_outputs = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].pureCN_outputs normal_str = compose_lib_capture_str(normal_capture) cancer_str = compose_lib_capture_str(cancer_capture) liqbio_cna = LiqbioCNAPlot() liqbio_cna.input_tumor_cnr = self.capture_to_results[cancer_capture].cnr liqbio_cna.input_tumor_cns = self.capture_to_results[cancer_capture].cns liqbio_cna.input_normal_cnr = self.capture_to_results[normal_capture].cnr liqbio_cna.input_normal_cns = self.capture_to_results[normal_capture].cns liqbio_cna.input_het_snps_vcf = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].vcf_addsample_output liqbio_cna.input_purecn_csv = pureCN_outputs[ csv ] liqbio_cna.input_purecn_genes_csv = pureCN_outputs[ genes_csv ] liqbio_cna.input_purecn_loh_csv = pureCN_outputs[ loh_csv ] liqbio_cna.input_purecn_variants_csv = pureCN_outputs[ variants_csv ] liqbio_cna.input_svcaller_T_DEL = self.capture_to_results[cancer_capture].svs[ DEL ][1] liqbio_cna.input_svcaller_T_DUP = self.capture_to_results[cancer_capture].svs[ DUP ][1] liqbio_cna.input_svcaller_T_INV = self.capture_to_results[cancer_capture].svs[ INV ][1] liqbio_cna.input_svcaller_T_TRA = self.capture_to_results[cancer_capture].svs[ TRA ][1] liqbio_cna.input_svcaller_N_DEL = self.capture_to_results[normal_capture].svs[ DEL ][1] liqbio_cna.input_svcaller_N_DUP = self.capture_to_results[normal_capture].svs[ DUP ][1] liqbio_cna.input_svcaller_N_INV = self.capture_to_results[normal_capture].svs[ INV ][1] liqbio_cna.input_svcaller_N_TRA = self.capture_to_results[normal_capture].svs[ TRA ][1] liqbio_cna.input_germline_mut_vcf = self.get_vepped_germline_vcf(normal_capture) # Vepped germline variants liqbio_cna.input_somatic_mut_vcf = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].vepped_vcf liqbio_cna.output_plot_png = {}/qc/{}-{}-liqbio-cna.png .format(self.outdir, normal_str, cancer_str) liqbio_cna.output_cna_json = {}/variants/{}-{}-liqbio-cna.json .format(self.outdir, normal_str, cancer_str) liqbio_cna.output_purity_json = {}/qc/{}-{}-liqbio-purity.json .format(self.outdir, normal_str, cancer_str) self.add(liqbio_cna) configure_panel_analysis_cancer_vs_normal_liqbio .. note:: In-house developed PureCN script is not compatiable with latest version of PureCN. As of now We have commented out this step in latest version. def configure_panel_analysis_cancer_vs_normal_liqbio(self, normal_capture, cancer_capture): capture_name = self.get_capture_name(cancer_capture.capture_kit_id) self.configure_sv_calling(normal_capture, cancer_capture) # self.configure_manta(normal_capture, cancer_capture) # if self.refdata['targets'][capture_name]['purecn_targets']: # self.configure_purecn(normal_capture, cancer_capture) # self.configure_liqbio_cna(normal_capture, cancer_capture) configure_umi_processing Configure the umi process for SNV, small INDEL calling and normal process (markdups after realignment-1 finishes) for structural variant calling def configure_umi_processing(self): capture_to_barcodes = self.get_unique_capture_to_clinseq_barcodes() for unique_capture in capture_to_barcodes.keys(): capture_kit = unique_capture.capture_kit_id for clinseq_barcode in capture_to_barcodes[unique_capture]: trimmed_fqfiles = fq_trimming(self, fq1_files=find_fastqs(clinseq_barcode, self.libdir)[0], fq2_files=find_fastqs(clinseq_barcode, self.libdir)[1], clinseq_barcode=clinseq_barcode, ref=self.refdata['bwaIndex'], outdir= {}/bams/{} .format(self.outdir, capture_kit), maxcores=self.maxcores) bam_file = self.configure_fastq_to_bam(fq_files=trimmed_fqfiles, clinseq_barcode=clinseq_barcode, capture_kit=capture_kit) realigned_bam = self.configure_alignment_with_umi(bamfile=bam_file, clinseq_barcode=clinseq_barcode, capture_kit=capture_kit, jobname='1') consensus_reads = self.configure_consensus_reads_calling(bam=realigned_bam, clinseq_barcode=clinseq_barcode, capture_kit=capture_kit) realigned_bam2 = self.configure_alignment_with_umi(bamfile=consensus_reads, clinseq_barcode=clinseq_barcode, capture_kit=capture_kit, jobname='2') filtered_bam = self.configure_consensus_read_filter(bam=realigned_bam2 , clinseq_barcode=clinseq_barcode, capture_kit=capture_kit) clip_overlap_bam = self.configure_clip_overlapping(bam=filtered_bam, clinseq_barcode=clinseq_barcode, capture_kit=capture_kit) mark_dups_bam = self.configure_markdups(bamfile=realigned_bam, unique_capture=unique_capture) self.set_capture_bam(unique_capture, filtered_bam, self.umi) configure_alignment_with_umi Map the reads with bwa and merge with the UMI tags (picard SamToFastq, bwa mem, picard MergeBamAlignment) GATK - Indel Realignment def configure_alignment_with_umi(self, bamfile, clinseq_barcode, capture_kit, jobname): align_unmap_bam = AlignUnmappedBam() align_unmap_bam.input_bam = bamfile align_unmap_bam.reference_genome = self.refdata['bwaIndex'] align_unmap_bam.output_bam = {}/bams/{}/{}.mapped-{}.bam .format(self.outdir, capture_kit, clinseq_barcode, jobname) align_unmap_bam.jobname = alignment-of-unmapped-bam- + jobname + '-' + clinseq_barcode self.add(align_unmap_bam) realingment = Realignment() realingment.input_bam = align_unmap_bam.output_bam realingment.output_bam = {}/bams/{}/{}.realigned-{}.bam .format(self.outdir, capture_kit, clinseq_barcode, jobname) realingment.reference_genome = self.refdata['reference_genome'] realingment.known_indel1 = self.refdata['1KG'] realingment.known_indel2 = self.refdata['Mills_and_1KG_gold_standard'] realingment.target_intervals = {}/bams/{}/{}.intervals .format(self.outdir, capture_kit, clinseq_barcode) realingment.jobname = realignment- + jobname + '-' + clinseq_barcode self.add(realingment) return realingment.output_bam configure_fastq_to_bam Extract UMIs from trimmed fastq and store in RX tag of unmapped bam (fgbio FastqToBam) def configure_fastq_to_bam(self, fq_files, clinseq_barcode, capture_kit): library = parse_prep_id(clinseq_barcode) sample = compose_sample_str(extract_unique_capture(clinseq_barcode)) fastq_to_bam = FastqToBam() fastq_to_bam.input_fastq1 = fq_files[0] fastq_to_bam.input_fastq2 = fq_files[1] fastq_to_bam.sample = sample fastq_to_bam.library = library fastq_to_bam.output_bam = {}/bams/{}/{}.unmapped.bam .format(self.outdir, capture_kit, clinseq_barcode) fastq_to_bam.jobname = fastq-to-bam + '-' + clinseq_barcode self.add(fastq_to_bam) return fastq_to_bam.output_bam configure_consensus_reads_calling Group the reads together based on UMI tags or sequences Calls duplex consensus sequences from reads generated from the same double-stranded source molecule. def configure_consensus_reads_calling(self, bam, clinseq_barcode, capture_kit): group_reads = GroupReadsByUmi() group_reads.input_bam = bam group_reads.output_histogram = {}/bams/{}/{}.grouped.bam.fs.txt .format(self.outdir, capture_kit, clinseq_barcode) group_reads.output_bam = {}/bams/{}/{}.grouped.bam .format(self.outdir, capture_kit, clinseq_barcode) group_reads.jobname = group-reads-by-umi + '-' + clinseq_barcode self.add(group_reads) call_consensus_reads = CallDuplexConsensusReads() call_consensus_reads.input_bam = group_reads.output_bam call_consensus_reads.output_bam = {}/bams/{}/{}.consensus.bam .format(self.outdir, capture_kit, clinseq_barcode) call_consensus_reads.jobname = call-duplex-consensus-reads + '-' + clinseq_barcode self.add(call_consensus_reads) return call_consensus_reads.output_bam configure_consensus_read_filter def configure_consensus_read_filter(self, bam, clinseq_barcode, capture_kit): filter_con_reads = FilterConsensusReads() filter_con_reads.input_bam = bam filter_con_reads.reference_genome = self.refdata['reference_genome'] filter_con_reads.output_bam = {}/bams/{}/{}.consensus.filtered.bam .format(self.outdir, capture_kit, clinseq_barcode) filter_con_reads.jobname = filter-consensus-reads-{} .format(clinseq_barcode) self.add(filter_con_reads) return filter_con_reads.output_bam configure_clip_overlapping def configure_clip_overlapping(self, bam, clinseq_barcode, capture_kit): clip_overlap_reads = ClipBam() clip_overlap_reads.input_bam = bam clip_overlap_reads.reference_genome = self.refdata['reference_genome'] clip_overlap_reads.output_bam = {}/bams/{}/{}.clip.overlapped.bam .format(self.outdir, capture_kit, clinseq_barcode) clip_overlap_reads.metrics_txt = {}/qc/{}-clip_overlap_metrix.txt .format(self.outdir, clinseq_barcode) self.add(clip_overlap_reads) return clip_overlap_reads.output_bam","title":"Liqbio"},{"location":"lld/liqbio/#liqbiopipeline","text":"Liqbiopipeline comprises, Set the min alt frac value Remove clinseq barcodes for which data is not available: Configure the umi processes from fastq to bam file: Configure alignment and merging of fastq data for all clinseq barcodes Configure all panel analyses Configure liqbio-specific panel analyses Configure additional msings analysis Configure QC of all panel data Configure fastq QCs Configure the low-pass whole genome analysis Configure low-pass whole genome data QC Configure MultiQC class LiqBioPipeline(ClinseqPipeline): def __init__(self, sampledata, refdata, job_params, outdir, libdir, umi, maxcores=1, scratch= /scratch/tmp/tmp , **kwargs): ClinseqPipeline.__init__(self, sampledata, refdata, job_params, outdir, libdir, umi, maxcores, scratch, **kwargs) # Set the min alt frac value: self.default_job_params[ vardict-min-alt-frac ] = 0.01 self.default_job_params[ vardict-min-num-reads ] = None self.default_job_params[ vep-additional-options ] = --pick --filter_common self.check_sampledata() if umi: self.configure_umi_processing() else: self.configure_align_and_merge() self.configure_panel_analyses() self.configure_panel_analyses_liqbio() self.configure_panel_msings_analyses() self.configure_all_panel_qcs() self.configure_fastq_qcs() self.configure_lowpass_analyses() self.configure_all_lowpass_qcs() self.configure_multi_qc()","title":"LiqBioPipeline"},{"location":"lld/liqbio/#configure_single_capture_analysis_liqbio","text":"Configure svcaller analysis for each event type def configure_single_capture_analysis_liqbio(self, unique_capture): input_bam = self.get_capture_bam(unique_capture) sample_str = compose_lib_capture_str(unique_capture) for event_type in [ DEL , DUP , INV , TRA ]: svcaller = Svcaller() svcaller.input_bam = input_bam svcaller.event_type = event_type svcaller.output_bam = {}/svs/{}-{}.bam .format(self.outdir, sample_str, event_type) svcaller.output_gtf = {}/svs/{}-{}.gtf .format(self.outdir, sample_str, event_type) svcaller.reference_sequence = self.refdata[ reference_genome ] svcaller.scratch = self.scratch self.add(svcaller) self.set_capture_svs(unique_capture, event_type, (svcaller.output_bam, svcaller.output_gtf)) sveffect = Sveffect() sveffect.input_del_gtf = self.capture_to_results[unique_capture].svs[ DEL ][1] sveffect.input_dup_gtf = self.capture_to_results[unique_capture].svs[ DUP ][1] sveffect.input_inv_gtf = self.capture_to_results[unique_capture].svs[ INV ][1] sveffect.input_tra_gtf = self.capture_to_results[unique_capture].svs[ TRA ][1] sveffect.ts_regions = self.refdata[ ts_regions ] sveffect.ar_regions = self.refdata[ ar_regions ] sveffect.fusion_regions = self.refdata[ fusion_regions ] sveffect.output_combined_bed = {}/svs/{}_combined.bed .format(self.outdir, sample_str) sveffect.output_effects_json = {}/svs/{}_effects.json .format(self.outdir, sample_str) self.add(sveffect) self.set_capture_sveffect(unique_capture, sveffect.output_effects_json)","title":"configure_single_capture_analysis_liqbio"},{"location":"lld/liqbio/#configure_panel_analyses_liqbio","text":"Configure liqbio analyses to be run on all unique panel captures individually and Configure a liqbio analyses for each normal-cancer pairing. def configure_panel_analyses_liqbio(self): for unique_capture in self.get_mapped_captures_no_wgs(): self.configure_single_capture_analysis_liqbio(unique_capture) self.configure_svict(unique_capture) for normal_capture in self.get_mapped_captures_normal(): for cancer_capture in self.get_mapped_captures_cancer(): self.configure_panel_analysis_cancer_vs_normal_liqbio( normal_capture, cancer_capture)","title":"configure_panel_analyses_liqbio"},{"location":"lld/liqbio/#configure_svict","text":"configuring SViCT which is a computational tool for detecting structural variations from cell free DNA (cfDNA) containing low dilutions of circulating tumor DNA (ctDNA). def configure_svict(self, unique_capture): input_bam = self.get_capture_bam(unique_capture, umi=False) sample_str = compose_lib_capture_str(unique_capture) svict = SViCT() svict.input_bam = input_bam svict.reference_sequence = self.refdata[ reference_genome ] svict.output = {}/svs/{}-svict .format(self.outdir, sample_str) self.add(svict)","title":"configure_svict"},{"location":"lld/liqbio/#configure_sv_calling","text":"Configure Structural Variant Calling, to identify structural variants in sample, Varinat callers are Svaba, Lumpy. param normal_capture: A unique normal sample library capture param cancer_capture: A unique cancer sample library capture def configure_sv_calling(self, normal_capture, cancer_capture): cancer_bam = self.get_capture_bam(cancer_capture, umi=False) normal_bam = self.get_capture_bam(normal_capture, umi=False) target_name = self.get_capture_name(cancer_capture.capture_kit_id) cancer_capture_str = compose_lib_capture_str(cancer_capture) normal_capture_str = compose_lib_capture_str(normal_capture) svaba = Svaba() svaba.input_normal = normal_bam svaba.input_tumor = cancer_bam svaba.reference_sequence = self.refdata[ bwaIndex ] svaba.threads = self.maxcores svaba.target_bed = self.refdata['targets'][target_name]['targets-bed-slopped20'] svaba.output_sample = {}/svs/svaba/{}-{}-svaba .format(self.outdir, normal_capture_str, cancer_capture_str) self.add(svaba) lumpy = Lumpy() lumpy.input_normal = normal_bam lumpy.input_tumor = cancer_bam lumpy.normal_discordants = {}/svs/lumpy/{}-discordants.bam .format(self.outdir, normal_capture_str) lumpy.tumor_discordants = {}/svs/lumpy/{}-discordants.bam .format(self.outdir, cancer_capture_str) lumpy.normal_splitters = {}/svs/lumpy/{}-splitters.bam .format(self.outdir, normal_capture_str) lumpy.tumor_splitters = {}/svs/lumpy/{}-splitters.bam .format(self.outdir, cancer_capture_str) lumpy.output = {}/svs/lumpy/{}-{}-lumpy.vcf .format(self.outdir, normal_capture_str, cancer_capture_str) lumpy.threads = self.maxcores self.add(lumpy)","title":"configure_sv_calling"},{"location":"lld/liqbio/#configure_liqbio_cna","text":"def configure_liqbio_cna(self, normal_capture, cancer_capture): tumor_vs_normal_results = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)] tumor_results = self.capture_to_results[cancer_capture] pureCN_outputs = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].pureCN_outputs normal_str = compose_lib_capture_str(normal_capture) cancer_str = compose_lib_capture_str(cancer_capture) liqbio_cna = LiqbioCNAPlot() liqbio_cna.input_tumor_cnr = self.capture_to_results[cancer_capture].cnr liqbio_cna.input_tumor_cns = self.capture_to_results[cancer_capture].cns liqbio_cna.input_normal_cnr = self.capture_to_results[normal_capture].cnr liqbio_cna.input_normal_cns = self.capture_to_results[normal_capture].cns liqbio_cna.input_het_snps_vcf = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].vcf_addsample_output liqbio_cna.input_purecn_csv = pureCN_outputs[ csv ] liqbio_cna.input_purecn_genes_csv = pureCN_outputs[ genes_csv ] liqbio_cna.input_purecn_loh_csv = pureCN_outputs[ loh_csv ] liqbio_cna.input_purecn_variants_csv = pureCN_outputs[ variants_csv ] liqbio_cna.input_svcaller_T_DEL = self.capture_to_results[cancer_capture].svs[ DEL ][1] liqbio_cna.input_svcaller_T_DUP = self.capture_to_results[cancer_capture].svs[ DUP ][1] liqbio_cna.input_svcaller_T_INV = self.capture_to_results[cancer_capture].svs[ INV ][1] liqbio_cna.input_svcaller_T_TRA = self.capture_to_results[cancer_capture].svs[ TRA ][1] liqbio_cna.input_svcaller_N_DEL = self.capture_to_results[normal_capture].svs[ DEL ][1] liqbio_cna.input_svcaller_N_DUP = self.capture_to_results[normal_capture].svs[ DUP ][1] liqbio_cna.input_svcaller_N_INV = self.capture_to_results[normal_capture].svs[ INV ][1] liqbio_cna.input_svcaller_N_TRA = self.capture_to_results[normal_capture].svs[ TRA ][1] liqbio_cna.input_germline_mut_vcf = self.get_vepped_germline_vcf(normal_capture) # Vepped germline variants liqbio_cna.input_somatic_mut_vcf = self.normal_cancer_pair_to_results[(normal_capture, cancer_capture)].vepped_vcf liqbio_cna.output_plot_png = {}/qc/{}-{}-liqbio-cna.png .format(self.outdir, normal_str, cancer_str) liqbio_cna.output_cna_json = {}/variants/{}-{}-liqbio-cna.json .format(self.outdir, normal_str, cancer_str) liqbio_cna.output_purity_json = {}/qc/{}-{}-liqbio-purity.json .format(self.outdir, normal_str, cancer_str) self.add(liqbio_cna)","title":"configure_liqbio_cna"},{"location":"lld/liqbio/#configure_panel_analysis_cancer_vs_normal_liqbio","text":".. note:: In-house developed PureCN script is not compatiable with latest version of PureCN. As of now We have commented out this step in latest version. def configure_panel_analysis_cancer_vs_normal_liqbio(self, normal_capture, cancer_capture): capture_name = self.get_capture_name(cancer_capture.capture_kit_id) self.configure_sv_calling(normal_capture, cancer_capture) # self.configure_manta(normal_capture, cancer_capture) # if self.refdata['targets'][capture_name]['purecn_targets']: # self.configure_purecn(normal_capture, cancer_capture) # self.configure_liqbio_cna(normal_capture, cancer_capture)","title":"configure_panel_analysis_cancer_vs_normal_liqbio"},{"location":"lld/liqbio/#configure_umi_processing","text":"Configure the umi process for SNV, small INDEL calling and normal process (markdups after realignment-1 finishes) for structural variant calling def configure_umi_processing(self): capture_to_barcodes = self.get_unique_capture_to_clinseq_barcodes() for unique_capture in capture_to_barcodes.keys(): capture_kit = unique_capture.capture_kit_id for clinseq_barcode in capture_to_barcodes[unique_capture]: trimmed_fqfiles = fq_trimming(self, fq1_files=find_fastqs(clinseq_barcode, self.libdir)[0], fq2_files=find_fastqs(clinseq_barcode, self.libdir)[1], clinseq_barcode=clinseq_barcode, ref=self.refdata['bwaIndex'], outdir= {}/bams/{} .format(self.outdir, capture_kit), maxcores=self.maxcores) bam_file = self.configure_fastq_to_bam(fq_files=trimmed_fqfiles, clinseq_barcode=clinseq_barcode, capture_kit=capture_kit) realigned_bam = self.configure_alignment_with_umi(bamfile=bam_file, clinseq_barcode=clinseq_barcode, capture_kit=capture_kit, jobname='1') consensus_reads = self.configure_consensus_reads_calling(bam=realigned_bam, clinseq_barcode=clinseq_barcode, capture_kit=capture_kit) realigned_bam2 = self.configure_alignment_with_umi(bamfile=consensus_reads, clinseq_barcode=clinseq_barcode, capture_kit=capture_kit, jobname='2') filtered_bam = self.configure_consensus_read_filter(bam=realigned_bam2 , clinseq_barcode=clinseq_barcode, capture_kit=capture_kit) clip_overlap_bam = self.configure_clip_overlapping(bam=filtered_bam, clinseq_barcode=clinseq_barcode, capture_kit=capture_kit) mark_dups_bam = self.configure_markdups(bamfile=realigned_bam, unique_capture=unique_capture) self.set_capture_bam(unique_capture, filtered_bam, self.umi)","title":"configure_umi_processing"},{"location":"lld/liqbio/#configure_alignment_with_umi","text":"Map the reads with bwa and merge with the UMI tags (picard SamToFastq, bwa mem, picard MergeBamAlignment) GATK - Indel Realignment def configure_alignment_with_umi(self, bamfile, clinseq_barcode, capture_kit, jobname): align_unmap_bam = AlignUnmappedBam() align_unmap_bam.input_bam = bamfile align_unmap_bam.reference_genome = self.refdata['bwaIndex'] align_unmap_bam.output_bam = {}/bams/{}/{}.mapped-{}.bam .format(self.outdir, capture_kit, clinseq_barcode, jobname) align_unmap_bam.jobname = alignment-of-unmapped-bam- + jobname + '-' + clinseq_barcode self.add(align_unmap_bam) realingment = Realignment() realingment.input_bam = align_unmap_bam.output_bam realingment.output_bam = {}/bams/{}/{}.realigned-{}.bam .format(self.outdir, capture_kit, clinseq_barcode, jobname) realingment.reference_genome = self.refdata['reference_genome'] realingment.known_indel1 = self.refdata['1KG'] realingment.known_indel2 = self.refdata['Mills_and_1KG_gold_standard'] realingment.target_intervals = {}/bams/{}/{}.intervals .format(self.outdir, capture_kit, clinseq_barcode) realingment.jobname = realignment- + jobname + '-' + clinseq_barcode self.add(realingment) return realingment.output_bam","title":"configure_alignment_with_umi"},{"location":"lld/liqbio/#configure_fastq_to_bam","text":"Extract UMIs from trimmed fastq and store in RX tag of unmapped bam (fgbio FastqToBam) def configure_fastq_to_bam(self, fq_files, clinseq_barcode, capture_kit): library = parse_prep_id(clinseq_barcode) sample = compose_sample_str(extract_unique_capture(clinseq_barcode)) fastq_to_bam = FastqToBam() fastq_to_bam.input_fastq1 = fq_files[0] fastq_to_bam.input_fastq2 = fq_files[1] fastq_to_bam.sample = sample fastq_to_bam.library = library fastq_to_bam.output_bam = {}/bams/{}/{}.unmapped.bam .format(self.outdir, capture_kit, clinseq_barcode) fastq_to_bam.jobname = fastq-to-bam + '-' + clinseq_barcode self.add(fastq_to_bam) return fastq_to_bam.output_bam","title":"configure_fastq_to_bam"},{"location":"lld/liqbio/#configure_consensus_reads_calling","text":"Group the reads together based on UMI tags or sequences Calls duplex consensus sequences from reads generated from the same double-stranded source molecule. def configure_consensus_reads_calling(self, bam, clinseq_barcode, capture_kit): group_reads = GroupReadsByUmi() group_reads.input_bam = bam group_reads.output_histogram = {}/bams/{}/{}.grouped.bam.fs.txt .format(self.outdir, capture_kit, clinseq_barcode) group_reads.output_bam = {}/bams/{}/{}.grouped.bam .format(self.outdir, capture_kit, clinseq_barcode) group_reads.jobname = group-reads-by-umi + '-' + clinseq_barcode self.add(group_reads) call_consensus_reads = CallDuplexConsensusReads() call_consensus_reads.input_bam = group_reads.output_bam call_consensus_reads.output_bam = {}/bams/{}/{}.consensus.bam .format(self.outdir, capture_kit, clinseq_barcode) call_consensus_reads.jobname = call-duplex-consensus-reads + '-' + clinseq_barcode self.add(call_consensus_reads) return call_consensus_reads.output_bam","title":"configure_consensus_reads_calling"},{"location":"lld/liqbio/#configure_consensus_read_filter","text":"def configure_consensus_read_filter(self, bam, clinseq_barcode, capture_kit): filter_con_reads = FilterConsensusReads() filter_con_reads.input_bam = bam filter_con_reads.reference_genome = self.refdata['reference_genome'] filter_con_reads.output_bam = {}/bams/{}/{}.consensus.filtered.bam .format(self.outdir, capture_kit, clinseq_barcode) filter_con_reads.jobname = filter-consensus-reads-{} .format(clinseq_barcode) self.add(filter_con_reads) return filter_con_reads.output_bam","title":"configure_consensus_read_filter"},{"location":"lld/liqbio/#configure_clip_overlapping","text":"def configure_clip_overlapping(self, bam, clinseq_barcode, capture_kit): clip_overlap_reads = ClipBam() clip_overlap_reads.input_bam = bam clip_overlap_reads.reference_genome = self.refdata['reference_genome'] clip_overlap_reads.output_bam = {}/bams/{}/{}.clip.overlapped.bam .format(self.outdir, capture_kit, clinseq_barcode) clip_overlap_reads.metrics_txt = {}/qc/{}-clip_overlap_metrix.txt .format(self.outdir, clinseq_barcode) self.add(clip_overlap_reads) return clip_overlap_reads.output_bam","title":"configure_clip_overlapping"},{"location":"lld/picard/","text":"PicardCollectInsertSizeMetrics class PicardCollectInsertSizeMetrics(Job): def __init__(self): Job.__init__(self) self.input = None self.output_metrics = None self.jobname = picard-isize def command(self): return picard -XX:ParallelGCThreads=1 CollectInsertSizeMetrics H=/dev/null + \\ required( I= , self.input) + \\ required( O= , self.output_metrics) PicardCollectGcBiasMetrics class PicardCollectGcBiasMetrics(Job): def __init__(self): Job.__init__(self) self.input = None self.reference_sequence = None self.output_metrics = None self.output_summary = None self.stop_after = None self.jobname = picard-gcbias def command(self): return picard -XX:ParallelGCThreads=1 -Xmx5g CollectGcBiasMetrics CHART=/dev/null + \\ required( I= , self.input) + \\ required( O= , self.output_metrics) + \\ required( S= , self.output_summary) + \\ required( R= , self.reference_sequence) + \\ optional( STOP_AFTER= , self.stop_after) PicardCollectOxoGMetrics class PicardCollectOxoGMetrics(Job): def __init__(self): Job.__init__(self) self.input = None self.reference_sequence = None self.output_metrics = None self.jobname = picard-oxog def command(self): return picard -XX:ParallelGCThreads=1 -Xmx2g CollectOxoGMetrics + \\ required( I= , self.input) + \\ required( R= , self.reference_sequence) + \\ required( O= , self.output_metrics) PicardCollectHsMetrics class PicardCollectHsMetrics(Job): def __init__(self): Job.__init__(self) self.input = None self.reference_sequence = None self.target_regions = None self.bait_regions = None self.bait_name = None self.output_metrics = None self.accumulation_level = ['LIBRARY'] self.jobname = picard-hsmetrics def command(self): return picard -XX:ParallelGCThreads=1 CollectHsMetrics + \\ required( I= , self.input) + \\ required( R= , self.reference_sequence) + \\ required( O= , self.output_metrics) + \\ required( TI= , self.target_regions) + \\ required( BI= , self.bait_regions) + \\ optional( BAIT_SET_NAME= , self.bait_name) + \\ repeat('METRIC_ACCUMULATION_LEVEL=', self.accumulation_level) PicardCollectWgsMetrics class PicardCollectWgsMetrics(Job): def __init__(self): Job.__init__(self) self.input = None self.reference_sequence = None self.minimum_mapping_quality = None self.minimum_base_quality = None self.coverage_cap = None self.output_metrics = None self.jobname = picard-wgsmetrics def command(self): return picard -XX:ParallelGCThreads=1 CollectWgsMetrics + \\ required( I= , self.input) + \\ required( R= , self.reference_sequence) + \\ required( O= , self.output_metrics) + \\ optional( MINIMUM_MAPPING_QUALITY= , self.minimum_mapping_quality) + \\ optional( MINIMUM_BASE_QUALITY= , self.minimum_base_quality) + \\ optional( COVERAGE_CAP= , self.coverage_cap) PicardCreateSequenceDictionary class PicardCreateSequenceDictionary(Job): def __init__(self): Job.__init__(self) self.input = None self.output_dict = None self.jobname = picard-createdict def command(self): return picard -XX:ParallelGCThreads=1 CreateSequenceDictionary + \\ required( REFERENCE= , self.input) + \\ required( OUTPUT= , self.output_dict) PicardBedToIntervalList class PicardBedToIntervalList(Job): def __init__(self): Job.__init__(self) self.input = None self.reference_dict = None self.output = None self.jobname = picard-bedtointervallist def command(self): return picard -XX:ParallelGCThreads=1 BedToIntervalList + \\ required( INPUT= , self.input) + \\ required( SEQUENCE_DICTIONARY= , self.reference_dict) + \\ required( OUTPUT= , self.output) PicardMergeSamFiles class PicardMergeSamFiles(Job): def __init__(self, input_bams, output_bam, assume_sorted=True, merge_dicts=True): Job.__init__(self) self.input_bams = input_bams self.output_bam = output_bam self.assume_sorted = assume_sorted self.merge_dicts = merge_dicts self.jobname = picard-mergesamfiles def command(self): return picard -XX:ParallelGCThreads=1 MergeSamFiles + \\ repeat( INPUT= , self.input_bams) + \\ required( ASSUME_SORTED= , str(self.assume_sorted).lower()) + \\ required( MERGE_SEQUENCE_DICTIONARIES= , str(self.merge_dicts).lower()) + \\ required( OUTPUT= , self.output_bam) + \\ samtools index + required( , self.output_bam) PicardMarkDuplicates class PicardMarkDuplicates(Job): def __init__(self, input_bam, output_bam, output_metrics, remove_duplicates=False): Job.__init__(self) self.input_bam = input_bam self.output_bam = output_bam self.output_metrics = output_metrics self.remove_duplicates = remove_duplicates self.jobname = picard-markdups def command(self): return picard -Xmx5g -XX:ParallelGCThreads=1 + \\ required( -Djava.io.tmpdir= , self.scratch) + \\ MarkDuplicates + \\ required( INPUT= , self.input_bam) + \\ required( METRICS_FILE= , self.output_metrics) + \\ required( OUTPUT= , self.output_bam) + \\ conditional(self.remove_duplicates, REMOVE_DUPLICATES=true ) + \\ samtools index + required( , self.output_bam)","title":"Picard"},{"location":"lld/picard/#picardcollectinsertsizemetrics","text":"class PicardCollectInsertSizeMetrics(Job): def __init__(self): Job.__init__(self) self.input = None self.output_metrics = None self.jobname = picard-isize def command(self): return picard -XX:ParallelGCThreads=1 CollectInsertSizeMetrics H=/dev/null + \\ required( I= , self.input) + \\ required( O= , self.output_metrics)","title":"PicardCollectInsertSizeMetrics"},{"location":"lld/picard/#picardcollectgcbiasmetrics","text":"class PicardCollectGcBiasMetrics(Job): def __init__(self): Job.__init__(self) self.input = None self.reference_sequence = None self.output_metrics = None self.output_summary = None self.stop_after = None self.jobname = picard-gcbias def command(self): return picard -XX:ParallelGCThreads=1 -Xmx5g CollectGcBiasMetrics CHART=/dev/null + \\ required( I= , self.input) + \\ required( O= , self.output_metrics) + \\ required( S= , self.output_summary) + \\ required( R= , self.reference_sequence) + \\ optional( STOP_AFTER= , self.stop_after)","title":"PicardCollectGcBiasMetrics"},{"location":"lld/picard/#picardcollectoxogmetrics","text":"class PicardCollectOxoGMetrics(Job): def __init__(self): Job.__init__(self) self.input = None self.reference_sequence = None self.output_metrics = None self.jobname = picard-oxog def command(self): return picard -XX:ParallelGCThreads=1 -Xmx2g CollectOxoGMetrics + \\ required( I= , self.input) + \\ required( R= , self.reference_sequence) + \\ required( O= , self.output_metrics)","title":"PicardCollectOxoGMetrics"},{"location":"lld/picard/#picardcollecthsmetrics","text":"class PicardCollectHsMetrics(Job): def __init__(self): Job.__init__(self) self.input = None self.reference_sequence = None self.target_regions = None self.bait_regions = None self.bait_name = None self.output_metrics = None self.accumulation_level = ['LIBRARY'] self.jobname = picard-hsmetrics def command(self): return picard -XX:ParallelGCThreads=1 CollectHsMetrics + \\ required( I= , self.input) + \\ required( R= , self.reference_sequence) + \\ required( O= , self.output_metrics) + \\ required( TI= , self.target_regions) + \\ required( BI= , self.bait_regions) + \\ optional( BAIT_SET_NAME= , self.bait_name) + \\ repeat('METRIC_ACCUMULATION_LEVEL=', self.accumulation_level)","title":"PicardCollectHsMetrics"},{"location":"lld/picard/#picardcollectwgsmetrics","text":"class PicardCollectWgsMetrics(Job): def __init__(self): Job.__init__(self) self.input = None self.reference_sequence = None self.minimum_mapping_quality = None self.minimum_base_quality = None self.coverage_cap = None self.output_metrics = None self.jobname = picard-wgsmetrics def command(self): return picard -XX:ParallelGCThreads=1 CollectWgsMetrics + \\ required( I= , self.input) + \\ required( R= , self.reference_sequence) + \\ required( O= , self.output_metrics) + \\ optional( MINIMUM_MAPPING_QUALITY= , self.minimum_mapping_quality) + \\ optional( MINIMUM_BASE_QUALITY= , self.minimum_base_quality) + \\ optional( COVERAGE_CAP= , self.coverage_cap)","title":"PicardCollectWgsMetrics"},{"location":"lld/picard/#picardcreatesequencedictionary","text":"class PicardCreateSequenceDictionary(Job): def __init__(self): Job.__init__(self) self.input = None self.output_dict = None self.jobname = picard-createdict def command(self): return picard -XX:ParallelGCThreads=1 CreateSequenceDictionary + \\ required( REFERENCE= , self.input) + \\ required( OUTPUT= , self.output_dict)","title":"PicardCreateSequenceDictionary"},{"location":"lld/picard/#picardbedtointervallist","text":"class PicardBedToIntervalList(Job): def __init__(self): Job.__init__(self) self.input = None self.reference_dict = None self.output = None self.jobname = picard-bedtointervallist def command(self): return picard -XX:ParallelGCThreads=1 BedToIntervalList + \\ required( INPUT= , self.input) + \\ required( SEQUENCE_DICTIONARY= , self.reference_dict) + \\ required( OUTPUT= , self.output)","title":"PicardBedToIntervalList"},{"location":"lld/picard/#picardmergesamfiles","text":"class PicardMergeSamFiles(Job): def __init__(self, input_bams, output_bam, assume_sorted=True, merge_dicts=True): Job.__init__(self) self.input_bams = input_bams self.output_bam = output_bam self.assume_sorted = assume_sorted self.merge_dicts = merge_dicts self.jobname = picard-mergesamfiles def command(self): return picard -XX:ParallelGCThreads=1 MergeSamFiles + \\ repeat( INPUT= , self.input_bams) + \\ required( ASSUME_SORTED= , str(self.assume_sorted).lower()) + \\ required( MERGE_SEQUENCE_DICTIONARIES= , str(self.merge_dicts).lower()) + \\ required( OUTPUT= , self.output_bam) + \\ samtools index + required( , self.output_bam)","title":"PicardMergeSamFiles"},{"location":"lld/picard/#picardmarkduplicates","text":"class PicardMarkDuplicates(Job): def __init__(self, input_bam, output_bam, output_metrics, remove_duplicates=False): Job.__init__(self) self.input_bam = input_bam self.output_bam = output_bam self.output_metrics = output_metrics self.remove_duplicates = remove_duplicates self.jobname = picard-markdups def command(self): return picard -Xmx5g -XX:ParallelGCThreads=1 + \\ required( -Djava.io.tmpdir= , self.scratch) + \\ MarkDuplicates + \\ required( INPUT= , self.input_bam) + \\ required( METRICS_FILE= , self.output_metrics) + \\ required( OUTPUT= , self.output_bam) + \\ conditional(self.remove_duplicates, REMOVE_DUPLICATES=true ) + \\ samtools index + required( , self.output_bam)","title":"PicardMarkDuplicates"},{"location":"lld/qc/","text":"Microsatelite Instability MsiSensor class MsiSensor(Job): def __init__(self): Job.__init__(self) self.msi_sites = None self.input_normal_bam = None self.input_tumor_bam = None self.output = None self.jobname = msisensor def command(self): output_prefix = {scratch}/msisensor-{uuid} .format(scratch=self.scratch, uuid=uuid.uuid4()) output_table = {} .format(output_prefix) output_dis = {}_dis .format(output_prefix) output_germline = {}_germline .format(output_prefix) output_somatic = {}_somatic .format(output_prefix) return msisensor msi + \\ required( -d , self.msi_sites) + \\ required( -n , self.input_normal_bam) + \\ required( -t , self.input_tumor_bam) + \\ required( -o , output_prefix) + \\ required( -b , self.threads) + \\ cp {} {} .format(output_prefix, self.output) + \\ rm {} {} {} {} .format(output_table, output_dis, output_germline, output_somatic) MsiSensorScan class MsiSensorScan(Job): def __init__(self): Job.__init__(self) self.input_fasta = None self.output = None self.homopolymers_only = True self.jobname = msisensor-scan def command(self): return msisensor scan + \\ required( -d , self.input_fasta) + \\ required( -o , self.output) + \\ conditional(self.homopolymers_only, -p 1 ) IntersectMsiSites class IntersectMsiSites(Job): def __init__(self): Job.__init__(self) self.input_msi_sites = None self.target_bed = None self.output_msi_sites = True self.jobname = msi-intersect def command(self): return intersect-msi-sites.sh + \\ required( , self.input_msi_sites) + \\ required( , self.target_bed) + \\ required( , self.output_msi_sites) Msings class Msings(Job): def __init__(self): Job.__init__(self) self.input_fasta = None self.outdir = None self.output = None self.msings_baseline = None self.msings_bed = None self.msings_intervals = None self.input_bam = None self.jobname = msings def command(self): return run_msings.sh + \\ required( -b , self.msings_bed) + \\ required( -f , self.input_fasta) + \\ required( -i , self.msings_intervals) + \\ required( -n , self.msings_baseline) + \\ required( -o , self.outdir) + \\ required( , self.input_bam) PureCN PureCN is a purity and ploidy aware copy number caller for cancer samples inspired by the ABSOLUTE algorithm Determining output here purely as a means of determining when the job has completed; PureCN itself determines the output file paths based on the specified \"--output\" and \"--sampleid\" arguments. class PureCN(Job): def __init__(self, input_seg=None, input_vcf=None, tumorid=None, outdir=None, gcgene_file=None, minpurity=0.05, hzdev=0.1, seg_sdev=None, genome= hg19 , postopt=False): Job.__init__(self) self.input_seg = input_seg self.input_vcf = input_vcf self.tumorid = tumorid # should be the same as the ID in the seg file self.outdir = outdir self.gcgene_file = gcgene_file self.minpurity = minpurity self.hzdev = hzdev self.seg_sdev = seg_sdev self.genome = genome self.postopt = postopt self.output = {}/{}_genes.csv .format( self.outdir, self.tumorid) self.jobname = purecn def command(self): # activating conda env activate_cmd = source activate purecn-env # running PureCN running_cmd = PureCN.R + required( --out , self.outdir) + \\ required( --sampleid , self.tumorid) + \\ required( --segfile , self.input_seg) + \\ required( --vcf , self.input_vcf) + \\ required( --gcgene , self.gcgene_file) + \\ required( --genome , self.genome) + \\ optional( --minpurity , self.minpurity) + \\ optional( --hzdev , self.hzdev) + \\ optional( --segfilesdev , self.seg_sdev) + \\ conditional(self.postopt, --postoptimize ) # deactivating the conda env deactivate_cmd = source deactivate return .join([activate_cmd, running_cmd, deactivate_cmd]) Quality Check HeterzygoteConcordance Heterzygote Concoradance module customly added to GATKv3.5 by Klevebring. Jar file is located in autoseq-scripts github repo. class HeterzygoteConcordance(Job): def __init__(self): Job.__init__(self) self.input_vcf = None self.input_bam = None self.reference_sequence = None self.target_regions = None self.normalid = None self.filter_reads_with_N_cigar = True self.output = None self.jobname = hzconcordance def command(self): return gatk-klevebring -T HeterozygoteConcordance + \\ required( -R , self.reference_sequence) + \\ required( -V , self.input_vcf) + \\ required( -I , self.input_bam) + \\ required( -sid , self.normalid) + \\ optional( -L , self.target_regions) + \\ conditional(self.filter_reads_with_N_cigar, --filter_reads_with_N_cigar ) + \\ required( -o , self.output) FastQC class FastQC(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.outdir = None self.extract = False self.jobname = fastqc def command(self): return fastqc -o {} .format(self.outdir) + \\ conditional(self.extract, --extract ) + \\ --nogroup {} .format(self.input) MultiQC class MultiQC(Job): def __init__(self): Job.__init__(self) self.input_files = None self.search_dir = None self.output = None self.report_title = None self.data_format = 'json' self.jobname = multiqc def command(self): required( input_files , self.input_files) required( output_base , self.output) required( dir_to_search , self.search_dir) basefn = os.path.basename(self.output) odir = os.path.dirname(self.output) return multiqc + \\ required( , self.search_dir) + \\ required( -o , odir) + \\ optional( -n , basefn) + \\ optional( -k , self.data_format) + \\ optional( -i , self.report_title) + \\ --data-dir --zip-data-dir -v -f SambambaDepth class SambambaDepth(Job): def __init__(self): Job.__init__(self) self.input = None self.targets_bed = None self.coverage_thresholds = [30, 50, 70, 100, 200, 300] self.output = None self.jobname = sambamba-depth def command(self): return sambamba depth region + \\ repeat( -T , self.coverage_thresholds) + \\ required( --regions , self.targets_bed) + \\ required( , self.input) + \\ required( , self.output) BedtoolsCoverageHistogram class BedtoolsCoverageHistogram(Job): def __init__(self): Job.__init__(self) self.input_bam = None self.input_bed = None self.tag = None self.output = None def command(self): tag_cmd = if self.tag: tag_cmd = echo \\ # {}\\ {} \\n .format(self.tag, self.output) return echo \\ # bedtools-coverage-hist: {}\\ .format(self.input_bam) + \\ required( d , self.output) + \\n + \\ tag_cmd + \\ bedtools coverage -hist + \\ required( -a , self.input_bed) + \\ required( -b , self.input_bam) + \\ |grep \\ ^all\\ + required( , self.output) CoverageHistogram class CoverageHistogram(Job): def __init__(self): Job.__init__(self) self.input_bam = None self.input_bed = None self.min_basequal = None self.output = None def command(self): return target_coverage_histogram.py + \\ required( --targets , self.input_bed) + \\ required( , self.input_bam) + \\ optional( --min_basequal , self.min_basequal) + \\ required( , self.output) CoverageCaveat class CoverageCaveat(Job): def __init__(self): Job.__init__(self) self.input_histogram = None self.output = None self.high_thresh_fraction = 0.95 self.high_thresh_fold_cov = 100 self.low_thresh_fraction = 0.95 self.low_thresh_fold_cov = 50 def command(self): return extract_coverage_caveat.py + \\ required( , self.input_histogram) + \\ required( --high-thresh-fraction , self.high_thresh_fraction) + \\ required( --high-thresh-fold-cov , self.high_thresh_fold_cov) + \\ required( --low-thresh-fraction , self.low_thresh_fraction) + \\ required( --low-thresh-fold-cov , self.low_thresh_fold_cov) + \\ required( , self.output)","title":"Qc"},{"location":"lld/qc/#microsatelite-instability","text":"","title":"Microsatelite Instability"},{"location":"lld/qc/#msisensor","text":"class MsiSensor(Job): def __init__(self): Job.__init__(self) self.msi_sites = None self.input_normal_bam = None self.input_tumor_bam = None self.output = None self.jobname = msisensor def command(self): output_prefix = {scratch}/msisensor-{uuid} .format(scratch=self.scratch, uuid=uuid.uuid4()) output_table = {} .format(output_prefix) output_dis = {}_dis .format(output_prefix) output_germline = {}_germline .format(output_prefix) output_somatic = {}_somatic .format(output_prefix) return msisensor msi + \\ required( -d , self.msi_sites) + \\ required( -n , self.input_normal_bam) + \\ required( -t , self.input_tumor_bam) + \\ required( -o , output_prefix) + \\ required( -b , self.threads) + \\ cp {} {} .format(output_prefix, self.output) + \\ rm {} {} {} {} .format(output_table, output_dis, output_germline, output_somatic)","title":"MsiSensor"},{"location":"lld/qc/#msisensorscan","text":"class MsiSensorScan(Job): def __init__(self): Job.__init__(self) self.input_fasta = None self.output = None self.homopolymers_only = True self.jobname = msisensor-scan def command(self): return msisensor scan + \\ required( -d , self.input_fasta) + \\ required( -o , self.output) + \\ conditional(self.homopolymers_only, -p 1 )","title":"MsiSensorScan"},{"location":"lld/qc/#intersectmsisites","text":"class IntersectMsiSites(Job): def __init__(self): Job.__init__(self) self.input_msi_sites = None self.target_bed = None self.output_msi_sites = True self.jobname = msi-intersect def command(self): return intersect-msi-sites.sh + \\ required( , self.input_msi_sites) + \\ required( , self.target_bed) + \\ required( , self.output_msi_sites)","title":"IntersectMsiSites"},{"location":"lld/qc/#msings","text":"class Msings(Job): def __init__(self): Job.__init__(self) self.input_fasta = None self.outdir = None self.output = None self.msings_baseline = None self.msings_bed = None self.msings_intervals = None self.input_bam = None self.jobname = msings def command(self): return run_msings.sh + \\ required( -b , self.msings_bed) + \\ required( -f , self.input_fasta) + \\ required( -i , self.msings_intervals) + \\ required( -n , self.msings_baseline) + \\ required( -o , self.outdir) + \\ required( , self.input_bam)","title":"Msings"},{"location":"lld/qc/#purecn","text":"PureCN is a purity and ploidy aware copy number caller for cancer samples inspired by the ABSOLUTE algorithm Determining output here purely as a means of determining when the job has completed; PureCN itself determines the output file paths based on the specified \"--output\" and \"--sampleid\" arguments. class PureCN(Job): def __init__(self, input_seg=None, input_vcf=None, tumorid=None, outdir=None, gcgene_file=None, minpurity=0.05, hzdev=0.1, seg_sdev=None, genome= hg19 , postopt=False): Job.__init__(self) self.input_seg = input_seg self.input_vcf = input_vcf self.tumorid = tumorid # should be the same as the ID in the seg file self.outdir = outdir self.gcgene_file = gcgene_file self.minpurity = minpurity self.hzdev = hzdev self.seg_sdev = seg_sdev self.genome = genome self.postopt = postopt self.output = {}/{}_genes.csv .format( self.outdir, self.tumorid) self.jobname = purecn def command(self): # activating conda env activate_cmd = source activate purecn-env # running PureCN running_cmd = PureCN.R + required( --out , self.outdir) + \\ required( --sampleid , self.tumorid) + \\ required( --segfile , self.input_seg) + \\ required( --vcf , self.input_vcf) + \\ required( --gcgene , self.gcgene_file) + \\ required( --genome , self.genome) + \\ optional( --minpurity , self.minpurity) + \\ optional( --hzdev , self.hzdev) + \\ optional( --segfilesdev , self.seg_sdev) + \\ conditional(self.postopt, --postoptimize ) # deactivating the conda env deactivate_cmd = source deactivate return .join([activate_cmd, running_cmd, deactivate_cmd])","title":"PureCN"},{"location":"lld/qc/#quality-check","text":"","title":"Quality Check"},{"location":"lld/qc/#heterzygoteconcordance","text":"Heterzygote Concoradance module customly added to GATKv3.5 by Klevebring. Jar file is located in autoseq-scripts github repo. class HeterzygoteConcordance(Job): def __init__(self): Job.__init__(self) self.input_vcf = None self.input_bam = None self.reference_sequence = None self.target_regions = None self.normalid = None self.filter_reads_with_N_cigar = True self.output = None self.jobname = hzconcordance def command(self): return gatk-klevebring -T HeterozygoteConcordance + \\ required( -R , self.reference_sequence) + \\ required( -V , self.input_vcf) + \\ required( -I , self.input_bam) + \\ required( -sid , self.normalid) + \\ optional( -L , self.target_regions) + \\ conditional(self.filter_reads_with_N_cigar, --filter_reads_with_N_cigar ) + \\ required( -o , self.output)","title":"HeterzygoteConcordance"},{"location":"lld/qc/#fastqc","text":"class FastQC(Job): def __init__(self): Job.__init__(self) self.input = None self.output = None self.outdir = None self.extract = False self.jobname = fastqc def command(self): return fastqc -o {} .format(self.outdir) + \\ conditional(self.extract, --extract ) + \\ --nogroup {} .format(self.input)","title":"FastQC"},{"location":"lld/qc/#multiqc","text":"class MultiQC(Job): def __init__(self): Job.__init__(self) self.input_files = None self.search_dir = None self.output = None self.report_title = None self.data_format = 'json' self.jobname = multiqc def command(self): required( input_files , self.input_files) required( output_base , self.output) required( dir_to_search , self.search_dir) basefn = os.path.basename(self.output) odir = os.path.dirname(self.output) return multiqc + \\ required( , self.search_dir) + \\ required( -o , odir) + \\ optional( -n , basefn) + \\ optional( -k , self.data_format) + \\ optional( -i , self.report_title) + \\ --data-dir --zip-data-dir -v -f","title":"MultiQC"},{"location":"lld/qc/#sambambadepth","text":"class SambambaDepth(Job): def __init__(self): Job.__init__(self) self.input = None self.targets_bed = None self.coverage_thresholds = [30, 50, 70, 100, 200, 300] self.output = None self.jobname = sambamba-depth def command(self): return sambamba depth region + \\ repeat( -T , self.coverage_thresholds) + \\ required( --regions , self.targets_bed) + \\ required( , self.input) + \\ required( , self.output)","title":"SambambaDepth"},{"location":"lld/qc/#bedtoolscoveragehistogram","text":"class BedtoolsCoverageHistogram(Job): def __init__(self): Job.__init__(self) self.input_bam = None self.input_bed = None self.tag = None self.output = None def command(self): tag_cmd = if self.tag: tag_cmd = echo \\ # {}\\ {} \\n .format(self.tag, self.output) return echo \\ # bedtools-coverage-hist: {}\\ .format(self.input_bam) + \\ required( d , self.output) + \\n + \\ tag_cmd + \\ bedtools coverage -hist + \\ required( -a , self.input_bed) + \\ required( -b , self.input_bam) + \\ |grep \\ ^all\\ + required( , self.output)","title":"BedtoolsCoverageHistogram"},{"location":"lld/qc/#coveragehistogram","text":"class CoverageHistogram(Job): def __init__(self): Job.__init__(self) self.input_bam = None self.input_bed = None self.min_basequal = None self.output = None def command(self): return target_coverage_histogram.py + \\ required( --targets , self.input_bed) + \\ required( , self.input_bam) + \\ optional( --min_basequal , self.min_basequal) + \\ required( , self.output)","title":"CoverageHistogram"},{"location":"lld/qc/#coveragecaveat","text":"class CoverageCaveat(Job): def __init__(self): Job.__init__(self) self.input_histogram = None self.output = None self.high_thresh_fraction = 0.95 self.high_thresh_fold_cov = 100 self.low_thresh_fraction = 0.95 self.low_thresh_fold_cov = 50 def command(self): return extract_coverage_caveat.py + \\ required( , self.input_histogram) + \\ required( --high-thresh-fraction , self.high_thresh_fraction) + \\ required( --high-thresh-fold-cov , self.high_thresh_fold_cov) + \\ required( --low-thresh-fraction , self.low_thresh_fraction) + \\ required( --low-thresh-fold-cov , self.low_thresh_fold_cov) + \\ required( , self.output)","title":"CoverageCaveat"},{"location":"lld/reports/","text":"CompileMetadata return: compileMetadata 3098121 3098849 --db_config $HOME/repos/reportgen/tests/referral-db-config.json --output /dev/stdout --address_table_file reportgen/assets/addresses.csv class CompileMetadata(Job): def __init__(self, referral_db_conf, blood_barcode, tumor_barcode, output_json, addresses): Job.__init__(self) self.referral_db_conf = referral_db_conf self.blood_barcode = blood_barcode self.tumor_barcode = tumor_barcode self.output_json = output_json self.addresses = addresses def command(self): return compileMetadata + \\ required('', self.blood_barcode) + \\ required('', self.tumor_barcode) + \\ required('--db_config ', self.referral_db_conf) + \\ required('--address_table_file ', self.addresses) + \\ required('--output ', self.output_json) class CompileAlasccaGenomicJson(Job): def __init__(self, input_somatic_vcf, input_cn_calls, input_msisensor, input_purity_qc, input_contam_qc, input_tcov_qc, input_ncov_qc, output_json): Job.__init__(self) self.input_somatic_vcf = input_somatic_vcf self.input_cn_calls = input_cn_calls self.input_msisensor = input_msisensor self.input_purity_qc = input_purity_qc self.input_contam_qc = input_contam_qc self.input_tcov_qc = input_tcov_qc self.input_ncov_qc = input_ncov_qc self.output_json = output_json def command(self): return 'compileAlasccaGenomicReport ' + \\ required('', self.input_somatic_vcf) + \\ required('', self.input_cn_calls) + \\ required('', self.input_msisensor) + \\ required('--tumorCovJSON ', self.input_tcov_qc) + \\ required('--normalCovJSON ', self.input_ncov_qc) + \\ required('--purityJSON ', self.input_purity_qc) + \\ required('--contaminationJSON ', self.input_contam_qc) + \\ required('--output ', self.output_json) class WriteAlasccaReport(Job): def __init__(self, input_genomic_json, input_metadata_json, output_pdf, alascca_only=False): Job.__init__(self) self.input_metadata_json = input_metadata_json self.input_genomic_json = input_genomic_json self.output_pdf = output_pdf self.alascca_only = alascca_only def command(self): tmpdir = {}/write-alascca-report-{} .format(self.scratch, uuid.uuid4()) mkdir_tmp_cmd = mkdir -p {} .format(tmpdir) tmp_pdf = os.path.join(tmpdir, 'Report.pdf') cmd = 'writeAlasccaReport ' + \\ required(' --tmp_dir ', tmpdir) + \\ required(' --output_dir ', tmpdir) + \\ conditional(self.alascca_only, --alascca_only ) + \\ required('', self.input_genomic_json) + \\ required('', self.input_metadata_json) cp_cmd = cp {} {} .format(tmp_pdf, self.output_pdf) rmdir_cmd = rm -r {} .format(tmpdir) return .join([mkdir_tmp_cmd, cmd, cp_cmd, rmdir_cmd])","title":"Reports"},{"location":"lld/reports/#compilemetadata","text":"return: compileMetadata 3098121 3098849 --db_config $HOME/repos/reportgen/tests/referral-db-config.json --output /dev/stdout --address_table_file reportgen/assets/addresses.csv class CompileMetadata(Job): def __init__(self, referral_db_conf, blood_barcode, tumor_barcode, output_json, addresses): Job.__init__(self) self.referral_db_conf = referral_db_conf self.blood_barcode = blood_barcode self.tumor_barcode = tumor_barcode self.output_json = output_json self.addresses = addresses def command(self): return compileMetadata + \\ required('', self.blood_barcode) + \\ required('', self.tumor_barcode) + \\ required('--db_config ', self.referral_db_conf) + \\ required('--address_table_file ', self.addresses) + \\ required('--output ', self.output_json) class CompileAlasccaGenomicJson(Job): def __init__(self, input_somatic_vcf, input_cn_calls, input_msisensor, input_purity_qc, input_contam_qc, input_tcov_qc, input_ncov_qc, output_json): Job.__init__(self) self.input_somatic_vcf = input_somatic_vcf self.input_cn_calls = input_cn_calls self.input_msisensor = input_msisensor self.input_purity_qc = input_purity_qc self.input_contam_qc = input_contam_qc self.input_tcov_qc = input_tcov_qc self.input_ncov_qc = input_ncov_qc self.output_json = output_json def command(self): return 'compileAlasccaGenomicReport ' + \\ required('', self.input_somatic_vcf) + \\ required('', self.input_cn_calls) + \\ required('', self.input_msisensor) + \\ required('--tumorCovJSON ', self.input_tcov_qc) + \\ required('--normalCovJSON ', self.input_ncov_qc) + \\ required('--purityJSON ', self.input_purity_qc) + \\ required('--contaminationJSON ', self.input_contam_qc) + \\ required('--output ', self.output_json) class WriteAlasccaReport(Job): def __init__(self, input_genomic_json, input_metadata_json, output_pdf, alascca_only=False): Job.__init__(self) self.input_metadata_json = input_metadata_json self.input_genomic_json = input_genomic_json self.output_pdf = output_pdf self.alascca_only = alascca_only def command(self): tmpdir = {}/write-alascca-report-{} .format(self.scratch, uuid.uuid4()) mkdir_tmp_cmd = mkdir -p {} .format(tmpdir) tmp_pdf = os.path.join(tmpdir, 'Report.pdf') cmd = 'writeAlasccaReport ' + \\ required(' --tmp_dir ', tmpdir) + \\ required(' --output_dir ', tmpdir) + \\ conditional(self.alascca_only, --alascca_only ) + \\ required('', self.input_genomic_json) + \\ required('', self.input_metadata_json) cp_cmd = cp {} {} .format(tmp_pdf, self.output_pdf) rmdir_cmd = rm -r {} .format(tmpdir) return .join([mkdir_tmp_cmd, cmd, cp_cmd, rmdir_cmd])","title":"CompileMetadata"},{"location":"lld/utils/","text":"UniqueCapture Fields defining a unique library capture item. Note: A library capture item can include an item where no capture was performed; i.e. capture_kit_id indicates whole-genome sequencing: UniqueCapture = collections.namedtuple( 'UniqueCapture', ['project', 'sdid', 'sample_type', 'sample_id', 'library_kit_id', 'capture_kit_id'] ) data_available_for_clinseq_barcode Check that data is available for the specified clinseq barcode in the specified library folder. param libdir: Directory name where fastqs are organised. param clinseq_barcode: A valid clinseq barcode string return: True if data is available, False otherwise def data_available_for_clinseq_barcode(libdir, clinseq_barcode): if not clinseq_barcode_is_valid(clinseq_barcode): raise ValueError( Invalid clinseq barcode: + clinseq_barcode) filedir = os.path.join(libdir, clinseq_barcode) if not os.path.exists(filedir): logging.warn( Dir {} does not exists for {}. Not using library. .format(filedir, clinseq_barcode)) return False if find_fastqs(clinseq_barcode, libdir) == (None, None): logging.warn( No fastq files found for {} in dir {} .format(clinseq_barcode, filedir)) return False logging.debug( Library {} has data. Using it. .format(clinseq_barcode)) return True convert_to_libdict Converts a clinseq_barcode into a dictionary - introduced to maintain compatibility with the mongoDB \"libraries\" collection, when parse_orderform is used in autoseq-api. param clinseq_barcode: A clinseq barcode string return: A dictionary containing the field types and values present in the barcode def convert_to_libdict(clinseq_barcode): library_id = clinseq_barcode if not clinseq_barcode_is_valid(clinseq_barcode): raise ValueError( Invalid clinseq barcode: + clinseq_barcode) capture_id = parse_capture_id(clinseq_barcode) type = parse_sample_type(clinseq_barcode) sample_id = parse_sample_id(clinseq_barcode) project_id = parse_project(clinseq_barcode) sdid = parse_sdid(clinseq_barcode) prep_id = parse_prep_id(clinseq_barcode) return { library_id : library_id, capture_id : capture_id, type : type, sample_id : sample_id, project_id : project_id, sdid : sdid, prep_id : prep_id} extract_unique_capture Parses the specified clinseq barcode and produces a corresponding UniqueCapture representing the unique library capture corresponding to this clinseq barcode. param clinseq_barcode_tuple: A clinseq barcode string return: UniqueCapture named tuple def extract_unique_capture(clinseq_barcode): if not clinseq_barcode_is_valid(clinseq_barcode): raise ValueError( Invalid clinseq barcode: + clinseq_barcode) project = parse_project(clinseq_barcode) sdid = parse_sdid(clinseq_barcode) sample_type = parse_sample_type(clinseq_barcode) sample_id = parse_sample_id(clinseq_barcode) prep_id = parse_prep_id(clinseq_barcode) capture_id = parse_capture_id(clinseq_barcode) return UniqueCapture(project, sdid, sample_type, sample_id, extract_kit_id(prep_id), extract_kit_id(capture_id)) parse_project Extract the project string from the specified clinseq barcode. param clinseq_barcode: Dash-delimited clinseq barcode string. return: The project field from the input string. def parse_project(clinseq_barcode): return clinseq_barcode.split( - )[0] compose_sample_str Produce a string representing the unique sample for the specified library capture item. param capture: Named tuple indicating a unique sample library capture. return: Dash-delimited string of the fields uniquely identifying the sample. def compose_sample_str(capture): return {}-{}-{}-{} .format( capture.project, capture.sdid, capture.sample_type, capture.sample_id ) compose_lib_capture_str Produce a string for a unique library capture item. param capture: A named tuple identifying a unique sample library capture. return: A dash-delimted string of the fields uniquely identifying the capture. def compose_lib_capture_str(capture): return {}-{}-{}-{}-{}-{} .format( capture.project, capture.sdid, capture.sample_type, capture.sample_id, capture.library_kit_id, capture.capture_kit_id) parse_sample_type Extract the sample type from the clinseq barcode. param clinseq_barcode: Dash-delimited clinseq barcode string. return: The sample type field from the input string. def parse_sample_type(clinseq_barcode): return clinseq_barcode.split( - )[3] parse_sample_id Extract the sample ID from the clinseq barcode. param clinseq_barcode: Dash-delimited clinseq barcode string. return: The sample ID field from the input string. def parse_sample_id(clinseq_barcode): return clinseq_barcode.split( - )[4] parse_sdid Extract the SDID from the clinseq barcode, including the \"P-\" prefix. param clinseq_barcode: Dash-delimited clinseq barcode string. return: The SDID field from the input string. def parse_sdid(clinseq_barcode): return - .join(clinseq_barcode.split( - )[1:3]) parse_prep_id Extract the library prep ID (the entire string - not just the kit ID) from the specified clinseq barcode. param clinseq_barcode: Dash-delimited clinseq barcode string. return: Library prep ID string. def parse_prep_id(clinseq_barcode): return clinseq_barcode.split( - )[5] parse_capture_id Extract the library capture ID (the entire string - not just the capture kit ID) from the specified clinseq barcode. param clinseq_barcode: Dash-delimited clinseq barcode string. return: Library capture ID string. def parse_capture_id(clinseq_barcode): return clinseq_barcode.split( - )[6] extract_kit_id Extract the kit type from a specified kit string (either library or capture kit). param kit_string: A string indicating a kit type, where the first two letters indicate the kit ID. return: The kit ID, comprising the first two letters. def extract_kit_id(kit_string): if len(kit_string) 3: raise ValueError( Invalid kit string: + kit_string) return kit_string[:2] project_valid Validate the project code return True or False (Boolean) def project_valid(project_str): return project_str in [ AL , LB , OT ] sdid_valid Validate the sd id return True or False (Boolean) def sdid_valid(sdid_str): return re.match( ^P-[a-zA-Z0-9]+$ , sdid_str) is not None sample_type_valid validation for sample type. It should be N, T, CFDNA return Boolean (True or False) def sample_type_valid(sample_type_str): return sample_type_str in [ N , T , CFDNA ] sample_id_valid validation for sample id return Boolean (True or False) def sample_id_valid(sample_id_str): return re.match( ^[a-zA-Z0-9]+$ , sample_id_str) is not None prep_id_valid validate the library prep id return Boolean (True or False) def prep_id_valid(prep_id_str): return re.match( ^[A-Z]{2}[0-9]+$ , prep_id_str) is not None capture_id_valid validate the capture kit id return Boolean (True or False) def capture_id_valid(capture_id_str): return (re.match( ^[A-Z]{2}[0-9]+$ , capture_id_str) is not None) or \\ (capture_id_str == WGS ) clinseq_barcode_is_valid Test the structure of the specified clinseq barcode for validity. Barcode format is defined at https://github.com/clinseq/autoseq param clinseq_barcode: The input clinseq barcode. return: True if the barcode has valid structure, False otherwise. def clinseq_barcode_is_valid(clinseq_barcode): fields = clinseq_barcode.split( - ) if len(fields) != 7: return False project_str = fields[0] sdid_str = - .join(fields[1:3]) sample_type_str = fields[3] sample_id_str = fields[4] prep_id_str = fields[5] capture_id_str = fields[6] barcode_valid = \\ project_valid(project_str) and sdid_valid(sdid_str) and sample_type_valid(sample_type_str) and \\ sample_id_valid(sample_id_str) and prep_id_valid(prep_id_str) and capture_id_valid(capture_id_str) return barcode_valid extract_clinseq_barcodes Extrat clinseq barcodes from the specified input file: param input_filename: Either a .txt listing clinseq barcodes one per line, or a .xlsx order form file containing the barcodes. return: A list of (not-yet validated) dash-delimited clinseq barcodes. def extract_clinseq_barcodes(input_filename): toks = input_filename.split( . ) if toks[-1] == txt : return list(set([line.strip() for line in open(input_filename).readlines()])) elif toks[-1] == xlsx : return list(set(parse_orderform(input_filename))) else: raise ValueError( Invalid clinseq barcodes file type: + input_filename) validate_clinseq_barcodes Checks all the specified clinseq barcodes for validity and raises an Exception if any are not. param clinseq_barcodes: List of clinseq barcode strings. def validate_clinseq_barcodes(clinseq_barcodes): # Check the input clinseq barcodes for validity: for clinseq_barcode in clinseq_barcodes: if not clinseq_barcode_is_valid(clinseq_barcode): raise ValueError( Invalid clinseq barcode: + clinseq_barcode) create_scaffold_sampledict Generate a scaffold sample dictionary, with SDIDs as keys and clinseq barcode information dictionaries as values. param sdids: List of SDID strings return: Dictionary with SDID keys and empty clinseq barcode information dictionaries as values. def create_scaffold_sampledict(sdids): scaffold_dict = {} for sdid in sdids: curr_clinseq_barcode_info = { sdid : sdid, N : [], T : [], CFDNA : []} scaffold_dict[sdid] = curr_clinseq_barcode_info return scaffold_dict populate_clinseq_barcode_info Populates the specified clinseq barcode information item with the information in the specified clinseq barcode. param clinseq_barcode_info: A dictionary containing clinseq barcode information for a single SDID. param clinseq_barcode: A validated clinseq barcode string. def populate_clinseq_barcode_info(clinseq_barcode_info, clinseq_barcode): if not clinseq_barcode_is_valid(clinseq_barcode): raise ValueError( Invalid clinseq barcode: , clinseq_barcode) # Append this clinseq barcode to the relevant field in the specified clinseq barcode # info dictionary: sample_type = parse_sample_type(clinseq_barcode) clinseq_barcode_info[sample_type].append(clinseq_barcode) convert_barcodes_to_sampledict Coverts the specified clinseq barcode strings into a dictionary linking from SDID to clinseq barcode information for that individual. param clinseq_barcodes: A list of valid clinseq barcode strings return: A dictionary with the required structure. def convert_barcodes_to_sampledict(clinseq_barcodes): for curr_clinseq_barcode in clinseq_barcodes: if not clinseq_barcode_is_valid(curr_clinseq_barcode): raise ValueError( Invalid clinseq barcode: , curr_clinseq_barcode) # Extract set of unique SDIDs from the specified clinseq barcodes: sdids = set([parse_sdid(clinseq_barcode) for clinseq_barcode in clinseq_barcodes]) # Create a scaffold dictionary from those SDIDs: sdid_to_clinseq_barcode_info = create_scaffold_sampledict(sdids) for clinseq_barcode in clinseq_barcodes: populate_clinseq_barcode_info(sdid_to_clinseq_barcode_info[parse_sdid(clinseq_barcode)], clinseq_barcode) return sdid_to_clinseq_barcode_info","title":"Utils"},{"location":"lld/utils/#uniquecapture","text":"Fields defining a unique library capture item. Note: A library capture item can include an item where no capture was performed; i.e. capture_kit_id indicates whole-genome sequencing: UniqueCapture = collections.namedtuple( 'UniqueCapture', ['project', 'sdid', 'sample_type', 'sample_id', 'library_kit_id', 'capture_kit_id'] )","title":"UniqueCapture"},{"location":"lld/utils/#data_available_for_clinseq_barcode","text":"Check that data is available for the specified clinseq barcode in the specified library folder. param libdir: Directory name where fastqs are organised. param clinseq_barcode: A valid clinseq barcode string return: True if data is available, False otherwise def data_available_for_clinseq_barcode(libdir, clinseq_barcode): if not clinseq_barcode_is_valid(clinseq_barcode): raise ValueError( Invalid clinseq barcode: + clinseq_barcode) filedir = os.path.join(libdir, clinseq_barcode) if not os.path.exists(filedir): logging.warn( Dir {} does not exists for {}. Not using library. .format(filedir, clinseq_barcode)) return False if find_fastqs(clinseq_barcode, libdir) == (None, None): logging.warn( No fastq files found for {} in dir {} .format(clinseq_barcode, filedir)) return False logging.debug( Library {} has data. Using it. .format(clinseq_barcode)) return True","title":"data_available_for_clinseq_barcode"},{"location":"lld/utils/#convert_to_libdict","text":"Converts a clinseq_barcode into a dictionary - introduced to maintain compatibility with the mongoDB \"libraries\" collection, when parse_orderform is used in autoseq-api. param clinseq_barcode: A clinseq barcode string return: A dictionary containing the field types and values present in the barcode def convert_to_libdict(clinseq_barcode): library_id = clinseq_barcode if not clinseq_barcode_is_valid(clinseq_barcode): raise ValueError( Invalid clinseq barcode: + clinseq_barcode) capture_id = parse_capture_id(clinseq_barcode) type = parse_sample_type(clinseq_barcode) sample_id = parse_sample_id(clinseq_barcode) project_id = parse_project(clinseq_barcode) sdid = parse_sdid(clinseq_barcode) prep_id = parse_prep_id(clinseq_barcode) return { library_id : library_id, capture_id : capture_id, type : type, sample_id : sample_id, project_id : project_id, sdid : sdid, prep_id : prep_id}","title":"convert_to_libdict"},{"location":"lld/utils/#extract_unique_capture","text":"Parses the specified clinseq barcode and produces a corresponding UniqueCapture representing the unique library capture corresponding to this clinseq barcode. param clinseq_barcode_tuple: A clinseq barcode string return: UniqueCapture named tuple def extract_unique_capture(clinseq_barcode): if not clinseq_barcode_is_valid(clinseq_barcode): raise ValueError( Invalid clinseq barcode: + clinseq_barcode) project = parse_project(clinseq_barcode) sdid = parse_sdid(clinseq_barcode) sample_type = parse_sample_type(clinseq_barcode) sample_id = parse_sample_id(clinseq_barcode) prep_id = parse_prep_id(clinseq_barcode) capture_id = parse_capture_id(clinseq_barcode) return UniqueCapture(project, sdid, sample_type, sample_id, extract_kit_id(prep_id), extract_kit_id(capture_id))","title":"extract_unique_capture"},{"location":"lld/utils/#parse_project","text":"Extract the project string from the specified clinseq barcode. param clinseq_barcode: Dash-delimited clinseq barcode string. return: The project field from the input string. def parse_project(clinseq_barcode): return clinseq_barcode.split( - )[0]","title":"parse_project"},{"location":"lld/utils/#compose_sample_str","text":"Produce a string representing the unique sample for the specified library capture item. param capture: Named tuple indicating a unique sample library capture. return: Dash-delimited string of the fields uniquely identifying the sample. def compose_sample_str(capture): return {}-{}-{}-{} .format( capture.project, capture.sdid, capture.sample_type, capture.sample_id )","title":"compose_sample_str"},{"location":"lld/utils/#compose_lib_capture_str","text":"Produce a string for a unique library capture item. param capture: A named tuple identifying a unique sample library capture. return: A dash-delimted string of the fields uniquely identifying the capture. def compose_lib_capture_str(capture): return {}-{}-{}-{}-{}-{} .format( capture.project, capture.sdid, capture.sample_type, capture.sample_id, capture.library_kit_id, capture.capture_kit_id)","title":"compose_lib_capture_str"},{"location":"lld/utils/#parse_sample_type","text":"Extract the sample type from the clinseq barcode. param clinseq_barcode: Dash-delimited clinseq barcode string. return: The sample type field from the input string. def parse_sample_type(clinseq_barcode): return clinseq_barcode.split( - )[3]","title":"parse_sample_type"},{"location":"lld/utils/#parse_sample_id","text":"Extract the sample ID from the clinseq barcode. param clinseq_barcode: Dash-delimited clinseq barcode string. return: The sample ID field from the input string. def parse_sample_id(clinseq_barcode): return clinseq_barcode.split( - )[4]","title":"parse_sample_id"},{"location":"lld/utils/#parse_sdid","text":"Extract the SDID from the clinseq barcode, including the \"P-\" prefix. param clinseq_barcode: Dash-delimited clinseq barcode string. return: The SDID field from the input string. def parse_sdid(clinseq_barcode): return - .join(clinseq_barcode.split( - )[1:3])","title":"parse_sdid"},{"location":"lld/utils/#parse_prep_id","text":"Extract the library prep ID (the entire string - not just the kit ID) from the specified clinseq barcode. param clinseq_barcode: Dash-delimited clinseq barcode string. return: Library prep ID string. def parse_prep_id(clinseq_barcode): return clinseq_barcode.split( - )[5]","title":"parse_prep_id"},{"location":"lld/utils/#parse_capture_id","text":"Extract the library capture ID (the entire string - not just the capture kit ID) from the specified clinseq barcode. param clinseq_barcode: Dash-delimited clinseq barcode string. return: Library capture ID string. def parse_capture_id(clinseq_barcode): return clinseq_barcode.split( - )[6]","title":"parse_capture_id"},{"location":"lld/utils/#extract_kit_id","text":"Extract the kit type from a specified kit string (either library or capture kit). param kit_string: A string indicating a kit type, where the first two letters indicate the kit ID. return: The kit ID, comprising the first two letters. def extract_kit_id(kit_string): if len(kit_string) 3: raise ValueError( Invalid kit string: + kit_string) return kit_string[:2]","title":"extract_kit_id"},{"location":"lld/utils/#project_valid","text":"Validate the project code return True or False (Boolean) def project_valid(project_str): return project_str in [ AL , LB , OT ]","title":"project_valid"},{"location":"lld/utils/#sdid_valid","text":"Validate the sd id return True or False (Boolean) def sdid_valid(sdid_str): return re.match( ^P-[a-zA-Z0-9]+$ , sdid_str) is not None","title":"sdid_valid"},{"location":"lld/utils/#sample_type_valid","text":"validation for sample type. It should be N, T, CFDNA return Boolean (True or False) def sample_type_valid(sample_type_str): return sample_type_str in [ N , T , CFDNA ]","title":"sample_type_valid"},{"location":"lld/utils/#sample_id_valid","text":"validation for sample id return Boolean (True or False) def sample_id_valid(sample_id_str): return re.match( ^[a-zA-Z0-9]+$ , sample_id_str) is not None","title":"sample_id_valid"},{"location":"lld/utils/#prep_id_valid","text":"validate the library prep id return Boolean (True or False) def prep_id_valid(prep_id_str): return re.match( ^[A-Z]{2}[0-9]+$ , prep_id_str) is not None","title":"prep_id_valid"},{"location":"lld/utils/#capture_id_valid","text":"validate the capture kit id return Boolean (True or False) def capture_id_valid(capture_id_str): return (re.match( ^[A-Z]{2}[0-9]+$ , capture_id_str) is not None) or \\ (capture_id_str == WGS )","title":"capture_id_valid"},{"location":"lld/utils/#clinseq_barcode_is_valid","text":"Test the structure of the specified clinseq barcode for validity. Barcode format is defined at https://github.com/clinseq/autoseq param clinseq_barcode: The input clinseq barcode. return: True if the barcode has valid structure, False otherwise. def clinseq_barcode_is_valid(clinseq_barcode): fields = clinseq_barcode.split( - ) if len(fields) != 7: return False project_str = fields[0] sdid_str = - .join(fields[1:3]) sample_type_str = fields[3] sample_id_str = fields[4] prep_id_str = fields[5] capture_id_str = fields[6] barcode_valid = \\ project_valid(project_str) and sdid_valid(sdid_str) and sample_type_valid(sample_type_str) and \\ sample_id_valid(sample_id_str) and prep_id_valid(prep_id_str) and capture_id_valid(capture_id_str) return barcode_valid","title":"clinseq_barcode_is_valid"},{"location":"lld/utils/#extract_clinseq_barcodes","text":"Extrat clinseq barcodes from the specified input file: param input_filename: Either a .txt listing clinseq barcodes one per line, or a .xlsx order form file containing the barcodes. return: A list of (not-yet validated) dash-delimited clinseq barcodes. def extract_clinseq_barcodes(input_filename): toks = input_filename.split( . ) if toks[-1] == txt : return list(set([line.strip() for line in open(input_filename).readlines()])) elif toks[-1] == xlsx : return list(set(parse_orderform(input_filename))) else: raise ValueError( Invalid clinseq barcodes file type: + input_filename)","title":"extract_clinseq_barcodes"},{"location":"lld/utils/#validate_clinseq_barcodes","text":"Checks all the specified clinseq barcodes for validity and raises an Exception if any are not. param clinseq_barcodes: List of clinseq barcode strings. def validate_clinseq_barcodes(clinseq_barcodes): # Check the input clinseq barcodes for validity: for clinseq_barcode in clinseq_barcodes: if not clinseq_barcode_is_valid(clinseq_barcode): raise ValueError( Invalid clinseq barcode: + clinseq_barcode)","title":"validate_clinseq_barcodes"},{"location":"lld/utils/#create_scaffold_sampledict","text":"Generate a scaffold sample dictionary, with SDIDs as keys and clinseq barcode information dictionaries as values. param sdids: List of SDID strings return: Dictionary with SDID keys and empty clinseq barcode information dictionaries as values. def create_scaffold_sampledict(sdids): scaffold_dict = {} for sdid in sdids: curr_clinseq_barcode_info = { sdid : sdid, N : [], T : [], CFDNA : []} scaffold_dict[sdid] = curr_clinseq_barcode_info return scaffold_dict","title":"create_scaffold_sampledict"},{"location":"lld/utils/#populate_clinseq_barcode_info","text":"Populates the specified clinseq barcode information item with the information in the specified clinseq barcode. param clinseq_barcode_info: A dictionary containing clinseq barcode information for a single SDID. param clinseq_barcode: A validated clinseq barcode string. def populate_clinseq_barcode_info(clinseq_barcode_info, clinseq_barcode): if not clinseq_barcode_is_valid(clinseq_barcode): raise ValueError( Invalid clinseq barcode: , clinseq_barcode) # Append this clinseq barcode to the relevant field in the specified clinseq barcode # info dictionary: sample_type = parse_sample_type(clinseq_barcode) clinseq_barcode_info[sample_type].append(clinseq_barcode)","title":"populate_clinseq_barcode_info"},{"location":"lld/utils/#convert_barcodes_to_sampledict","text":"Coverts the specified clinseq barcode strings into a dictionary linking from SDID to clinseq barcode information for that individual. param clinseq_barcodes: A list of valid clinseq barcode strings return: A dictionary with the required structure. def convert_barcodes_to_sampledict(clinseq_barcodes): for curr_clinseq_barcode in clinseq_barcodes: if not clinseq_barcode_is_valid(curr_clinseq_barcode): raise ValueError( Invalid clinseq barcode: , curr_clinseq_barcode) # Extract set of unique SDIDs from the specified clinseq barcodes: sdids = set([parse_sdid(clinseq_barcode) for clinseq_barcode in clinseq_barcodes]) # Create a scaffold dictionary from those SDIDs: sdid_to_clinseq_barcode_info = create_scaffold_sampledict(sdids) for clinseq_barcode in clinseq_barcodes: populate_clinseq_barcode_info(sdid_to_clinseq_barcode_info[parse_sdid(clinseq_barcode)], clinseq_barcode) return sdid_to_clinseq_barcode_info","title":"convert_barcodes_to_sampledict"},{"location":"lld/variantcalling/","text":"call_somatic_variants Configuring calling of somatic variants on a given pairing of cancer and normal bam files, using a set of specified algorithms. pipeline: The analysis pipeline for which to configure somatic calling. cancer_bam: Location of the cancer sample bam file normal_bam: Location of the normal sample bam file cancer_capture: A UniqueCapture item identifying the cancer sample library capture normal_capture: A UniqueCapture item identifying the normal sample library capture target_name: The name of the capture panel used outdir: Output location callers: List of calling algorithms to use - can include 'vardict' and/or 'freebayes' min_alt_frac: The minimum allelic fraction value in order to retain a called variant return: A dictionary with somatic caller name as key and corresponding output file location as value def call_somatic_variants(pipeline, cancer_bam, normal_bam, cancer_capture, normal_capture, target_name, outdir, callers=['vardict'], min_alt_frac=0.1, min_num_reads=None): cancer_capture_str = compose_lib_capture_str(cancer_capture) normal_capture_str = compose_lib_capture_str(normal_capture) normal_sample_str = compose_sample_str(normal_capture) tumor_sample_str = compose_sample_str(cancer_capture) d = {} if 'freebayes' in callers: freebayes = Freebayes() freebayes.input_bams = [cancer_bam, normal_bam] freebayes.tumorid = cancer_capture_str freebayes.normalid = normal_capture_str freebayes.somatic_only = True freebayes.reference_sequence = pipeline.refdata['reference_genome'] freebayes.target_bed = pipeline.refdata['targets'][target_name]['targets-bed-slopped20'] freebayes.threads = pipeline.maxcores freebayes.min_alt_frac = min_alt_frac freebayes.scratch = pipeline.scratch freebayes.jobname = freebayes-somatic/{} .format(cancer_capture_str) freebayes.output = {}/variants/{}-{}.freebayes-somatic.vcf.gz .format(outdir, cancer_capture_str, normal_capture_str) pipeline.add(freebayes) d['freebayes'] = freebayes.output capture_name = pipeline.get_capture_name(cancer_capture.capture_kit_id) blacklist_bed = pipeline.refdata[ targets ][capture_name][ blacklist-bed ] if 'vardict' in callers: vardict = VarDict(input_tumor=cancer_bam, input_normal=normal_bam, tumorid=tumor_sample_str, normalid=normal_sample_str, reference_sequence=pipeline.refdata['reference_genome'], reference_dict=pipeline.refdata['reference_dict'], target_bed=pipeline.refdata['targets'][target_name]['targets-bed-slopped20'][:-3], output= {}/variants/vardict/{}-{}.vardict-somatic.vcf.gz .format(outdir, cancer_capture_str, normal_capture_str), min_alt_frac=min_alt_frac, min_num_reads=min_num_reads, blacklist_bed=blacklist_bed ) vardict.jobname = vardict/{} .format(cancer_capture_str) pipeline.add(vardict) d['vardict'] = vardict.output return d Freebayes Freebayes is a bayesian haplotype-based genetic polymorphism discovery and genotyping. class Freebayes(Job): def __init__(self): Job.__init__(self) self.input_bams = None self.reference_sequence = None self.target_bed = None self.somatic_only = False self.params = --pooled-discrete --pooled-continuous --genotype-qualities --report-genotype-likelihood-max --allele-balance-priors-off self.min_coverage = 20 self.min_alt_frac = 0.01 self.use_harmonic_indel_quals = False self.output = self.jobname = freebayes-somatic def command(self): regions_file = {scratch}/{uuid}.regions .format(scratch=self.scratch, uuid=uuid.uuid4()) bed_to_regions_cmd = cat {} | bed_to_regions.py {} .format(self.target_bed, regions_file) call_somatic_cmd = | {} -c 'from autoseq.util.bcbio import call_somatic; import sys; print call_somatic(sys.stdin.read())' .format( sys.executable) freebayes_cmd = freebayes-parallel {} {} .format(regions_file, self.threads) + \\ required( -f , self.reference_sequence) + --use-mapping-quality + \\ optional( --min-alternate-fraction , self.min_alt_frac) + \\ optional( --min-coverage , self.min_coverage) + \\ conditional(self.use_harmonic_indel_quals, --harmonic-indel-quality ) + \\ optional( , self.params) + \\ repeat( , self.input_bams) + \\ | bcftools filter -i 'ALT= * || QUAL 5' + \\ | filter_erroneus_alt.py -V /dev/stdin + \\ conditional(self.somatic_only, call_somatic_cmd) + \\ | + vt_split_and_leftaln(self.reference_sequence) + \\ | vcfuniq | bcftools view --apply-filters .,PASS + \\ | bgzip {output} tabix -p vcf {output} .format(output=self.output) # reason for 'vcfuniq': freebayes sometimes report duplicate variants that need to be uniqified. rm_regions_cmd = rm {} .format(regions_file) return .join([bed_to_regions_cmd, freebayes_cmd, rm_regions_cmd]) VarDict VarDict is an ultra sensitive variant caller for both single and paired sample variant calling from BAM files. VarDict implements several novel features such as amplicon bias aware variant calling from targeted sequencing experiments, rescue of long indels by realigning bwa soft clipped reads and better scalability than many Java based variant callers. class VarDict(Job): def __init__(self, input_tumor=None, input_normal=None, tumorid=None, normalid=None, reference_sequence=None, reference_dict=None, target_bed=None, output=None, min_alt_frac=0.1, min_num_reads=None, blacklist_bed=None): Job.__init__(self) self.input_tumor = input_tumor self.input_normal = input_normal self.tumorid = tumorid self.normalid = normalid self.reference_sequence = reference_sequence self.reference_dict = reference_dict self.target_bed = target_bed self.blacklist_bed = blacklist_bed self.output = output self.min_alt_frac = min_alt_frac self.min_num_reads = min_num_reads def command(self): required( , self.input_tumor) required( , self.input_normal) freq_filter = ( bcftools filter -e 'STATUS !~ \\ .*Somatic\\ ' 2 /dev/null | %s -c 'from autoseq.util.bcbio import depth_freq_filter_input_stream; import sys; print depth_freq_filter_input_stream(sys.stdin, %s, \\ %s\\ )' % (sys.executable, 0, 'bwa')) somatic_filter = ( sed 's/\\\\.*Somatic\\\\\\ /Somatic/' # changes \\ .*Somatic\\ to Somatic | sed 's/REJECT,Description=\\ .*\\ /REJECT,Description=\\ Not Somatic via VarDict\\ /' | %s -c 'from autoseq.util.bcbio import call_somatic; import sys; print call_somatic(sys.stdin.read())' % sys.executable) blacklist_filter = | intersectBed -a . -b {} | .format(self.blacklist_bed) cmd = vardict-java + required( -G , self.reference_sequence) + \\ optional( -f , self.min_alt_frac) + \\ required( -N , self.tumorid) + \\ optional( -r , self.min_num_reads) + \\ -b \\ {}|{}\\ .format(self.input_tumor, self.input_normal) + \\ -c 1 -S 2 -E 3 -g 4 -Q 10 + required( , self.target_bed) + \\ | testsomatic.R + \\ | var2vcf_paired.pl -P 0.9 -m 4.25 -M + required( -f , self.min_alt_frac) + \\ -N \\ {}|{}\\ .format(self.tumorid, self.normalid) + \\ | + freq_filter + | + somatic_filter + | + fix_ambiguous_cl() + | + remove_dup_cl() + \\ | vcfstreamsort -w 1000 + \\ | + vt_split_and_leftaln(self.reference_sequence) + \\ | bcftools view --apply-filters .,PASS + \\ | vcfsorter.pl {} /dev/stdin .format(self.reference_dict) + \\ conditional(blacklist_filter, self.blacklist_bed) + \\ | bgzip {output} tabix -p vcf {output} .format(output=self.output) return cmd VarDictForPureCN VarDict is used to generate vcf file which is can be given as input for PureCN to identify tumor purity. class VarDictForPureCN(Job): def __init__(self, input_tumor=None, input_normal=None, tumorid=None, normalid=None, reference_sequence=None, reference_dict=None, target_bed=None, output=None, min_alt_frac=0.1, min_num_reads=None, dbsnp=None): Job.__init__(self) self.input_tumor = input_tumor self.input_normal = input_normal self.tumorid = tumorid self.normalid = normalid self.reference_sequence = reference_sequence self.reference_dict = reference_dict self.target_bed = target_bed self.output = output self.min_alt_frac = min_alt_frac self.min_num_reads = min_num_reads self.dbsnp = dbsnp def command(self): required( , self.input_tumor) required( , self.input_normal) tmp_vcf = {scratch}/{uuid}.vcf.gz .format(scratch=self.scratch, uuid=uuid.uuid4()) # run vardict without removing non-somatic variants, and adding SOMATIC INFO field for somatic variants vardict_cmd = vardict-java + required( -G , self.reference_sequence) + \\ optional( -f , self.min_alt_frac) + \\ required( -N , self.tumorid) + \\ optional( -r , self.min_num_reads) + \\ -b \\ {}|{}\\ .format(self.input_tumor, self.input_normal) + \\ -c 1 -S 2 -E 3 -g 4 -Q 10 + required( , self.target_bed) + \\ | testsomatic.R + \\ | var2vcf_paired.pl -P 0.9 -m 4.25 + required( -f , self.min_alt_frac) + \\ -N \\ {}|{}\\ .format(self.tumorid, self.normalid) + \\ | + fix_ambiguous_cl() + | + remove_dup_cl() + \\ | sed 's/Somatic;/Somatic;SOMATIC;/g' + \\ | sed '/^#CHROM/i ##INFO= ID=SOMATIC,Number=0,Type=Flag,Description=\\ Somatic event\\ ' + \\ | vcfstreamsort -w 1000 + \\ | bcftools view --apply-filters .,PASS + \\ | vcfsorter.pl {} /dev/stdin .format(self.reference_dict) + \\ | bgzip + tmp_vcf + tabix -p vcf + tmp_vcf # annotate variants with dbSNP id annotate_cmd = bcftools annotate --annotation {} --columns ID .format(self.dbsnp) + \\ --output-type z --output {} .format(self.output) + tmp_vcf + \\ tabix -p vcf {} .format(self.output) # remove temporary vcf and tabix rm_tmp_cmd = rm + tmp_vcf + * return .join([vardict_cmd, annotate_cmd, rm_tmp_cmd]) VEP VEP is a annotation tool, adds clinical significance information to INFO colum in vcf file using CSQ tag. CSQ tag can be used to filter out variants based on multiple features. class VEP(Job): def __init__(self): Job.__init__(self) self.input_vcf = None self.output_vcf = None self.reference_sequence = None self.vep_dir = None self.brca_exchange_vcf = None self.jobname = vep self.additional_options = def command(self): fork = if self.threads 1: # vep does not accept --fork 1 , so need to check. fork = --fork {} .format(self.threads) cmdstr = vep --vcf --output_file STDOUT + \\ self.additional_options + required( --dir , self.vep_dir) + \\ required( --fasta , self.reference_sequence) + \\ required( -i , self.input_vcf) + \\ --check_existing --total_length --allele_number + \\ --no_escape --no_stats --everything --offline + \\ --custom {},,vcf,exact,0,ClinicalSignificance .format(self.brca_exchange_vcf) + \\ fork + + required( , self.output_vcf) # tabix -p vcf {} .format(self.output_vcf) return cmdstr VcfAddSample Add DP, RO and AO tags for a new sample to a VCF, filter low-qual variants on the fly class VcfAddSample(Job): def __init__(self): Job.__init__(self) self.input_vcf = None self.input_bam = None self.samplename = None self.filter_hom = True self.output = None self.jobname = vcf-add-sample def command(self): filt_vcf = {scratch}/{uuid}.vcf.gz .format(scratch=self.scratch, uuid=uuid.uuid4()) bgzip = tabix = if self.output.endswith('gz'): bgzip = | bgzip tabix = tabix -p vcf {} .format(self.output) filt_vcf_cmd = vcf_filter.py --no-filtered + required( , self.input_vcf) + sq --site-quality 5 + \\ |bgzip + + filt_vcf vcf_add_sample_cmd = vcf_add_sample.py + \\ conditional(self.filter_hom, --filter_hom ) + \\ required( --samplename , self.samplename) + \\ filt_vcf + + \\ required( , self.input_bam) + \\ bgzip + + self.output + tabix rm_filt_cmd = rm + filt_vcf return .join([filt_vcf_cmd, vcf_add_sample_cmd, rm_filt_cmd]) SVcaller SVcaller is in-house structural variant caller based on delly algorithym with strigent filters. class Svcaller(Job): def __init__(self): Job.__init__(self) self.input_bam = None self.event_type = None self.output_bam = None self.output_gtf = None self.reference_sequence = None self.scratch = None self.jobname = svcaller-run-all def command(self): activate_env_cmd = source activate svcallerenv run_all_cmd = ( svcaller run-all --tmp-dir {scratch} + --event-type {event_type} + --fasta-filename {reference_seq} + --filter-event-overlap --events-gtf {output_gtf} --events-bam {output_bam} {input_bam} ).format( scratch=self.scratch, event_type=self.event_type, reference_seq=self.reference_sequence, output_gtf=self.output_gtf, output_bam=self.output_bam, input_bam=self.input_bam, ) deactivate_env_cmd = source deactivate return {} {} {} .format( activate_env_cmd, run_all_cmd, deactivate_env_cmd, ) class Sveffect(Job): def __init__(self): Job.__init__(self) self.input_del_gtf = None self.input_dup_gtf = None self.input_inv_gtf = None self.input_tra_gtf = None self.ts_regions = None self.ar_regions = None self.fusion_regions = None self.output_combined_bed = None self.output_effects_json = None self.jobname = sveffect def command(self): activate_env_cmd = source activate svcallerenv make_bed_cmd = ( sveffect make-bed + --del-gtf {del_gtf} + --dup-gtf {dup_gtf} + --inv-gtf {inv_gtf} + --tra-gtf {tra_gtf} + {output_combined_bed} ).format( del_gtf=self.input_del_gtf, dup_gtf=self.input_dup_gtf, inv_gtf=self.input_inv_gtf, tra_gtf=self.input_tra_gtf, output_combined_bed=self.output_combined_bed ) predict_cmd = ( sveffect predict + --ts-regions {ts_regions} + --ar-regions {ar_regions} + --fusion-regions {fusion_regions} + --effects-filename {output_effects_json} {combined_effects_bed} ).format( ts_regions=self.ts_regions, ar_regions=self.ar_regions, fusion_regions=self.fusion_regions, output_effects_json=self.output_effects_json, combined_effects_bed=self.output_combined_bed, ) deactivate_env_cmd = source deactivate return {} {} {} {} .format( activate_env_cmd, make_bed_cmd, predict_cmd, deactivate_env_cmd, ) CNV calling Copy Number Variation is called by cnvkit class CNVkit(Job): Runs CNVkit. Either reference or targets_bed must be supplied def __init__(self, input_bam, output_cns, output_cnr, reference=None, targets_bed=None, scratch= /tmp , fasta=None): self.input_bam = input_bam self.reference = reference self.fasta = fasta self.output_cnr = output_cnr self.output_cns = output_cns self.targets_bed = targets_bed self.scratch = scratch self.jobname = cnvkit def command(self): if not self.reference and not self.targets_bed: raise ValueError( Either reference or targets_bed must be supplied ) if self.reference and self.targets_bed: raise ValueError( Supply either reference OR targets_bed ) tmpdir = {}/cnvkit-{} .format(self.scratch, uuid.uuid4()) sample_prefix = stripsuffix(os.path.basename(self.input_bam), .bam ) cnvkit_cmd = cnvkit.py batch + required( , self.input_bam) + \\ optional( -r , self.reference) + \\ conditional(self.targets_bed, --fasta + str(self.fasta) ) + \\ conditional(self.targets_bed, -n ) + \\ optional( -t , self.targets_bed) + \\ required( -d , tmpdir) copy_cns_cmd = cp {}/{}.cns .format(tmpdir, sample_prefix) + required( , self.output_cns) copy_cnr_cmd = cp {}/{}.cnr .format(tmpdir, sample_prefix) + required( , self.output_cnr) rm_cmd = rm -r {} .format(tmpdir) return .join([cnvkit_cmd, copy_cns_cmd, copy_cnr_cmd, rm_cmd]) Fixes the output of CNV-kit, using a specified table of reference data that should be specific to the sample type, capture kit, and library prep kit. class CNVkitFix(Job): def __init__(self, input_cnr, input_cns, input_ref, output_cnr, output_cns): self.input_cnr = input_cnr self.input_cns = input_cns self.input_ref = input_ref self.output_cns = output_cns self.output_cnr = output_cnr self.jobname = cnvkit-fix def command(self): return ( fix_cnvkit.py --input-cnr {input_cnr} --input-cns {input_cns} + --input-reference {input_ref} --output-cnr {output_cnr} + --output-cns {output_cns} ).format( input_cnr=self.input_cnr, input_cns=self.input_cns, input_ref=self.input_ref, output_cns=self.output_cns, output_cnr=self.output_cnr) Converts CNVkit segment format (.cns files) to DNAcopy segment format (.seg files). class Cns2Seg(Job): def __init__(self, input_cns, output_seg): self.input_cns = input_cns self.output_seg = output_seg def command(self): cmd = cnvkit.py export seg + required( -o , self.output_seg) + required( , self.input_cns) return cmd QDNASeq class QDNASeq(Job): def __init__(self, input_bam, output_segments, background=None): Job.__init__(self) self.input = input_bam self.output = output_segments self.background = background self.jobname = qdnaseq def command(self): activate_env_cmd = source activate qdnaseqenv qdnaseq_cmd = qdnaseq.R + \\ required( --bam , self.input) + \\ required( --output , self.output) + \\ optional( --background , self.background) deactivate_env_cmd = source deactivate return {} {} {} .format( activate_env_cmd, qdnaseq_cmd, deactivate_env_cmd, ) class QDNASeq2Bed(Job): def __init__(self, input_segments, output_bed, genes_gtf): Job.__init__(self) self.input_segments = input_segments self.output_bed = output_bed self.genes_gtf = genes_gtf def command(self): qdnaseq2bed_cmd = qdnaseq2bed.py -n segments + \\ required( -i , self.input_segments) + \\ | sort -k1,1 -k2,2n + \\ | bedtools median -c 5 -o mean + \\ required( -a , self.genes_gtf) + -b - + \\ | cnvgtf2bed.py -i /dev/stdin -n gene_id + \\ required( , self.output_bed) return qdnaseq2bed_cmd AlasccaCNAPlot class AlasccaCNAPlot(Job): def __init__(self): Job.__init__(self) self.input_cnr = None self.input_cns = None self.input_germline_vcf = None self.input_somatic_vcf = None self.chrsizes = None self.output_png = None self.output_cna = None self.output_purity = None self.jobname = alascca-cna def command(self): return alasccaCNA.R + \\ required( --cnr , self.input_cnr) + \\ required( --cns , self.input_cns) + \\ required( --germlinevcf , self.input_germline_vcf) + \\ required( --somaticvcf , self.input_somatic_vcf) + \\ required( --chrsizes , self.chrsizes) + \\ required( --png , self.output_png) + \\ required( --json.cna , self.output_cna) + \\ required( --json.purity , self.output_purity) LiqbioCNAPlot class LiqbioCNAPlot(Job): def __init__(self): Job.__init__(self) self.input_tumor_cnr = None self.input_tumor_cns = None self.input_normal_cnr = None self.input_normal_cns = None self.input_het_snps_vcf = None self.input_purecn_csv = None self.input_purecn_genes_csv = None self.input_purecn_loh_csv = None self.input_purecn_variants_csv = None self.input_svcaller_T_DEL = None self.input_svcaller_T_DUP = None self.input_svcaller_T_INV = None self.input_svcaller_T_TRA = None self.input_svcaller_N_DEL = None self.input_svcaller_N_DUP = None self.input_svcaller_N_INV = None self.input_svcaller_N_TRA = None self.input_germline_mut_vcf = None self.input_somatic_mut_vcf = None self.output_plot_png = None self.output_cna_json = None self.output_purity_json = None self.jobname = liqbio-cna def command(self): return liqbioCNA.R + \\ required( --tumor_cnr , self.input_tumor_cnr) + \\ required( --tumor_cns , self.input_tumor_cns) + \\ required( --normal_cnr , self.input_normal_cnr) + \\ required( --normal_cns , self.input_normal_cns) + \\ required( --het_snps_vcf , self.input_het_snps_vcf) + \\ required( --purecn_csv , self.input_purecn_csv) + \\ required( --purecn_genes_csv , self.input_purecn_genes_csv) + \\ required( --purecn_loh_csv , self.input_purecn_loh_csv) + \\ required( --purecn_variants_csv , self.input_purecn_variants_csv) + \\ required( --svcaller_T_DEL , self.input_svcaller_T_DEL) + \\ required( --svcaller_T_DUP , self.input_svcaller_T_DUP) + \\ required( --svcaller_T_INV , self.input_svcaller_T_INV) + \\ required( --svcaller_T_TRA , self.input_svcaller_T_TRA) + \\ required( --svcaller_N_DEL , self.input_svcaller_N_DEL) + \\ required( --svcaller_N_DUP , self.input_svcaller_N_DUP) + \\ required( --svcaller_N_INV , self.input_svcaller_N_INV) + \\ required( --svcaller_N_TRA , self.input_svcaller_N_TRA) + \\ required( --germline_mut_vcf , self.input_germline_mut_vcf) + \\ required( --somatic_mut_vcf , self.input_somatic_mut_vcf) + \\ required( --plot_png , self.output_plot_png) + \\ required( --cna_json , self.output_cna_json) + \\ required( --purity_json , self.output_purity_json) MakeCNVkitTracks Generate a IGV tracks representing the profile and segment information from a CNV-kit run. class MakeCNVkitTracks(Job): def __init__(self): Job.__init__(self) self.input_cns = None self.input_cnr = None self.output_profile_bedgraph = None self.output_segments_bedgraph = None self.jobname = make_cnvkit_tracks def command(self): awk_cmd1 = awk '$1 != \\ chromosome\\ {print $1\\ \\\\t\\ $2\\ \\\\t\\ $3\\ \\\\t\\ $5}' %s %s % \\ (self.input_cnr, self.output_profile_bedgraph) awk_cmd2 = awk '$1 != \\ chromosome\\ {print $1\\ \\\\t\\ $2\\ \\\\t\\ $3\\ \\\\t\\ $5}' %s %s % \\ (self.input_cns, self.output_segments_bedgraph) return {} {} .format(awk_cmd1, awk_cmd2) MakeQDNAseqTracks Generate a IGV tracks representing the information from a QDNA-seq run. class MakeQDNAseqTracks(Job): def __init__(self): Job.__init__(self) self.input_qdnaseq_file = None self.output_segments_bedgraph = None self.output_copynumber_tdf = None self.output_readcount_tdf = None self.jobname = make_qdnaseq_tracks def command(self): copynumber_wig = {scratch}/copynumber-{uuid}.wig .format( scratch=self.scratch, uuid=uuid.uuid4()) readcount_wig = {scratch}/readcount-{uuid}.wig .format( scratch=self.scratch, uuid=uuid.uuid4()) qdnaseq_to_bedgraph_cmd = qdnaseq_to_bedgraph.py {} {} .format( self.input_qdnaseq_file, self.output_segments_bedgraph) qdnaseq_to_wig_cmd = qdnaseq_to_wig.py {} {} {} .format( self.input_qdnaseq_file, copynumber_wig, readcount_wig) igvtools_cmd1 = igvtools toTDF {copynumber_wig} {copynumber_tdf} hg19 .format( copynumber_wig=copynumber_wig, copynumber_tdf=self.output_copynumber_tdf ) igvtools_cmd2 = igvtools toTDF {readcount_wig} {readcount_tdf} hg19 .format( readcount_wig=readcount_wig, readcount_tdf=self.output_readcount_tdf ) return {} {} {} .format(qdnaseq_to_bedgraph_cmd, qdnaseq_to_wig_cmd, igvtools_cmd1, igvtools_cmd2)","title":"Variantcalling"},{"location":"lld/variantcalling/#call_somatic_variants","text":"Configuring calling of somatic variants on a given pairing of cancer and normal bam files, using a set of specified algorithms. pipeline: The analysis pipeline for which to configure somatic calling. cancer_bam: Location of the cancer sample bam file normal_bam: Location of the normal sample bam file cancer_capture: A UniqueCapture item identifying the cancer sample library capture normal_capture: A UniqueCapture item identifying the normal sample library capture target_name: The name of the capture panel used outdir: Output location callers: List of calling algorithms to use - can include 'vardict' and/or 'freebayes' min_alt_frac: The minimum allelic fraction value in order to retain a called variant return: A dictionary with somatic caller name as key and corresponding output file location as value def call_somatic_variants(pipeline, cancer_bam, normal_bam, cancer_capture, normal_capture, target_name, outdir, callers=['vardict'], min_alt_frac=0.1, min_num_reads=None): cancer_capture_str = compose_lib_capture_str(cancer_capture) normal_capture_str = compose_lib_capture_str(normal_capture) normal_sample_str = compose_sample_str(normal_capture) tumor_sample_str = compose_sample_str(cancer_capture) d = {} if 'freebayes' in callers: freebayes = Freebayes() freebayes.input_bams = [cancer_bam, normal_bam] freebayes.tumorid = cancer_capture_str freebayes.normalid = normal_capture_str freebayes.somatic_only = True freebayes.reference_sequence = pipeline.refdata['reference_genome'] freebayes.target_bed = pipeline.refdata['targets'][target_name]['targets-bed-slopped20'] freebayes.threads = pipeline.maxcores freebayes.min_alt_frac = min_alt_frac freebayes.scratch = pipeline.scratch freebayes.jobname = freebayes-somatic/{} .format(cancer_capture_str) freebayes.output = {}/variants/{}-{}.freebayes-somatic.vcf.gz .format(outdir, cancer_capture_str, normal_capture_str) pipeline.add(freebayes) d['freebayes'] = freebayes.output capture_name = pipeline.get_capture_name(cancer_capture.capture_kit_id) blacklist_bed = pipeline.refdata[ targets ][capture_name][ blacklist-bed ] if 'vardict' in callers: vardict = VarDict(input_tumor=cancer_bam, input_normal=normal_bam, tumorid=tumor_sample_str, normalid=normal_sample_str, reference_sequence=pipeline.refdata['reference_genome'], reference_dict=pipeline.refdata['reference_dict'], target_bed=pipeline.refdata['targets'][target_name]['targets-bed-slopped20'][:-3], output= {}/variants/vardict/{}-{}.vardict-somatic.vcf.gz .format(outdir, cancer_capture_str, normal_capture_str), min_alt_frac=min_alt_frac, min_num_reads=min_num_reads, blacklist_bed=blacklist_bed ) vardict.jobname = vardict/{} .format(cancer_capture_str) pipeline.add(vardict) d['vardict'] = vardict.output return d","title":"call_somatic_variants"},{"location":"lld/variantcalling/#freebayes","text":"Freebayes is a bayesian haplotype-based genetic polymorphism discovery and genotyping. class Freebayes(Job): def __init__(self): Job.__init__(self) self.input_bams = None self.reference_sequence = None self.target_bed = None self.somatic_only = False self.params = --pooled-discrete --pooled-continuous --genotype-qualities --report-genotype-likelihood-max --allele-balance-priors-off self.min_coverage = 20 self.min_alt_frac = 0.01 self.use_harmonic_indel_quals = False self.output = self.jobname = freebayes-somatic def command(self): regions_file = {scratch}/{uuid}.regions .format(scratch=self.scratch, uuid=uuid.uuid4()) bed_to_regions_cmd = cat {} | bed_to_regions.py {} .format(self.target_bed, regions_file) call_somatic_cmd = | {} -c 'from autoseq.util.bcbio import call_somatic; import sys; print call_somatic(sys.stdin.read())' .format( sys.executable) freebayes_cmd = freebayes-parallel {} {} .format(regions_file, self.threads) + \\ required( -f , self.reference_sequence) + --use-mapping-quality + \\ optional( --min-alternate-fraction , self.min_alt_frac) + \\ optional( --min-coverage , self.min_coverage) + \\ conditional(self.use_harmonic_indel_quals, --harmonic-indel-quality ) + \\ optional( , self.params) + \\ repeat( , self.input_bams) + \\ | bcftools filter -i 'ALT= * || QUAL 5' + \\ | filter_erroneus_alt.py -V /dev/stdin + \\ conditional(self.somatic_only, call_somatic_cmd) + \\ | + vt_split_and_leftaln(self.reference_sequence) + \\ | vcfuniq | bcftools view --apply-filters .,PASS + \\ | bgzip {output} tabix -p vcf {output} .format(output=self.output) # reason for 'vcfuniq': freebayes sometimes report duplicate variants that need to be uniqified. rm_regions_cmd = rm {} .format(regions_file) return .join([bed_to_regions_cmd, freebayes_cmd, rm_regions_cmd])","title":"Freebayes"},{"location":"lld/variantcalling/#vardict","text":"VarDict is an ultra sensitive variant caller for both single and paired sample variant calling from BAM files. VarDict implements several novel features such as amplicon bias aware variant calling from targeted sequencing experiments, rescue of long indels by realigning bwa soft clipped reads and better scalability than many Java based variant callers. class VarDict(Job): def __init__(self, input_tumor=None, input_normal=None, tumorid=None, normalid=None, reference_sequence=None, reference_dict=None, target_bed=None, output=None, min_alt_frac=0.1, min_num_reads=None, blacklist_bed=None): Job.__init__(self) self.input_tumor = input_tumor self.input_normal = input_normal self.tumorid = tumorid self.normalid = normalid self.reference_sequence = reference_sequence self.reference_dict = reference_dict self.target_bed = target_bed self.blacklist_bed = blacklist_bed self.output = output self.min_alt_frac = min_alt_frac self.min_num_reads = min_num_reads def command(self): required( , self.input_tumor) required( , self.input_normal) freq_filter = ( bcftools filter -e 'STATUS !~ \\ .*Somatic\\ ' 2 /dev/null | %s -c 'from autoseq.util.bcbio import depth_freq_filter_input_stream; import sys; print depth_freq_filter_input_stream(sys.stdin, %s, \\ %s\\ )' % (sys.executable, 0, 'bwa')) somatic_filter = ( sed 's/\\\\.*Somatic\\\\\\ /Somatic/' # changes \\ .*Somatic\\ to Somatic | sed 's/REJECT,Description=\\ .*\\ /REJECT,Description=\\ Not Somatic via VarDict\\ /' | %s -c 'from autoseq.util.bcbio import call_somatic; import sys; print call_somatic(sys.stdin.read())' % sys.executable) blacklist_filter = | intersectBed -a . -b {} | .format(self.blacklist_bed) cmd = vardict-java + required( -G , self.reference_sequence) + \\ optional( -f , self.min_alt_frac) + \\ required( -N , self.tumorid) + \\ optional( -r , self.min_num_reads) + \\ -b \\ {}|{}\\ .format(self.input_tumor, self.input_normal) + \\ -c 1 -S 2 -E 3 -g 4 -Q 10 + required( , self.target_bed) + \\ | testsomatic.R + \\ | var2vcf_paired.pl -P 0.9 -m 4.25 -M + required( -f , self.min_alt_frac) + \\ -N \\ {}|{}\\ .format(self.tumorid, self.normalid) + \\ | + freq_filter + | + somatic_filter + | + fix_ambiguous_cl() + | + remove_dup_cl() + \\ | vcfstreamsort -w 1000 + \\ | + vt_split_and_leftaln(self.reference_sequence) + \\ | bcftools view --apply-filters .,PASS + \\ | vcfsorter.pl {} /dev/stdin .format(self.reference_dict) + \\ conditional(blacklist_filter, self.blacklist_bed) + \\ | bgzip {output} tabix -p vcf {output} .format(output=self.output) return cmd","title":"VarDict"},{"location":"lld/variantcalling/#vardictforpurecn","text":"VarDict is used to generate vcf file which is can be given as input for PureCN to identify tumor purity. class VarDictForPureCN(Job): def __init__(self, input_tumor=None, input_normal=None, tumorid=None, normalid=None, reference_sequence=None, reference_dict=None, target_bed=None, output=None, min_alt_frac=0.1, min_num_reads=None, dbsnp=None): Job.__init__(self) self.input_tumor = input_tumor self.input_normal = input_normal self.tumorid = tumorid self.normalid = normalid self.reference_sequence = reference_sequence self.reference_dict = reference_dict self.target_bed = target_bed self.output = output self.min_alt_frac = min_alt_frac self.min_num_reads = min_num_reads self.dbsnp = dbsnp def command(self): required( , self.input_tumor) required( , self.input_normal) tmp_vcf = {scratch}/{uuid}.vcf.gz .format(scratch=self.scratch, uuid=uuid.uuid4()) # run vardict without removing non-somatic variants, and adding SOMATIC INFO field for somatic variants vardict_cmd = vardict-java + required( -G , self.reference_sequence) + \\ optional( -f , self.min_alt_frac) + \\ required( -N , self.tumorid) + \\ optional( -r , self.min_num_reads) + \\ -b \\ {}|{}\\ .format(self.input_tumor, self.input_normal) + \\ -c 1 -S 2 -E 3 -g 4 -Q 10 + required( , self.target_bed) + \\ | testsomatic.R + \\ | var2vcf_paired.pl -P 0.9 -m 4.25 + required( -f , self.min_alt_frac) + \\ -N \\ {}|{}\\ .format(self.tumorid, self.normalid) + \\ | + fix_ambiguous_cl() + | + remove_dup_cl() + \\ | sed 's/Somatic;/Somatic;SOMATIC;/g' + \\ | sed '/^#CHROM/i ##INFO= ID=SOMATIC,Number=0,Type=Flag,Description=\\ Somatic event\\ ' + \\ | vcfstreamsort -w 1000 + \\ | bcftools view --apply-filters .,PASS + \\ | vcfsorter.pl {} /dev/stdin .format(self.reference_dict) + \\ | bgzip + tmp_vcf + tabix -p vcf + tmp_vcf # annotate variants with dbSNP id annotate_cmd = bcftools annotate --annotation {} --columns ID .format(self.dbsnp) + \\ --output-type z --output {} .format(self.output) + tmp_vcf + \\ tabix -p vcf {} .format(self.output) # remove temporary vcf and tabix rm_tmp_cmd = rm + tmp_vcf + * return .join([vardict_cmd, annotate_cmd, rm_tmp_cmd])","title":"VarDictForPureCN"},{"location":"lld/variantcalling/#vep","text":"VEP is a annotation tool, adds clinical significance information to INFO colum in vcf file using CSQ tag. CSQ tag can be used to filter out variants based on multiple features. class VEP(Job): def __init__(self): Job.__init__(self) self.input_vcf = None self.output_vcf = None self.reference_sequence = None self.vep_dir = None self.brca_exchange_vcf = None self.jobname = vep self.additional_options = def command(self): fork = if self.threads 1: # vep does not accept --fork 1 , so need to check. fork = --fork {} .format(self.threads) cmdstr = vep --vcf --output_file STDOUT + \\ self.additional_options + required( --dir , self.vep_dir) + \\ required( --fasta , self.reference_sequence) + \\ required( -i , self.input_vcf) + \\ --check_existing --total_length --allele_number + \\ --no_escape --no_stats --everything --offline + \\ --custom {},,vcf,exact,0,ClinicalSignificance .format(self.brca_exchange_vcf) + \\ fork + + required( , self.output_vcf) # tabix -p vcf {} .format(self.output_vcf) return cmdstr","title":"VEP"},{"location":"lld/variantcalling/#vcfaddsample","text":"Add DP, RO and AO tags for a new sample to a VCF, filter low-qual variants on the fly class VcfAddSample(Job): def __init__(self): Job.__init__(self) self.input_vcf = None self.input_bam = None self.samplename = None self.filter_hom = True self.output = None self.jobname = vcf-add-sample def command(self): filt_vcf = {scratch}/{uuid}.vcf.gz .format(scratch=self.scratch, uuid=uuid.uuid4()) bgzip = tabix = if self.output.endswith('gz'): bgzip = | bgzip tabix = tabix -p vcf {} .format(self.output) filt_vcf_cmd = vcf_filter.py --no-filtered + required( , self.input_vcf) + sq --site-quality 5 + \\ |bgzip + + filt_vcf vcf_add_sample_cmd = vcf_add_sample.py + \\ conditional(self.filter_hom, --filter_hom ) + \\ required( --samplename , self.samplename) + \\ filt_vcf + + \\ required( , self.input_bam) + \\ bgzip + + self.output + tabix rm_filt_cmd = rm + filt_vcf return .join([filt_vcf_cmd, vcf_add_sample_cmd, rm_filt_cmd])","title":"VcfAddSample"},{"location":"lld/variantcalling/#svcaller","text":"SVcaller is in-house structural variant caller based on delly algorithym with strigent filters. class Svcaller(Job): def __init__(self): Job.__init__(self) self.input_bam = None self.event_type = None self.output_bam = None self.output_gtf = None self.reference_sequence = None self.scratch = None self.jobname = svcaller-run-all def command(self): activate_env_cmd = source activate svcallerenv run_all_cmd = ( svcaller run-all --tmp-dir {scratch} + --event-type {event_type} + --fasta-filename {reference_seq} + --filter-event-overlap --events-gtf {output_gtf} --events-bam {output_bam} {input_bam} ).format( scratch=self.scratch, event_type=self.event_type, reference_seq=self.reference_sequence, output_gtf=self.output_gtf, output_bam=self.output_bam, input_bam=self.input_bam, ) deactivate_env_cmd = source deactivate return {} {} {} .format( activate_env_cmd, run_all_cmd, deactivate_env_cmd, ) class Sveffect(Job): def __init__(self): Job.__init__(self) self.input_del_gtf = None self.input_dup_gtf = None self.input_inv_gtf = None self.input_tra_gtf = None self.ts_regions = None self.ar_regions = None self.fusion_regions = None self.output_combined_bed = None self.output_effects_json = None self.jobname = sveffect def command(self): activate_env_cmd = source activate svcallerenv make_bed_cmd = ( sveffect make-bed + --del-gtf {del_gtf} + --dup-gtf {dup_gtf} + --inv-gtf {inv_gtf} + --tra-gtf {tra_gtf} + {output_combined_bed} ).format( del_gtf=self.input_del_gtf, dup_gtf=self.input_dup_gtf, inv_gtf=self.input_inv_gtf, tra_gtf=self.input_tra_gtf, output_combined_bed=self.output_combined_bed ) predict_cmd = ( sveffect predict + --ts-regions {ts_regions} + --ar-regions {ar_regions} + --fusion-regions {fusion_regions} + --effects-filename {output_effects_json} {combined_effects_bed} ).format( ts_regions=self.ts_regions, ar_regions=self.ar_regions, fusion_regions=self.fusion_regions, output_effects_json=self.output_effects_json, combined_effects_bed=self.output_combined_bed, ) deactivate_env_cmd = source deactivate return {} {} {} {} .format( activate_env_cmd, make_bed_cmd, predict_cmd, deactivate_env_cmd, )","title":"SVcaller"},{"location":"lld/variantcalling/#cnv-calling","text":"Copy Number Variation is called by cnvkit class CNVkit(Job): Runs CNVkit. Either reference or targets_bed must be supplied def __init__(self, input_bam, output_cns, output_cnr, reference=None, targets_bed=None, scratch= /tmp , fasta=None): self.input_bam = input_bam self.reference = reference self.fasta = fasta self.output_cnr = output_cnr self.output_cns = output_cns self.targets_bed = targets_bed self.scratch = scratch self.jobname = cnvkit def command(self): if not self.reference and not self.targets_bed: raise ValueError( Either reference or targets_bed must be supplied ) if self.reference and self.targets_bed: raise ValueError( Supply either reference OR targets_bed ) tmpdir = {}/cnvkit-{} .format(self.scratch, uuid.uuid4()) sample_prefix = stripsuffix(os.path.basename(self.input_bam), .bam ) cnvkit_cmd = cnvkit.py batch + required( , self.input_bam) + \\ optional( -r , self.reference) + \\ conditional(self.targets_bed, --fasta + str(self.fasta) ) + \\ conditional(self.targets_bed, -n ) + \\ optional( -t , self.targets_bed) + \\ required( -d , tmpdir) copy_cns_cmd = cp {}/{}.cns .format(tmpdir, sample_prefix) + required( , self.output_cns) copy_cnr_cmd = cp {}/{}.cnr .format(tmpdir, sample_prefix) + required( , self.output_cnr) rm_cmd = rm -r {} .format(tmpdir) return .join([cnvkit_cmd, copy_cns_cmd, copy_cnr_cmd, rm_cmd]) Fixes the output of CNV-kit, using a specified table of reference data that should be specific to the sample type, capture kit, and library prep kit. class CNVkitFix(Job): def __init__(self, input_cnr, input_cns, input_ref, output_cnr, output_cns): self.input_cnr = input_cnr self.input_cns = input_cns self.input_ref = input_ref self.output_cns = output_cns self.output_cnr = output_cnr self.jobname = cnvkit-fix def command(self): return ( fix_cnvkit.py --input-cnr {input_cnr} --input-cns {input_cns} + --input-reference {input_ref} --output-cnr {output_cnr} + --output-cns {output_cns} ).format( input_cnr=self.input_cnr, input_cns=self.input_cns, input_ref=self.input_ref, output_cns=self.output_cns, output_cnr=self.output_cnr) Converts CNVkit segment format (.cns files) to DNAcopy segment format (.seg files). class Cns2Seg(Job): def __init__(self, input_cns, output_seg): self.input_cns = input_cns self.output_seg = output_seg def command(self): cmd = cnvkit.py export seg + required( -o , self.output_seg) + required( , self.input_cns) return cmd","title":"CNV calling"},{"location":"lld/variantcalling/#qdnaseq","text":"class QDNASeq(Job): def __init__(self, input_bam, output_segments, background=None): Job.__init__(self) self.input = input_bam self.output = output_segments self.background = background self.jobname = qdnaseq def command(self): activate_env_cmd = source activate qdnaseqenv qdnaseq_cmd = qdnaseq.R + \\ required( --bam , self.input) + \\ required( --output , self.output) + \\ optional( --background , self.background) deactivate_env_cmd = source deactivate return {} {} {} .format( activate_env_cmd, qdnaseq_cmd, deactivate_env_cmd, ) class QDNASeq2Bed(Job): def __init__(self, input_segments, output_bed, genes_gtf): Job.__init__(self) self.input_segments = input_segments self.output_bed = output_bed self.genes_gtf = genes_gtf def command(self): qdnaseq2bed_cmd = qdnaseq2bed.py -n segments + \\ required( -i , self.input_segments) + \\ | sort -k1,1 -k2,2n + \\ | bedtools median -c 5 -o mean + \\ required( -a , self.genes_gtf) + -b - + \\ | cnvgtf2bed.py -i /dev/stdin -n gene_id + \\ required( , self.output_bed) return qdnaseq2bed_cmd","title":"QDNASeq"},{"location":"lld/variantcalling/#alasccacnaplot","text":"class AlasccaCNAPlot(Job): def __init__(self): Job.__init__(self) self.input_cnr = None self.input_cns = None self.input_germline_vcf = None self.input_somatic_vcf = None self.chrsizes = None self.output_png = None self.output_cna = None self.output_purity = None self.jobname = alascca-cna def command(self): return alasccaCNA.R + \\ required( --cnr , self.input_cnr) + \\ required( --cns , self.input_cns) + \\ required( --germlinevcf , self.input_germline_vcf) + \\ required( --somaticvcf , self.input_somatic_vcf) + \\ required( --chrsizes , self.chrsizes) + \\ required( --png , self.output_png) + \\ required( --json.cna , self.output_cna) + \\ required( --json.purity , self.output_purity)","title":"AlasccaCNAPlot"},{"location":"lld/variantcalling/#liqbiocnaplot","text":"class LiqbioCNAPlot(Job): def __init__(self): Job.__init__(self) self.input_tumor_cnr = None self.input_tumor_cns = None self.input_normal_cnr = None self.input_normal_cns = None self.input_het_snps_vcf = None self.input_purecn_csv = None self.input_purecn_genes_csv = None self.input_purecn_loh_csv = None self.input_purecn_variants_csv = None self.input_svcaller_T_DEL = None self.input_svcaller_T_DUP = None self.input_svcaller_T_INV = None self.input_svcaller_T_TRA = None self.input_svcaller_N_DEL = None self.input_svcaller_N_DUP = None self.input_svcaller_N_INV = None self.input_svcaller_N_TRA = None self.input_germline_mut_vcf = None self.input_somatic_mut_vcf = None self.output_plot_png = None self.output_cna_json = None self.output_purity_json = None self.jobname = liqbio-cna def command(self): return liqbioCNA.R + \\ required( --tumor_cnr , self.input_tumor_cnr) + \\ required( --tumor_cns , self.input_tumor_cns) + \\ required( --normal_cnr , self.input_normal_cnr) + \\ required( --normal_cns , self.input_normal_cns) + \\ required( --het_snps_vcf , self.input_het_snps_vcf) + \\ required( --purecn_csv , self.input_purecn_csv) + \\ required( --purecn_genes_csv , self.input_purecn_genes_csv) + \\ required( --purecn_loh_csv , self.input_purecn_loh_csv) + \\ required( --purecn_variants_csv , self.input_purecn_variants_csv) + \\ required( --svcaller_T_DEL , self.input_svcaller_T_DEL) + \\ required( --svcaller_T_DUP , self.input_svcaller_T_DUP) + \\ required( --svcaller_T_INV , self.input_svcaller_T_INV) + \\ required( --svcaller_T_TRA , self.input_svcaller_T_TRA) + \\ required( --svcaller_N_DEL , self.input_svcaller_N_DEL) + \\ required( --svcaller_N_DUP , self.input_svcaller_N_DUP) + \\ required( --svcaller_N_INV , self.input_svcaller_N_INV) + \\ required( --svcaller_N_TRA , self.input_svcaller_N_TRA) + \\ required( --germline_mut_vcf , self.input_germline_mut_vcf) + \\ required( --somatic_mut_vcf , self.input_somatic_mut_vcf) + \\ required( --plot_png , self.output_plot_png) + \\ required( --cna_json , self.output_cna_json) + \\ required( --purity_json , self.output_purity_json)","title":"LiqbioCNAPlot"},{"location":"lld/variantcalling/#makecnvkittracks","text":"Generate a IGV tracks representing the profile and segment information from a CNV-kit run. class MakeCNVkitTracks(Job): def __init__(self): Job.__init__(self) self.input_cns = None self.input_cnr = None self.output_profile_bedgraph = None self.output_segments_bedgraph = None self.jobname = make_cnvkit_tracks def command(self): awk_cmd1 = awk '$1 != \\ chromosome\\ {print $1\\ \\\\t\\ $2\\ \\\\t\\ $3\\ \\\\t\\ $5}' %s %s % \\ (self.input_cnr, self.output_profile_bedgraph) awk_cmd2 = awk '$1 != \\ chromosome\\ {print $1\\ \\\\t\\ $2\\ \\\\t\\ $3\\ \\\\t\\ $5}' %s %s % \\ (self.input_cns, self.output_segments_bedgraph) return {} {} .format(awk_cmd1, awk_cmd2)","title":"MakeCNVkitTracks"},{"location":"lld/variantcalling/#makeqdnaseqtracks","text":"Generate a IGV tracks representing the information from a QDNA-seq run. class MakeQDNAseqTracks(Job): def __init__(self): Job.__init__(self) self.input_qdnaseq_file = None self.output_segments_bedgraph = None self.output_copynumber_tdf = None self.output_readcount_tdf = None self.jobname = make_qdnaseq_tracks def command(self): copynumber_wig = {scratch}/copynumber-{uuid}.wig .format( scratch=self.scratch, uuid=uuid.uuid4()) readcount_wig = {scratch}/readcount-{uuid}.wig .format( scratch=self.scratch, uuid=uuid.uuid4()) qdnaseq_to_bedgraph_cmd = qdnaseq_to_bedgraph.py {} {} .format( self.input_qdnaseq_file, self.output_segments_bedgraph) qdnaseq_to_wig_cmd = qdnaseq_to_wig.py {} {} {} .format( self.input_qdnaseq_file, copynumber_wig, readcount_wig) igvtools_cmd1 = igvtools toTDF {copynumber_wig} {copynumber_tdf} hg19 .format( copynumber_wig=copynumber_wig, copynumber_tdf=self.output_copynumber_tdf ) igvtools_cmd2 = igvtools toTDF {readcount_wig} {readcount_tdf} hg19 .format( readcount_wig=readcount_wig, readcount_tdf=self.output_readcount_tdf ) return {} {} {} .format(qdnaseq_to_bedgraph_cmd, qdnaseq_to_wig_cmd, igvtools_cmd1, igvtools_cmd2)","title":"MakeQDNAseqTracks"}]}